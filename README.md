# GPT-2 From Scratch

This repository provides a minimal, educational, and annotated implementation of GPT-2, designed for learning and experimentation. The code is written from scratch to help users understand the inner workings of transformer-based language models.

## Features

- Pure Python implementation of core GPT-2 components
- Step-by-step explanations and comments throughout the code
- Minimal dependencies for easy setup and experimentation
- Example scripts for training and inference
- Configurable model parameters

## Getting Started

### Prerequisites

- Python 3.7+
- [NumPy](https://numpy.org/)
- [Torch](https://pytorch.org/) (if using GPU acceleration)
- Basic understanding of linear Algebra concepts like dot product,vectors and matrix multiplication


## Papers to read:
- Attention is all you need : https://arxiv.org/abs/1706.03762
- GPT-2 paper(Large Language Models are just Multitask learners)

## Diagram
[LLM](llm.webp)

### Installation

Clone this repository:
```bash
git clone https://github.com/Cohegen/gpt2-from-scratch.git
cd gpt2-from-scratch



