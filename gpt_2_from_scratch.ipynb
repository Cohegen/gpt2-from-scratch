{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP4OKRY+myG8p4N/XhlXL7i"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Introduction:Understanding Large Language Models through GPT-2"
      ],
      "metadata": {
        "id": "1S-EKQRfmZTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Large Language models are a class of deep learning models designed to understand and generate human-like text.\n",
        "* They're built using the transformer architecture and are widely used in applications like chatbots,translation and text generation\n"
      ],
      "metadata": {
        "id": "jLIMlxHQmfIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Understanding word embeddings"
      ],
      "metadata": {
        "id": "PJ9DloqAnn-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Deep neural net models, including LLMs, cannot process raw text directly.\n",
        "* Since text is categorical, it is not compatible with the mathematical operations used to implement and train neural networks.Therefore we need a way to represent words as continous-valued vectors.\n",
        "* This concept of converting data into a vector format is referred to embedding.\n",
        "* In simple terms, an embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a convert nonnumeric data into format that neural networks can process.\n",
        "* Instead of treating each word as a unique symbol, embeddings map words into a continuous vector space where similar words are close together.\n",
        "* For example, the vectors for \"king\" and \"queen\" or \"run\" and \"jog\" will be near each other because they share similar meanings. These embeddings are learned from data and capture semantic relationships, making them a fundamental building block in modern NLP models like GPT-2."
      ],
      "metadata": {
        "id": "DAPAiIK8nxUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Tokenizing text"
      ],
      "metadata": {
        "id": "ca9i_4jHpou2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* LLMs take text as inputs but wait before these words are mapped into embeddings, the go through a stage known as tokenization.\n",
        "* Tokenization is the process of breaking text into smaller units called tokens- which can be words,subwords, or even characters.\n",
        "* This step is crucial because language models like GPT-2  don't understand raw text; they work with numbers.\n",
        "* Tokenization converts texts into sequence of tokens that can be mapped to numerical IDs\n",
        "* The text we'll tokenizer for LLM training is `The dante's inferno dataset`.\n",
        "* You can find this dataset from the official project gutenberg website.\n",
        "* Let's get coding"
      ],
      "metadata": {
        "id": "ytG-DzRYpvl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## downloading dante's inferno dataset from project gutenberg website.\n",
        "import urllib.request\n",
        "\n",
        "url = 'https://www.gutenberg.org/cache/epub/41537/pg41537.txt'\n",
        "file_path = \"dante's-inferno.txt\"\n",
        "urllib.request.urlretrieve(url,file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfxwYwh7s06C",
        "outputId": "2107b286-e3dc-4d03-ca68-70322a8ba6e1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"dante's-inferno.txt\", <http.client.HTTPMessage at 0x7fd433358690>)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's load `dante's-inferno.txt` file using python file handling utilities."
      ],
      "metadata": {
        "id": "qLe6SX9ltY2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"dante's-inferno.txt\",'r',encoding='utf-8') as f:\n",
        "  raw_text = f.read()\n",
        "print(\"Total number of characters:\",len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB1fKG9lt0Bg",
        "outputId": "29a2db20-3284-45b8-fff4-ee381244c8f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 700670\n",
            "﻿The Project Gutenberg eBook of The Divine Comedy of Dante Alighieri: The Inferno\n",
            "    \n",
            "This ebook i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Our goal is to tokenize this 700,670 character long story into individual words and special characters that we can turn into embeddings for LLM training.\n",
        "* Which option do we have in splitting the texts, in this short story we can use python's regular expression library `re` for illustration purposes."
      ],
      "metadata": {
        "id": "zeI8coswwqAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using some simple text we can use `re.split` command with the following syntax to split a text on whitespaces characters."
      ],
      "metadata": {
        "id": "4SCkdqw3xXeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"Hello, world. I am Antonius, nice to meet you.\"\n",
        "result = re.split(r'(\\s)',text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwYj9HT5xpoy",
        "outputId": "071437c4-e280-4ff4-a984-796e5e1ab4d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'I', ' ', 'am', ' ', 'Antonius,', ' ', 'nice', ' ', 'to', ' ', 'meet', ' ', 'you.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This simple tokenization scheme mostly works for seperating the use case text into individual words, however some are still connected to punctuation characters that we want to have as seperate entities.\n",
        "* We have also avoided a step in tokenization in which we make all text inputs lowercase because capitalization helps LLMs distinguish between proper nouns and common nouns, understand sentence structure,and learn to geneate text with proper capitalization.\n",
        "\n",
        "\n",
        "* Let's modify the regular expression splits on whitespaces `(\\s)`, commas, and periods ([,.])."
      ],
      "metadata": {
        "id": "RWR70LcTyISL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.] | \\s)',text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49BXSzWtzbSv",
        "outputId": "bb4bbb71-d0b6-409a-a085-f249eec434d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ', ', 'world', '. ', 'I am Antonius', ', ', 'nice to meet you.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization works well here but let's increase the diversity of punctuation which our dummy tokenizer can work on e.g question marks, double-slashes."
      ],
      "metadata": {
        "id": "c_qS0b5wzzof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is attention is all you need-- a good research paper?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|\\s+|--)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Tjb_wkLX0Fj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae968a6-d8c8-4585-cf0b-5470fcf22213"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'attention', ' ', 'is', ' ', 'all', ' ', 'you', ' ', 'need', '--', '', ' ', 'a', ' ', 'good', ' ', 'research', ' ', 'paper', '?', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a basic tokenizer working, let's apply it to Dante's Inferno."
      ],
      "metadata": {
        "id": "9uIVD1ljQL0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text = re.split(r'([,.:;?_!\"()\\']|\\s+|--)',raw_text)\n",
        "preprocessed_text = [item.strip() for item in preprocessed_text if item.strip()]\n",
        "print(len(preprocessed_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUXqlxjuQXEW",
        "outputId": "11a914fa-9841-4a8f-990d-9c037c75adb1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "145180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The print statement outputs `145180` tokens in this text(without whitespaces).\n",
        "Now let's print the first 70 tokens for quick visual check"
      ],
      "metadata": {
        "id": "_LU-kz9rQ5C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed_text[:70])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEdsNmjRRMV5",
        "outputId": "36b4ef5a-db9f-4660-ebcc-18b5f9dd33ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'Divine', 'Comedy', 'of', 'Dante', 'Alighieri', ':', 'The', 'Inferno', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Converting tokens into token IDs"
      ],
      "metadata": {
        "id": "GsRtx5XDRaNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now, let's convert these tokens from a Python string to an integer representation to produce the token IDs.\n",
        "* This step is an intermediate step before converting the token IDs into embedding vectors.\n",
        "\n",
        "* Since we have tokenized Dante's Inferno and assigned it to Python variable called `preprocessed_text`, let's create a list of all unique tokens and sort them alphabetically to determine the vocabulary size."
      ],
      "metadata": {
        "id": "6DzQ9lNBRerK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed_text))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNS-EkiYSrdf",
        "outputId": "e0d5f300-07ee-4eed-97ff-1599422edd13"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* After finding out the `vocab_size=13909`, we create the vocabulary and print its first 500 entries."
      ],
      "metadata": {
        "id": "FYYQeWgcS9x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab  ={token:integer for integer,token in enumerate(all_words)}\n",
        "for i,item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i >= 500:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qKdtdt3TMa9",
        "outputId": "3597c06d-e109-490f-b266-a094bab10cda"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "('#41537]', 2)\n",
            "('$1', 3)\n",
            "('$5', 4)\n",
            "(\"'\", 5)\n",
            "('(', 6)\n",
            "(')', 7)\n",
            "('***', 8)\n",
            "(',', 9)\n",
            "('-', 10)\n",
            "('--', 11)\n",
            "('.', 12)\n",
            "('000', 13)\n",
            "('1', 14)\n",
            "('10', 15)\n",
            "('100', 16)\n",
            "('101', 17)\n",
            "('102', 18)\n",
            "('103', 19)\n",
            "('1037', 20)\n",
            "('104', 21)\n",
            "('105', 22)\n",
            "('106', 23)\n",
            "('107', 24)\n",
            "('108', 25)\n",
            "('1085', 26)\n",
            "('109', 27)\n",
            "('10th', 28)\n",
            "('11', 29)\n",
            "('110', 30)\n",
            "('1106', 31)\n",
            "('111', 32)\n",
            "('1115', 33)\n",
            "('112', 34)\n",
            "('113', 35)\n",
            "('114', 36)\n",
            "('115', 37)\n",
            "('1152-1190', 38)\n",
            "('116', 39)\n",
            "('117', 40)\n",
            "('118', 41)\n",
            "('1180', 42)\n",
            "('1185', 43)\n",
            "('119', 44)\n",
            "('1193', 45)\n",
            "('1198', 46)\n",
            "('12', 47)\n",
            "('120', 48)\n",
            "('121', 49)\n",
            "('1215', 50)\n",
            "('122', 51)\n",
            "('1220', 52)\n",
            "('123', 53)\n",
            "('1238', 54)\n",
            "('1239', 55)\n",
            "('124', 56)\n",
            "('1248', 57)\n",
            "('1249', 58)\n",
            "('125', 59)\n",
            "('1250', 60)\n",
            "('1252', 61)\n",
            "('1258', 62)\n",
            "('1259', 63)\n",
            "('126', 64)\n",
            "('1260', 65)\n",
            "('1261', 66)\n",
            "('1264', 67)\n",
            "('1265', 68)\n",
            "('1266', 69)\n",
            "('1267', 70)\n",
            "('1268', 71)\n",
            "('1269', 72)\n",
            "('127', 73)\n",
            "('1270', 74)\n",
            "('1271', 75)\n",
            "('1273', 76)\n",
            "('1275', 77)\n",
            "('1277', 78)\n",
            "('1278', 79)\n",
            "('128', 80)\n",
            "('1280', 81)\n",
            "('1281', 82)\n",
            "('1282', 83)\n",
            "('1283', 84)\n",
            "('1284', 85)\n",
            "('1285', 86)\n",
            "('1286', 87)\n",
            "('1288', 88)\n",
            "('1289', 89)\n",
            "('129', 90)\n",
            "('1290', 91)\n",
            "('1291', 92)\n",
            "('1292', 93)\n",
            "('1293', 94)\n",
            "('1294', 95)\n",
            "('1295', 96)\n",
            "('1296', 97)\n",
            "('1297', 98)\n",
            "('1298', 99)\n",
            "('1299', 100)\n",
            "('13', 101)\n",
            "('130', 102)\n",
            "('1300', 103)\n",
            "('1301', 104)\n",
            "('1301-1302', 105)\n",
            "('1301-2', 106)\n",
            "('1302', 107)\n",
            "('1303', 108)\n",
            "('1304', 109)\n",
            "('1305', 110)\n",
            "('1306', 111)\n",
            "('1307', 112)\n",
            "('1308', 113)\n",
            "('1309', 114)\n",
            "('131', 115)\n",
            "('1310', 116)\n",
            "('1311', 117)\n",
            "('1312', 118)\n",
            "('1313', 119)\n",
            "('1313-1316', 120)\n",
            "('1314', 121)\n",
            "('1315', 122)\n",
            "('1316', 123)\n",
            "('132', 124)\n",
            "('1320', 125)\n",
            "('1321', 126)\n",
            "('1326', 127)\n",
            "('1327', 128)\n",
            "('133', 129)\n",
            "('1332', 130)\n",
            "('1333', 131)\n",
            "('134', 132)\n",
            "('1343', 133)\n",
            "('135', 134)\n",
            "('1350', 135)\n",
            "('136', 136)\n",
            "('137', 137)\n",
            "('1370', 138)\n",
            "('138', 139)\n",
            "('139', 140)\n",
            "('14', 141)\n",
            "('140', 142)\n",
            "('141', 143)\n",
            "('142', 144)\n",
            "('143', 145)\n",
            "('144', 146)\n",
            "('145', 147)\n",
            "('1465', 148)\n",
            "('147', 149)\n",
            "('148', 150)\n",
            "('149', 151)\n",
            "('1496', 152)\n",
            "('15', 153)\n",
            "('150', 154)\n",
            "('1500', 155)\n",
            "('151', 156)\n",
            "('153', 157)\n",
            "('1539', 158)\n",
            "('154', 159)\n",
            "('155', 160)\n",
            "('156', 161)\n",
            "('16', 162)\n",
            "('161', 163)\n",
            "('168', 164)\n",
            "('17', 165)\n",
            "('176', 166)\n",
            "('18', 167)\n",
            "('1823', 168)\n",
            "('1826', 169)\n",
            "('1839', 170)\n",
            "('184', 171)\n",
            "('1854', 172)\n",
            "('1861', 173)\n",
            "('1864', 174)\n",
            "('1865', 175)\n",
            "('1879', 176)\n",
            "('1880', 177)\n",
            "('1882', 178)\n",
            "('1884', 179)\n",
            "('19', 180)\n",
            "('192', 181)\n",
            "('199', 182)\n",
            "('1st', 183)\n",
            "('2', 184)\n",
            "('20', 185)\n",
            "('20%', 186)\n",
            "('200', 187)\n",
            "('2001', 188)\n",
            "('2012', 189)\n",
            "('2024', 190)\n",
            "('203', 191)\n",
            "('209', 192)\n",
            "('21', 193)\n",
            "('213', 194)\n",
            "('217', 195)\n",
            "('22', 196)\n",
            "('225', 197)\n",
            "('23', 198)\n",
            "('233', 199)\n",
            "('24', 200)\n",
            "('241', 201)\n",
            "('249', 202)\n",
            "('25', 203)\n",
            "('25th', 204)\n",
            "('26', 205)\n",
            "('260', 206)\n",
            "('264', 207)\n",
            "('268', 208)\n",
            "('269', 209)\n",
            "('26th', 210)\n",
            "('27', 211)\n",
            "('279', 212)\n",
            "('27th', 213)\n",
            "('28', 214)\n",
            "('29', 215)\n",
            "('2d', 216)\n",
            "('3', 217)\n",
            "('30', 218)\n",
            "('30th', 219)\n",
            "('31', 220)\n",
            "('312', 221)\n",
            "('32', 222)\n",
            "('33', 223)\n",
            "('34', 224)\n",
            "('349', 225)\n",
            "('35', 226)\n",
            "('353', 227)\n",
            "('36', 228)\n",
            "('37', 229)\n",
            "('38', 230)\n",
            "('39', 231)\n",
            "('3d', 232)\n",
            "('4', 233)\n",
            "('40', 234)\n",
            "('41', 235)\n",
            "('42', 236)\n",
            "('43', 237)\n",
            "('44', 238)\n",
            "('45', 239)\n",
            "('46', 240)\n",
            "('47', 241)\n",
            "('48', 242)\n",
            "('482', 243)\n",
            "('49', 244)\n",
            "('496', 245)\n",
            "('4th', 246)\n",
            "('5', 247)\n",
            "('50', 248)\n",
            "('501', 249)\n",
            "('51', 250)\n",
            "('52', 251)\n",
            "('53', 252)\n",
            "('54', 253)\n",
            "('55', 254)\n",
            "('552', 255)\n",
            "('56', 256)\n",
            "('57', 257)\n",
            "('58', 258)\n",
            "('586', 259)\n",
            "('59', 260)\n",
            "('596-1887', 261)\n",
            "('5th', 262)\n",
            "('6', 263)\n",
            "('60', 264)\n",
            "('61', 265)\n",
            "('62', 266)\n",
            "('63', 267)\n",
            "('64', 268)\n",
            "('64-6221541', 269)\n",
            "('65', 270)\n",
            "('6500', 271)\n",
            "('66', 272)\n",
            "('67', 273)\n",
            "('68', 274)\n",
            "('69', 275)\n",
            "('6th', 276)\n",
            "('7', 277)\n",
            "('70', 278)\n",
            "('71', 279)\n",
            "('710', 280)\n",
            "('72', 281)\n",
            "('73', 282)\n",
            "('74', 283)\n",
            "('75', 284)\n",
            "('76', 285)\n",
            "('77', 286)\n",
            "('78', 287)\n",
            "('79', 288)\n",
            "('7th', 289)\n",
            "('8', 290)\n",
            "('80', 291)\n",
            "('801', 292)\n",
            "('809', 293)\n",
            "('81', 294)\n",
            "('82', 295)\n",
            "('83', 296)\n",
            "('84', 297)\n",
            "('84116', 298)\n",
            "('85', 299)\n",
            "('86', 300)\n",
            "('87', 301)\n",
            "('88', 302)\n",
            "('883', 303)\n",
            "('89', 304)\n",
            "('8th', 305)\n",
            "('9', 306)\n",
            "('90', 307)\n",
            "('91', 308)\n",
            "('92', 309)\n",
            "('93', 310)\n",
            "('94', 311)\n",
            "('95', 312)\n",
            "('96', 313)\n",
            "('97', 314)\n",
            "('98', 315)\n",
            "('99', 316)\n",
            "('9th', 317)\n",
            "(':', 318)\n",
            "(';', 319)\n",
            "('?', 320)\n",
            "('A', 321)\n",
            "('ACTUAL', 322)\n",
            "('AGREE', 323)\n",
            "('AGREEMENT', 324)\n",
            "('ALIGHIERI', 325)\n",
            "('AN', 326)\n",
            "('AND', 327)\n",
            "('ANY', 328)\n",
            "('ANYTHING', 329)\n",
            "('ASCII”', 330)\n",
            "('Abashed', 331)\n",
            "('Abati', 332)\n",
            "('Abbagliato', 333)\n",
            "('Abbey', 334)\n",
            "('Abbot', 335)\n",
            "('Abel', 336)\n",
            "('Abode', 337)\n",
            "('About', 338)\n",
            "('Above', 339)\n",
            "('Abraham', 340)\n",
            "('Abram', 341)\n",
            "('Abruzzi', 342)\n",
            "('Absalom', 343)\n",
            "('Accord', 344)\n",
            "('According', 345)\n",
            "('Accorso', 346)\n",
            "('Accorso[474]', 347)\n",
            "('Accounts', 348)\n",
            "('Accursed', 349)\n",
            "('Accurst', 350)\n",
            "('Accursèd', 351)\n",
            "('Acheron', 352)\n",
            "('Acheron[451]', 353)\n",
            "('Achilles', 354)\n",
            "('Achilles[781]', 355)\n",
            "('Acquacheta', 356)\n",
            "('Acquacheta[491]', 357)\n",
            "('Acquasparta', 358)\n",
            "('Acre', 359)\n",
            "('Across', 360)\n",
            "('Acting', 361)\n",
            "('Adam', 362)\n",
            "('Adam[769]', 363)\n",
            "('Add', 364)\n",
            "('Adding', 365)\n",
            "('Additional', 366)\n",
            "('Adelasia', 367)\n",
            "('Adelsberg', 368)\n",
            "('Adige', 369)\n",
            "('Adige[393]', 370)\n",
            "('Adimari', 371)\n",
            "('Adriatic', 372)\n",
            "('Afresh', 373)\n",
            "('Africa', 374)\n",
            "('Africanus', 375)\n",
            "('After', 376)\n",
            "('After[282]', 377)\n",
            "('Again', 378)\n",
            "('Against', 379)\n",
            "('Age', 380)\n",
            "('Ages', 381)\n",
            "('Agli', 382)\n",
            "('Aglow', 383)\n",
            "('Agnello', 384)\n",
            "('Ago', 385)\n",
            "('Agrigentum', 386)\n",
            "('Ah', 387)\n",
            "('Ahithophel', 388)\n",
            "('Aim', 389)\n",
            "('Alack', 390)\n",
            "('Alardo', 391)\n",
            "('Alas', 392)\n",
            "('Alberic', 393)\n",
            "('Alberic[850]', 394)\n",
            "('Alberigo', 395)\n",
            "('Albert', 396)\n",
            "('Albert[84]', 397)\n",
            "('Alberti', 398)\n",
            "('Alberto', 399)\n",
            "('Alchemists', 400)\n",
            "('Aldighieri', 401)\n",
            "('Aldighiero', 402)\n",
            "('Aldobrand', 403)\n",
            "('Aldobrandi', 404)\n",
            "('Aldobrando', 405)\n",
            "('Alecto', 406)\n",
            "('Aleppe', 407)\n",
            "('Alert', 408)\n",
            "('Alessio', 409)\n",
            "('Alexander', 410)\n",
            "('Alexander[437]', 411)\n",
            "('Ali', 412)\n",
            "('Ali[723]', 413)\n",
            "('Alichin', 414)\n",
            "('Alichin[591]', 415)\n",
            "('Alichino', 416)\n",
            "('Alighieri', 417)\n",
            "('Alike', 418)\n",
            "('All', 419)\n",
            "('Allegorico', 420)\n",
            "('Alliances', 421)\n",
            "('Allowing', 422)\n",
            "('Allured', 423)\n",
            "('Almighty', 424)\n",
            "('Almost', 425)\n",
            "('Alone', 426)\n",
            "('Along', 427)\n",
            "('Aloof', 428)\n",
            "('Alphonso', 429)\n",
            "('Alps', 430)\n",
            "('Alps[436]', 431)\n",
            "('Already', 432)\n",
            "('Also', 433)\n",
            "('Although', 434)\n",
            "('Always', 435)\n",
            "('Alyscampo', 436)\n",
            "('Am', 437)\n",
            "('Amazement', 438)\n",
            "('Ambassador', 439)\n",
            "('Amen', 440)\n",
            "('Amidei', 441)\n",
            "('Among', 442)\n",
            "('Amor', 443)\n",
            "('Amphiaraüs', 444)\n",
            "('Amphiaräus', 445)\n",
            "('Amphion', 446)\n",
            "('Amphion[798]', 447)\n",
            "('Ampère', 448)\n",
            "('An', 449)\n",
            "('Anagni', 450)\n",
            "('Anastasius', 451)\n",
            "('Anastasius[376]', 452)\n",
            "('Anaxagoras', 453)\n",
            "('Anchises', 454)\n",
            "('Ancona', 455)\n",
            "('And', 456)\n",
            "('Andrea', 457)\n",
            "('Andrews', 458)\n",
            "('Angelo', 459)\n",
            "('Angels', 460)\n",
            "('Anger', 461)\n",
            "('Angiolel', 462)\n",
            "('Angiolello', 463)\n",
            "('Animal', 464)\n",
            "('Anjou', 465)\n",
            "('Annas', 466)\n",
            "('Anonimo', 467)\n",
            "('Another', 468)\n",
            "('Anselm', 469)\n",
            "('Anselmuccio', 470)\n",
            "('Antenor', 471)\n",
            "('Antenora', 472)\n",
            "('Antioch', 473)\n",
            "('Antiochus', 474)\n",
            "('Antonio', 475)\n",
            "('Antæus', 476)\n",
            "('Antæus[790]', 477)\n",
            "('Anxious', 478)\n",
            "('Any', 479)\n",
            "('Apennine', 480)\n",
            "('Apennines', 481)\n",
            "('Apia', 482)\n",
            "('Apocalypse', 483)\n",
            "('Apollo', 484)\n",
            "('Apostle', 485)\n",
            "('Apothecaries', 486)\n",
            "('April', 487)\n",
            "('Apuana', 488)\n",
            "('Apulia', 489)\n",
            "('Apulian', 490)\n",
            "('Apulians', 491)\n",
            "('Aqua', 492)\n",
            "('Aquarius', 493)\n",
            "('Aquarius[630]', 494)\n",
            "('Aquinas', 495)\n",
            "('Aquitaine', 496)\n",
            "('Arab', 497)\n",
            "('Arabian', 498)\n",
            "('Arabic', 499)\n",
            "('Arabs', 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We see that, the dictionary contains individual tokens associated with unique integer labels.\n",
        "* Next we will apply this vocabulary to convert new text into token IDs."
      ],
      "metadata": {
        "id": "ds8yJw0mVwBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's create a tokenizer class in Python with encode method that splits texts into tokens and carries out string-to-integer mapping to produce token IDs via vocabularies.\n",
        "* We also include `decode` method that carries out the reverse integer-to-string mapping to convert the token IDs back into text."
      ],
      "metadata": {
        "id": "ipA-UkCTWLV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "class SimpleTokenizer1:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|\\s|--)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self,ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "pvUK4aLGW3f5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's test this class using a sample paragraph from Dante's Inferno"
      ],
      "metadata": {
        "id": "BDAD37w8ZSSM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c170d54",
        "outputId": "a6d08e45-3542-4cd2-ecd7-b893f8edc9b7"
      },
      "source": [
        "##instantiating the SimpleTokenizer1\n",
        "tokenizer = SimpleTokenizer1(vocab)\n",
        "text = \"\"\"CANTO XXXIV. The Ninth Circle--the Fourth Ring or Judecca, the deepest point\n",
        "of the Inferno and the Centre of the Universe--it is the place\n",
        "of those treacherous to their Lords or Benefactors--Lucifer with\n",
        "Judas, Brutus, and Cassius hanging from his mouths--passage\n",
        "through the Centre of the Earth--ascent from the depths to the\n",
        "light of the stars in the Southern Hemisphere,\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[763, 3412, 12, 3024, 2221, 942, 11, 12706, 1433, 2637, 10090, 1830, 9, 12706, 6385, 10524, 10012, 12706, 1763, 4624, 12706, 883, 10012, 12706, 3193, 11, 8933, 8924, 12706, 10451, 10012, 12769, 12969, 12857, 12710, 1958, 10090, 658, 11, 1969, 13708, 1829, 9, 739, 9, 4624, 843, 8244, 7856, 8442, 9748, 11, 10256, 12806, 12706, 883, 10012, 12706, 1233, 11, 4781, 7856, 12706, 6484, 12857, 12706, 9222, 10012, 12706, 12239, 8672, 12706, 2889, 1663, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's see if we can turn these token IDs back into text using the `decode` method."
      ],
      "metadata": {
        "id": "kv3wPfmJZf02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wldJqWwNZrMI",
        "outputId": "a469a867-3124-474c-f09f-da3a113b3737"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CANTO XXXIV. The Ninth Circle -- the Fourth Ring or Judecca, the deepest point of the Inferno and the Centre of the Universe -- it is the place of those treacherous to their Lords or Benefactors -- Lucifer with Judas, Brutus, and Cassius hanging from his mouths -- passage through the Centre of the Earth -- ascent from the depths to the light of the stars in the Southern Hemisphere,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* So I guess i works now let's apply it to a new text sample not contained in the training set:"
      ],
      "metadata": {
        "id": "e7iHq_sXZ3NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, welcome to Bogota\"\n",
        "##print(tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "Gh5JHcKXaEiW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can see that after we execute this code we get the above error.\n",
        "* We're getting error because the word `Hello` was not used in `Dante's Inferno` story.\n",
        "* Hence, it is not contained in the vocabulary.\n",
        "* This highlights the need to consider large and diverse training sets to extend the vocabulary when working on LLMs.\n",
        "\n",
        "* Next we'll try to test the tokenizer further on text that contain unkown words and also special tokens that can be used to provide further context for an LLM during training."
      ],
      "metadata": {
        "id": "6lIr-dQramq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Adding special context tokens"
      ],
      "metadata": {
        "id": "AzUesKJPb7_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Our task now is to modify the tokenizer to handle unknown words.\n",
        "* And also need to address the usage and additional of special context tokens that can enhance a model's understading of context or other information.\n",
        "* These special tokens iclude markers for unknown words and document boundaries.\n",
        "\n",
        "* Let'smodify the vocabulary to include these two special tokens , `<unk>` and `<|endoftext|>`, by adding them to our list of all unique words."
      ],
      "metadata": {
        "id": "3Acu6GU1cDgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed_text)))\n",
        "all_tokens.append(\"<|endoftext|>\") # Append as individual string\n",
        "all_tokens.append(\"<|unk|>\") # Append as individual string\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhhvHCNLdZ56",
        "outputId": "5685eceb-4f05-411c-81b4-2c586a2c456c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now let's print the last 500 enties of the updated vocabulary:"
      ],
      "metadata": {
        "id": "MCRs3JDfdqgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-500:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDQpxMEkd1BZ",
        "outputId": "a44538b7-3a18-4e60-8cb0-4c082b92a9a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('visage', 13411)\n",
            "('visible', 13412)\n",
            "('vision', 13413)\n",
            "('visions', 13414)\n",
            "('visit', 13415)\n",
            "('visited', 13416)\n",
            "('visiting', 13417)\n",
            "('vital', 13418)\n",
            "('vitally', 13419)\n",
            "('vitiate', 13420)\n",
            "('vivid', 13421)\n",
            "('vividly', 13422)\n",
            "('vizor', 13423)\n",
            "('vobis', 13424)\n",
            "('vocal', 13425)\n",
            "('vocation', 13426)\n",
            "('vogue', 13427)\n",
            "('voice', 13428)\n",
            "('voices', 13429)\n",
            "('void', 13430)\n",
            "('vol', 13431)\n",
            "('volition', 13432)\n",
            "('volume', 13433)\n",
            "('voluntarily', 13434)\n",
            "('volunteer', 13435)\n",
            "('volunteers', 13436)\n",
            "('vomiting', 13437)\n",
            "('voracity', 13438)\n",
            "('voted', 13439)\n",
            "('vouches', 13440)\n",
            "('vow', 13441)\n",
            "('voyage', 13442)\n",
            "('vulgar', 13443)\n",
            "('wafted', 13444)\n",
            "('wage', 13445)\n",
            "('waged', 13446)\n",
            "('wager', 13447)\n",
            "('wages', 13448)\n",
            "('wagged', 13449)\n",
            "('waging', 13450)\n",
            "('wags', 13451)\n",
            "('wail', 13452)\n",
            "('wailed', 13453)\n",
            "('wailed[530]', 13454)\n",
            "('wailing', 13455)\n",
            "('wailings', 13456)\n",
            "('wails', 13457)\n",
            "('waist', 13458)\n",
            "('wait', 13459)\n",
            "('waiting', 13460)\n",
            "('waits', 13461)\n",
            "('wake', 13462)\n",
            "('waketh', 13463)\n",
            "('waking', 13464)\n",
            "('walk', 13465)\n",
            "('walks', 13466)\n",
            "('wall', 13467)\n",
            "('wall-painting', 13468)\n",
            "('wall[651]', 13469)\n",
            "('walled', 13470)\n",
            "('wallet', 13471)\n",
            "('wallow', 13472)\n",
            "('walls', 13473)\n",
            "('wand', 13474)\n",
            "('wander', 13475)\n",
            "('wandered', 13476)\n",
            "('wanderer', 13477)\n",
            "('wandering', 13478)\n",
            "('wanderings', 13479)\n",
            "('wanders', 13480)\n",
            "('waned', 13481)\n",
            "('wanness', 13482)\n",
            "('want', 13483)\n",
            "('wanted', 13484)\n",
            "('wanting', 13485)\n",
            "('war', 13486)\n",
            "('war-bell', 13487)\n",
            "('war-cries', 13488)\n",
            "('ward', 13489)\n",
            "('warders', 13490)\n",
            "('wards', 13491)\n",
            "('warehouses', 13492)\n",
            "('warlike', 13493)\n",
            "('warm', 13494)\n",
            "('warmed', 13495)\n",
            "('warmest', 13496)\n",
            "('warmly', 13497)\n",
            "('warms', 13498)\n",
            "('warmth', 13499)\n",
            "('warned', 13500)\n",
            "('warning', 13501)\n",
            "('warns', 13502)\n",
            "('warranties', 13503)\n",
            "('warrior', 13504)\n",
            "('wars', 13505)\n",
            "('wary', 13506)\n",
            "('was', 13507)\n",
            "('wash', 13508)\n",
            "('washed', 13509)\n",
            "('wasps', 13510)\n",
            "('wast', 13511)\n",
            "('waste', 13512)\n",
            "('waste[396]', 13513)\n",
            "('wasted', 13514)\n",
            "('watch', 13515)\n",
            "('watch-dog', 13516)\n",
            "('watched', 13517)\n",
            "('watchfulness', 13518)\n",
            "('watching', 13519)\n",
            "('water', 13520)\n",
            "('water-brooks', 13521)\n",
            "('waterfall', 13522)\n",
            "('waterfalls', 13523)\n",
            "('waters', 13524)\n",
            "('watery', 13525)\n",
            "('wave', 13526)\n",
            "('waver', 13527)\n",
            "('wavered', 13528)\n",
            "('wavers', 13529)\n",
            "('waves', 13530)\n",
            "('waving', 13531)\n",
            "('wax', 13532)\n",
            "('waxed', 13533)\n",
            "('waxen', 13534)\n",
            "('way', 13535)\n",
            "('ways', 13536)\n",
            "('we', 13537)\n",
            "('weak', 13538)\n",
            "('weaken', 13539)\n",
            "('weakness', 13540)\n",
            "('wealth', 13541)\n",
            "('wealthier', 13542)\n",
            "('wealthy', 13543)\n",
            "('weapon', 13544)\n",
            "('wear', 13545)\n",
            "('wearers', 13546)\n",
            "('wearied', 13547)\n",
            "('weariness', 13548)\n",
            "('wearing', 13549)\n",
            "('wears', 13550)\n",
            "('weary', 13551)\n",
            "('weather', 13552)\n",
            "('weathered', 13553)\n",
            "('weaver', 13554)\n",
            "('weavers', 13555)\n",
            "('web', 13556)\n",
            "('website', 13557)\n",
            "('wed', 13558)\n",
            "('wedded', 13559)\n",
            "('wedding', 13560)\n",
            "('weeks', 13561)\n",
            "('ween', 13562)\n",
            "('weep', 13563)\n",
            "('weeping', 13564)\n",
            "('weeps', 13565)\n",
            "('weet', 13566)\n",
            "('weighed', 13567)\n",
            "('weighs', 13568)\n",
            "('weight', 13569)\n",
            "('weight[397]', 13570)\n",
            "('weights', 13571)\n",
            "('weighty', 13572)\n",
            "('welcome', 13573)\n",
            "('welcomed', 13574)\n",
            "('welcoming', 13575)\n",
            "('welfare', 13576)\n",
            "('well', 13577)\n",
            "('well-ascertained', 13578)\n",
            "('well-born', 13579)\n",
            "('well-defined', 13580)\n",
            "('well-fought', 13581)\n",
            "('well-informed', 13582)\n",
            "('well-known', 13583)\n",
            "('well-peeled', 13584)\n",
            "('well-wooded', 13585)\n",
            "('well[516]', 13586)\n",
            "('wellbeing', 13587)\n",
            "('wellnigh', 13588)\n",
            "('wends', 13589)\n",
            "('went', 13590)\n",
            "('wept', 13591)\n",
            "('were', 13592)\n",
            "('wert', 13593)\n",
            "('west', 13594)\n",
            "('western', 13595)\n",
            "('westwards', 13596)\n",
            "('whale', 13597)\n",
            "('what', 13598)\n",
            "('what[733]', 13599)\n",
            "('whatever', 13600)\n",
            "('whatsoever', 13601)\n",
            "('wheel', 13602)\n",
            "('wheeled', 13603)\n",
            "('wheeling', 13604)\n",
            "('wheels', 13605)\n",
            "('whelmed', 13606)\n",
            "('whelmèd', 13607)\n",
            "('when', 13608)\n",
            "('when[288]', 13609)\n",
            "('whence', 13610)\n",
            "('whene', 13611)\n",
            "('whenever', 13612)\n",
            "('where', 13613)\n",
            "('where[859]', 13614)\n",
            "('whereby', 13615)\n",
            "('wherefore', 13616)\n",
            "('wherein', 13617)\n",
            "('whereon', 13618)\n",
            "('wheresoe', 13619)\n",
            "('whereupon', 13620)\n",
            "('wherever', 13621)\n",
            "('whether', 13622)\n",
            "('which', 13623)\n",
            "('while', 13624)\n",
            "('whilom', 13625)\n",
            "('whip', 13626)\n",
            "('whipped', 13627)\n",
            "('whirling', 13628)\n",
            "('whirls', 13629)\n",
            "('whirlwind', 13630)\n",
            "('whist', 13631)\n",
            "('whistle', 13632)\n",
            "('whit', 13633)\n",
            "('white', 13634)\n",
            "('whither', 13635)\n",
            "('who', 13636)\n",
            "('whoever', 13637)\n",
            "('whole', 13638)\n",
            "('wholeness', 13639)\n",
            "('wholesome', 13640)\n",
            "('wholly', 13641)\n",
            "('whom', 13642)\n",
            "('whom[463]', 13643)\n",
            "('whore', 13644)\n",
            "('whose', 13645)\n",
            "('whoso', 13646)\n",
            "('why', 13647)\n",
            "('wick', 13648)\n",
            "('wicked', 13649)\n",
            "('wickedness', 13650)\n",
            "('wide', 13651)\n",
            "('widely', 13652)\n",
            "('widened', 13653)\n",
            "('widening', 13654)\n",
            "('wider', 13655)\n",
            "('widespread', 13656)\n",
            "('widest', 13657)\n",
            "('widow', 13658)\n",
            "('width', 13659)\n",
            "('wield', 13660)\n",
            "('wielded', 13661)\n",
            "('wife', 13662)\n",
            "('wight', 13663)\n",
            "('wild', 13664)\n",
            "('wilderness', 13665)\n",
            "('wile', 13666)\n",
            "('wiles', 13667)\n",
            "('will', 13668)\n",
            "('willed', 13669)\n",
            "('willed[252]', 13670)\n",
            "('willing', 13671)\n",
            "('willingly', 13672)\n",
            "('willingness', 13673)\n",
            "('wills', 13674)\n",
            "('wilt', 13675)\n",
            "('wily', 13676)\n",
            "('win', 13677)\n",
            "('winces', 13678)\n",
            "('wind', 13679)\n",
            "('windmill', 13680)\n",
            "('window', 13681)\n",
            "('windpipe', 13682)\n",
            "('winds', 13683)\n",
            "('wine', 13684)\n",
            "('wine-bibbers', 13685)\n",
            "('wing', 13686)\n",
            "('winged', 13687)\n",
            "('wings', 13688)\n",
            "('winning', 13689)\n",
            "('wins', 13690)\n",
            "('winter', 13691)\n",
            "('winter-tide', 13692)\n",
            "('winter-time', 13693)\n",
            "('wintry', 13694)\n",
            "('wipe', 13695)\n",
            "('wiped', 13696)\n",
            "('wisdom', 13697)\n",
            "('wise', 13698)\n",
            "('wisely', 13699)\n",
            "('wiser', 13700)\n",
            "('wisest', 13701)\n",
            "('wish', 13702)\n",
            "('wished', 13703)\n",
            "('wishes', 13704)\n",
            "('wishing', 13705)\n",
            "('wit', 13706)\n",
            "('witch', 13707)\n",
            "('with', 13708)\n",
            "('withdraw', 13709)\n",
            "('withdrawn', 13710)\n",
            "('withdrew', 13711)\n",
            "('withes', 13712)\n",
            "('withhold', 13713)\n",
            "('within', 13714)\n",
            "('without', 13715)\n",
            "('withstanding', 13716)\n",
            "('withstood', 13717)\n",
            "('witless', 13718)\n",
            "('witness', 13719)\n",
            "('witnessed', 13720)\n",
            "('witnessing', 13721)\n",
            "('wits', 13722)\n",
            "('witty', 13723)\n",
            "('wives', 13724)\n",
            "('wizards', 13725)\n",
            "('woe', 13726)\n",
            "('woeful', 13727)\n",
            "('woes', 13728)\n",
            "('woful', 13729)\n",
            "('woke', 13730)\n",
            "('wolf', 13731)\n",
            "('wolf-cubs', 13732)\n",
            "('wolves', 13733)\n",
            "('woman', 13734)\n",
            "('woman-like', 13735)\n",
            "('womanhood', 13736)\n",
            "('womankind', 13737)\n",
            "('womanly', 13738)\n",
            "('women', 13739)\n",
            "('won', 13740)\n",
            "('wonder', 13741)\n",
            "('wonderful', 13742)\n",
            "('wondering', 13743)\n",
            "('wonders', 13744)\n",
            "('wondrous', 13745)\n",
            "('wont', 13746)\n",
            "('wonted', 13747)\n",
            "('wood', 13748)\n",
            "('wood[161]', 13749)\n",
            "('wooden', 13750)\n",
            "('woodland', 13751)\n",
            "('woods', 13752)\n",
            "('woody', 13753)\n",
            "('woollens', 13754)\n",
            "('word', 13755)\n",
            "('words', 13756)\n",
            "('words[372]', 13757)\n",
            "('wore', 13758)\n",
            "('work', 13759)\n",
            "('worked', 13760)\n",
            "('worker', 13761)\n",
            "('working', 13762)\n",
            "('workmanship', 13763)\n",
            "('works', 13764)\n",
            "('workshops', 13765)\n",
            "('world', 13766)\n",
            "('world[828]', 13767)\n",
            "('worldly', 13768)\n",
            "('worlds', 13769)\n",
            "('worm', 13770)\n",
            "('worms', 13771)\n",
            "('worn', 13772)\n",
            "('worrying', 13773)\n",
            "('worse', 13774)\n",
            "('worser', 13775)\n",
            "('worship', 13776)\n",
            "('worshipped', 13777)\n",
            "('worst', 13778)\n",
            "('worsted', 13779)\n",
            "('worth', 13780)\n",
            "('worthier', 13781)\n",
            "('worthiest', 13782)\n",
            "('worthiness', 13783)\n",
            "('worthy', 13784)\n",
            "('would', 13785)\n",
            "('wouldest', 13786)\n",
            "('wouldst', 13787)\n",
            "('wound', 13788)\n",
            "('wounded', 13789)\n",
            "('wounds', 13790)\n",
            "('wrapped', 13791)\n",
            "('wrath', 13792)\n",
            "('wrathful', 13793)\n",
            "('wrenched', 13794)\n",
            "('wrested', 13795)\n",
            "('wrestle', 13796)\n",
            "('wretch', 13797)\n",
            "('wretched', 13798)\n",
            "('wretchedness', 13799)\n",
            "('wretches', 13800)\n",
            "('wriggle', 13801)\n",
            "('wrings', 13802)\n",
            "('writ', 13803)\n",
            "('write', 13804)\n",
            "('writer', 13805)\n",
            "('writers', 13806)\n",
            "('writes', 13807)\n",
            "('writhed', 13808)\n",
            "('writhes', 13809)\n",
            "('writhing', 13810)\n",
            "('writing', 13811)\n",
            "('writings', 13812)\n",
            "('written', 13813)\n",
            "('wrong', 13814)\n",
            "('wronged', 13815)\n",
            "('wrote', 13816)\n",
            "('wrought', 13817)\n",
            "('wrung', 13818)\n",
            "('www', 13819)\n",
            "('x', 13820)\n",
            "('xi', 13821)\n",
            "('xii', 13822)\n",
            "('xiii', 13823)\n",
            "('xiii^a', 13824)\n",
            "('xiv', 13825)\n",
            "('xix', 13826)\n",
            "('xv', 13827)\n",
            "('xvi', 13828)\n",
            "('xvii', 13829)\n",
            "('xviii', 13830)\n",
            "('xx', 13831)\n",
            "('xxi', 13832)\n",
            "('xxii', 13833)\n",
            "('xxiii', 13834)\n",
            "('xxiv', 13835)\n",
            "('xxix', 13836)\n",
            "('xxv', 13837)\n",
            "('xxvi', 13838)\n",
            "('xxvii', 13839)\n",
            "('xxviii', 13840)\n",
            "('xxx', 13841)\n",
            "('xxxi', 13842)\n",
            "('xxxii', 13843)\n",
            "('xxxiii', 13844)\n",
            "('xxxiv', 13845)\n",
            "('yards', 13846)\n",
            "('yawned', 13847)\n",
            "('yawning', 13848)\n",
            "('yawns', 13849)\n",
            "('ye', 13850)\n",
            "('yea', 13851)\n",
            "('year', 13852)\n",
            "('year[804]', 13853)\n",
            "('yearned', 13854)\n",
            "('years', 13855)\n",
            "('yelled', 13856)\n",
            "('yellow', 13857)\n",
            "('yells', 13858)\n",
            "('yelp', 13859)\n",
            "('yelping', 13860)\n",
            "('yestermorn', 13861)\n",
            "('yesternight', 13862)\n",
            "('yet', 13863)\n",
            "('yield', 13864)\n",
            "('yielded', 13865)\n",
            "('yieldeth', 13866)\n",
            "('yielding', 13867)\n",
            "('yields', 13868)\n",
            "('yoke', 13869)\n",
            "('yon', 13870)\n",
            "('yonder', 13871)\n",
            "('yore', 13872)\n",
            "('yore[876]', 13873)\n",
            "('you', 13874)\n",
            "('you[362]', 13875)\n",
            "('young', 13876)\n",
            "('younger', 13877)\n",
            "('youngest', 13878)\n",
            "('your', 13879)\n",
            "('yours', 13880)\n",
            "('yourself', 13881)\n",
            "('yourselves', 13882)\n",
            "('youth', 13883)\n",
            "('youthful', 13884)\n",
            "('zabi', 13885)\n",
            "('zeal', 13886)\n",
            "('zeniths', 13887)\n",
            "('zone', 13888)\n",
            "('zone[517]', 13889)\n",
            "('£1500', 13890)\n",
            "('£500', 13891)\n",
            "('Ægean', 13892)\n",
            "('Ægina', 13893)\n",
            "('Ægina[748]', 13894)\n",
            "('Æn', 13895)\n",
            "('Æneas', 13896)\n",
            "('Æneid', 13897)\n",
            "('Æsop', 13898)\n",
            "('‘AS-IS’', 13899)\n",
            "('“Defects', 13900)\n",
            "('“Information', 13901)\n",
            "('“Plain', 13902)\n",
            "('“Project', 13903)\n",
            "('“Right', 13904)\n",
            "('“the', 13905)\n",
            "('”', 13906)\n",
            "('•', 13907)\n",
            "('\\ufeffThe', 13908)\n",
            "('<|endoftext|>', 13909)\n",
            "('<|unk|>', 13910)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Based on the output we can confirm that the special tokens have indeed successfully incorporated into the vocabulary.\n",
        "\n",
        "* Now let's update our tokenizer."
      ],
      "metadata": {
        "id": "BXaHoOmMeGTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer2:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "    preprocessed = [\n",
        "        item.strip() for item in preprocessed if item.strip()\n",
        "    ]\n",
        "    preprocessed = [\n",
        "        item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed\n",
        "    ] #replaces unknown words by <|unk|> tokens\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self,ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r\"\\s+([,.:;?_!\\\"()'])\", r\"\\1\", text) # replaces spaces before the specified punctuation.\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "yPEGuG_see_x"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now test our new tokenizer.\n",
        "* For this we'll use text samples that we concatenate from two different from two independent and unrelated sentences."
      ],
      "metadata": {
        "id": "zcXuEiDQe6n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"In electrical engineering, a transformer is a passive component that transfers electrical energy from one electrical circuit to another circuit, or multiple circuits.\"\n",
        "text2 = \"In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.\"\n",
        "text =  \"<|endoftext|>\".join((text1,text2))\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsTb7BxXfr1x",
        "outputId": "5e0e152b-050b-430f-966d-17f407a6b6c8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In electrical engineering, a transformer is a passive component that transfers electrical energy from one electrical circuit to another circuit, or multiple circuits.<|endoftext|>In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now let's tokenize the sample text using `SimpleTokenizer2`on the vocab we previously created in 2.2"
      ],
      "metadata": {
        "id": "gfdZUYWngSVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizer2(vocab)\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txlPGfsCggpG",
        "outputId": "d7c4281b-e329-46cb-ab1a-dfcde4cbd12a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1751, 13910, 7072, 9, 4316, 13910, 8924, 4316, 13910, 13910, 12704, 13910, 13910, 7065, 7856, 10049, 13910, 5630, 12857, 4647, 5630, 9, 10090, 13910, 5631, 12, 13910, 6382, 9141, 9, 13910, 8924, 4612, 13910, 4986, 10046, 12706, 13910, 4861, 13910, 9, 8672, 13623, 12694, 8924, 6053, 12857, 13910, 11197, 5414, 13910, 9, 4624, 6889, 13910, 8924, 6053, 8870, 4316, 13910, 13910, 13910, 7856, 4316, 13755, 13910, 12584, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can see that the list of token IDs contain `13909` for the `<|endoftext|>` seperator token as well as  several `13910` tokens, which are used for unknown words.\n",
        "\n",
        "\n",
        "* Let's detokenize the text for a quick sanity check."
      ],
      "metadata": {
        "id": "0TsxIRtmgyCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenizer.encode(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXtHikzxhOjv",
        "outputId": "55e49c02-78eb-472b-b2a9-fd1ee0d2c48b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In <|unk|> engineering, a <|unk|> is a <|unk|> <|unk|> that <|unk|> <|unk|> energy from one <|unk|> circuit to another circuit, or <|unk|> circuits. <|unk|> deep learning, <|unk|> is an <|unk|> based on the <|unk|> attention <|unk|>, in which text is converted to <|unk|> representations called <|unk|>, and each <|unk|> is converted into a <|unk|> <|unk|> <|unk|> from a word <|unk|> table.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Based on comparing this detokenized text with the original input text, we know that the training dataset, Dante's Inferno, does not contain the words `electrical,architecture etc`.\n",
        "\n",
        "* Depending on the LLM, researchers also consider additional special tokens such as the following:\n",
        "\n",
        "  * [BOS](beginning of sequence) - This tokens marks the start of a text. It signifies to the LLM where a piece of content begins.\n",
        "  * [EOS] (end of sequence) - This token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts similar to `<|endoftext|>`.\n",
        "  * [PAD] (padding) - When training LLMs with batch sizes larger than one, the batch might contain text of varying lengths.To ensure all text have the same lengths, the shorter texts are extended or `padded`using thr [PAD] token, up to the length of the longest text in the batch."
      ],
      "metadata": {
        "id": "fYpEjKK9hbB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Byte pair encoding"
      ],
      "metadata": {
        "id": "ANSYAfN2jyV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Byte Pair Encoding (BPE) tokenizer was used in training LLMs like GPT-2,GPT-3,and the original model used in ChatGPT."
      ],
      "metadata": {
        "id": "Ldv7HpLNj32x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We first need to download a library that does bpe called `tiktoken` using the code below:"
      ],
      "metadata": {
        "id": "osuh6p8AkPFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu2SFdPXkeBf",
        "outputId": "64107fe9-f3da-4d1b-cfaf-78d2143c615e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Once installed, we instantiate the BPE tokenizer for tiktoken as follows:"
      ],
      "metadata": {
        "id": "DH50n1dTkoHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "EVNh4EkXkxAo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Example usage of tiktoken tokenizer is similar to `SimpleTokenizer2`."
      ],
      "metadata": {
        "id": "DRVjLbUkF64p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello,do you like coding? <|endoftext|> In the nvidia auditorium\"\n",
        "\n",
        "integers = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxkTjcuAGGWf",
        "outputId": "a482d468-be22-46e6-c750-d87eaea62a79"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 4598, 345, 588, 19617, 30, 220, 50256, 554, 262, 299, 21744, 30625, 1505]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can also convert the token IDs back into text using decode method."
      ],
      "metadata": {
        "id": "HbEDYe_IGtuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jDMLF6VG39W",
        "outputId": "b6baf2dc-5151-4a9a-b23f-ebf77a25d43c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello,do you like coding? <|endoftext|> In the nvidia auditorium\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Data sampling with a sliding window"
      ],
      "metadata": {
        "id": "2W9uk58aHlFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The next step in creating the embeddings for the LLM is to generate the input-target pairs which are essential for training an LLM.\n",
        "* LLMs are pretrained by predicting the next word in a text.\n",
        "* Let's implement a dataloader that fetches the input-target pairs from training dataset using a sliding window approach.\n",
        "* To start this process, we will tokenize the whole `Dante's Inferno` story using BPE tokenizer."
      ],
      "metadata": {
        "id": "v0D47l1HHqRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"dante's-inferno.txt\",'r',encoding='utf-8') as f:\n",
        "  raw_text = f.read()\n",
        "encoded_text = tokenizer.encode(raw_text)\n",
        "print(len(encoded_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ski4wo1ZI9pN",
        "outputId": "5396d49e-25d3-4d49-d574-60966a608ee5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "203798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can see that after executing the code above we that our dataset has `203798` tokens.\n",
        "* Now let's remove the first 500 tokens from the dataset for demonstration purposes."
      ],
      "metadata": {
        "id": "8RUxM-nNJaBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sample = encoded_text[500:]"
      ],
      "metadata": {
        "id": "kWgGKGMkJweg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* One of the easiest and most intuitive ways to create input-target pairs for the next-word prediction task is to create 2 variables `x` and `y`,where `x`contains the input tokens and `y` contains the targets,which are inputs shifted by 1:"
      ],
      "metadata": {
        "id": "iBYPC7hJJ_jX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4\n",
        "x = encoded_sample[:context_size]\n",
        "y = encoded_sample[1:context_size+1]\n",
        "print(f\"x:{x}\")\n",
        "print(f\"y:       {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN8lC4kWKfwv",
        "outputId": "d1c4e0db-70e9-4596-f45d-574c47102f3e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:[561, 475, 1165, 198]\n",
            "y:       [475, 1165, 198, 77]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* By processing the inputs along with targets,which are inputs are shifted by one position, we can create the next-word prediction task."
      ],
      "metadata": {
        "id": "aBGTJ1cfK2yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,context_size+1):\n",
        "  context = encoded_sample[:i]\n",
        "  desired = encoded_sample[i]\n",
        "  print(context,\"------>\",desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i15qFrfwLugH",
        "outputId": "0176ee67-735b-40b4-fd1c-ad79e2cfac24"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[561] ------> 475\n",
            "[561, 475] ------> 1165\n",
            "[561, 475, 1165] ------> 198\n",
            "[561, 475, 1165, 198] ------> 77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Everything left to the arrow refers to the input an LLM would receive, and token ID on the right side of arrow represents the target token ID that the LLM is supossed to predict.\n",
        "* Now let's do the reverse where we convert token IDs into text:"
      ],
      "metadata": {
        "id": "Y4jMP1jjMf0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,context_size+1):\n",
        "  context = encoded_sample[:i]\n",
        "  desired = encoded_sample[i]\n",
        "  print(tokenizer.decode(context),\"------>\",tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCUrQbnhNCed",
        "outputId": "419830ff-31ae-4624-f454-0b7b699bfd1b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " would ------>  but\n",
            " would but ------>  too\n",
            " would but too ------> \n",
            "\n",
            " would but too\n",
            " ------> n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We have now create the input-target pairs that we can use for LLM training\n",
        "\n",
        "* There's only one more task before we can turn tokens into embeddings.\n",
        "* That is implementing an efficient dataloader that iterates over the input dataset and returns the inputs ad targets as Pytorch tensor, which can be thought as multidimensional arrays.\n",
        "\n",
        "* For the efficient data loader implementation,we'll use Pytorch's built-in `Dataset` and `DataLoader` classes."
      ],
      "metadata": {
        "id": "T9rr8zeXNn9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class GPTDataset1(Dataset):\n",
        "  def __init__(self,txt,tokenizer,max_length,stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt)#tokenizing the entire text\n",
        "\n",
        "    for i in range(0,len(token_ids)- max_length,stride):#using sliding window to chunk the book into overlapping sequenced of max_length\n",
        "      input_sample = token_ids[i:i + max_length]\n",
        "      target_sample = token_ids[i+1:i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_sample))\n",
        "      self.target_ids.append(torch.tensor(target_sample))\n",
        "\n",
        "  def __len__(self):#returns the total number of rows in the dataset\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self,idx):#returns a single row fromthe dataset\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "EnDP-0lcA6Xq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The class above is based on Pytorch `Dataset` class and defines how individual rows are fetched from the dataset, where each row consists of a number of token IDs (based on a `max_length`) assigned to an `input_sample`tensor.\n",
        "* The `target_chunk ` tenosr contains the corresponding targets.\n",
        "\n"
      ],
      "metadata": {
        "id": "x0YZZMLAC8zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader1(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding('gpt2')#intializing the tokenizer\n",
        "  dataset = GPTDataset1(txt,tokenizer,max_length,stride)#creating the dataset\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last=drop_last,#drop_last= True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training.\n",
        "      num_workers=num_workers#number of CPU processes to use for preprocessing.\n",
        "\n",
        "      )\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "RQ-qeEBJDwua"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's test the `dataloader` with a batch of size 1 for an LLM with a context size of 4 :"
      ],
      "metadata": {
        "id": "vDFrAWeOISzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"dante's-inferno.txt\",'r',encoding='utf-8') as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "dataloader = create_dataloader1(\n",
        "    raw_text,batch_size=1,max_length=4,stride=1,shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc5o_zjnIfqe",
        "outputId": "6f5928f1-af1e-42d0-8182-0e418c406570"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[171, 119, 123, 464]]), tensor([[ 119,  123,  464, 4935]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first_batch variable contains two tensors: the first stores the input token IDs, and the second contains the target token IDs.\n",
        "* Since the `max_length` is set to 4, each of the two tensors contain 4 token Ids.\n",
        "\n",
        "* To understand the meaninng of `stride=1`, let's fetch another batch from this dataset:"
      ],
      "metadata": {
        "id": "OrXWFsBbJTmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fv-waSZJ1N4",
        "outputId": "f19df09f-4ee1-4858-e2f8-3d8d37485c79"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 119,  123,  464, 4935]]), tensor([[  123,   464,  4935, 20336]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* If we compare the first and second batches, we see that the second batch's token IDs are shifted by one position.\n",
        "\n",
        "* Let's try using a batch size that is greater than one:"
      ],
      "metadata": {
        "id": "PVKbqDgeJ77r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader1(\n",
        "    raw_text,batch_size=8,max_length=4,stride=4,\n",
        "    shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\",inputs)\n",
        "print(\"\\nTargets:\\n\",targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyGgdPbTKXky",
        "outputId": "311d215a-fcf4-4527-f0f0-4d32228e198d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[  171,   119,   123,   464],\n",
            "        [ 4935, 20336, 46566,   286],\n",
            "        [  383, 13009, 22329,   286],\n",
            "        [34898,   978,   394, 29864],\n",
            "        [   25,   383, 32458,   198],\n",
            "        [  220,   220,   220,   220],\n",
            "        [  198,  1212, 47179,   318],\n",
            "        [  329,   262,   779,   286]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  119,   123,   464,  4935],\n",
            "        [20336, 46566,   286,   383],\n",
            "        [13009, 22329,   286, 34898],\n",
            "        [  978,   394, 29864,    25],\n",
            "        [  383, 32458,   198,   220],\n",
            "        [  220,   220,   220,   198],\n",
            "        [ 1212, 47179,   318,   329],\n",
            "        [  262,   779,   286,  2687]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Note that we have increased the stride to 4 to utilize the dataset fully.\n",
        "* This avoids any overlapping between the batches since more overlapping could lead to increased overfitting."
      ],
      "metadata": {
        "id": "kaWDOx_rLFQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 Creating token embeddings"
      ],
      "metadata": {
        "id": "kn23Ag9mLaMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The last step in preparing the input text for LLM training is to convert the token IDs into embedding vectors.\n",
        "* A continous vector representation, or embedding, is necessary since GPT-like LLMs are deep neural networks trained with backpropagation algorithm.\n",
        "\n",
        "* Let's try to see how the token ID to embedding vector conversion works with a hands-on example.\n",
        "* Suppose have the following four input tokens with ids 12,15,1,10:"
      ],
      "metadata": {
        "id": "FCn-xBbQLeeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([1,2,3,4,5])"
      ],
      "metadata": {
        "id": "lON6669iMqGx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For simplicity purposes we have a small vocabulary of only 6 words, and we want to create embeddings of size 3(in gpt-3 , the embedding size is 12,288 dimensions):"
      ],
      "metadata": {
        "id": "xam1hjaLMxjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size  = 6\n",
        "output_dim= 3"
      ],
      "metadata": {
        "id": "WqPssTQxNETr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using `vocab_size` and `output_dim`, we can instantiate an embedding layer in Pytorch, setting the random seed to `42` for reproducibility purposes:"
      ],
      "metadata": {
        "id": "9-78OFKKNIqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k1vFMFgNZlS",
        "outputId": "4b0549dd-7933-4374-cc7a-eb36e5240e5b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 1.9269,  1.4873, -0.4974],\n",
            "        [ 0.4396, -0.7581,  1.0783],\n",
            "        [ 0.8008,  1.6806,  0.3559],\n",
            "        [-0.6866,  0.6105,  1.3347],\n",
            "        [-0.2316,  0.0418, -0.2516],\n",
            "        [ 0.8599, -0.3097, -0.3957]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We see that the print statements prints the embedding layer's underlying weight matrix.\n",
        "* The weight matrix of the embedding layer contains random values. These values are optimized during LLM training as part of the LLM  optimization itself.\n",
        "* We can also that the weight matrix has six rows and 3 columns.\n",
        "* There is one row for each of the six possible tokens in the vocabulary, and there is one column for each of the three embedding dimensions.\n",
        "* Now let's test it using a token ID to obtain the embedding vector:"
      ],
      "metadata": {
        "id": "Na0dY0EBNsRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([5])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VHn7bzBOnX7",
        "outputId": "7ee4b98b-1cee-4252-8c31-73929c87bb3a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.8599, -0.3097, -0.3957]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We have seen how to convert a single token ID into a three-dimensional embedding vector.Let'snow apply it to all the four input ids"
      ],
      "metadata": {
        "id": "9cSOZJvgPaYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfolBvH7QIwA",
        "outputId": "94c05733-d629-4386-8853-316be77546a3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4396, -0.7581,  1.0783],\n",
            "        [ 0.8008,  1.6806,  0.3559],\n",
            "        [-0.6866,  0.6105,  1.3347],\n",
            "        [-0.2316,  0.0418, -0.2516],\n",
            "        [ 0.8599, -0.3097, -0.3957]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 Encoding word positions"
      ],
      "metadata": {
        "id": "_0Xj5vOSQQZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In general, token embeddings are suitable for an input for an LLM.\n",
        "* However, a minor shortcoming of LLMs is that their self-attention mechanism(see section 3) does not have a notion of position or order for the tokens within a sequence.\n",
        "\n",
        "* The way the previously introduced embedding layer works is that the same token ID always gets mapped to the same vector representation, regardless of where the token ID is positioned in the input sequence.\n",
        "* The deterministic,position-independent embedding of the token ID is good for reproducibility purposes.\n",
        "* However, since the self-attention mechanism of LLMs itself is also position-agnostic, it is helpful to inject additional position infromation into the LLM.\n",
        "* To acheive this, we can use two broad categories of position aware embeddings they are:\n",
        "  * Relative positional embeddings.\n",
        "  * Absolute positional embeddings.\n",
        "* Absolute postional embeddings are directly associated with specific postions in a sequence.For each position in the input sequence, a unique embedding is added to the token's embedding to convey its exact location.\n",
        "* These embeddings are added to the token embeddings before feeding into the transformer.\n",
        "\n",
        "* Relative postional embeddings - Instead of encoding absolute positions, it encodes relative distances between tokens.\n",
        "* For example, the model learns what it means for one token to be two positions before another, rather than being at position 5 or 10.\n",
        "\n",
        "* OpenAI's GPT models use absolute positional embeddings that are optimized during the training process rather than being fixed or predefined like the postional encodings in the original transformer model."
      ],
      "metadata": {
        "id": "R8WsBKgkQUq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Previously we focused on very small embedding sizes for simplicity.\n",
        "* Let's now consider more realistic and useful embedding sizes and encode the input token into a 256-dimensional vector representation, which is smaller than ChatGPT models these days.\n",
        "* Furthermore, we assume that the token IDs were created by the BPE tokenizer we implemented earlier, which has a vocabulary size of 50,257:"
      ],
      "metadata": {
        "id": "4U4XSJ7UNn6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
      ],
      "metadata": {
        "id": "-dUOSjoNP7sn"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using the previous `token_embedding_layer`, if we sample data from the data loader, we embed each token in each batch into a 256-dimensional vector.\n",
        "* If we have a batch size of 8 with four tokens each, the result will be an 8x4x256 tensor.\n",
        "* Let's instantiate the dataloader we have previously created:"
      ],
      "metadata": {
        "id": "s3Fp0hAjQJ4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader1(\n",
        "    raw_text,batch_size=8, max_length=max_length,\n",
        "    stride=max_length,shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Token IDs:\\n\",inputs)\n",
        "print(\"\\nInputs shape:\\n\",inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bXvWqFmQ6EW",
        "outputId": "0bbc7d1c-2acf-48de-f5e3-b465bb09bc46"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[  171,   119,   123,   464],\n",
            "        [ 4935, 20336, 46566,   286],\n",
            "        [  383, 13009, 22329,   286],\n",
            "        [34898,   978,   394, 29864],\n",
            "        [   25,   383, 32458,   198],\n",
            "        [  220,   220,   220,   220],\n",
            "        [  198,  1212, 47179,   318],\n",
            "        [  329,   262,   779,   286]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can see, the token ID tensor is 8 x 4 dimensional, meaning that the data batch consists of eight text samples with four tokens each.\n",
        "\n",
        "* Let's now use the embedding layer to embed these token IDs into 256-dimensional vectors:"
      ],
      "metadata": {
        "id": "7VF-uXPIRhX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQIz3sJ6SMSU",
        "outputId": "47815477-bb69-4920-d489-e53e26f83fd8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The 8 x 4 x 256-dimensional tensor output shows that each token ID is now embedded as a 256-dimensional vector:\n",
        "\n",
        "* For a GPT model's absolute embedding approach, we just need to create another embedding layer that has the same embedding dimension as the `token_embedding_layer`:"
      ],
      "metadata": {
        "id": "k7uyA2srSZqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length,output_dim)\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehzK6A_MTnKJ",
        "outputId": "39fd4480-06c8-455c-bd66-5af817933179"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The input to the `pos_embeddings` is usually a placeholder vector `torch.arange(context_length)`,which contains a sequence of numbers 0,1,2..up to the maximum input length -1.\n",
        "* The `context_length` is a variable that represents the supported input size of the LLM.\n",
        "\n",
        "* As we can see, the positional embedding tensor consists of four 256-dimensional vectors.\n",
        "* We can now add these directly to the token embeddings, where Pytorch will add the 4 x 256- dimensional `pos_embeddings` tensor to eacg 4 x 256-dimensional token embedding tensor in each eight batchs:"
      ],
      "metadata": {
        "id": "OnAVAbDyU_io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmUbC4lBV2xA",
        "outputId": "245f4d37-bf84-4a35-8e49-492cf6e70d3b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The embeddings are typically added (not concatenated) because:\n",
        "  * `Efficiency`: Addition maintains the same dimensionality, keeping the model size manageable.\n",
        "\n",
        "  * `Information integration`: The model learns to encode both semantic content (what the token means) and positional context (where it appears) in the same representational space\n",
        "\n",
        "  * `Flexible learning`: During training, the model can learn how semantic and positional information should interact\n",
        "\n"
      ],
      "metadata": {
        "id": "O-4XIfTnXc25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Hands programming attention mechanisms\n",
        "\n",
        "* Now, we will look at an integral part of the LLM architecture itself , this is the attentional mechanisms in isolation and focus on them at a mechanistic level."
      ],
      "metadata": {
        "id": "geOrTQEQX-6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 The problem with modelling long sequences.\n",
        "* Before we dive into the `self-attention` mechanism at the core of LLMs, let's consider the probelm with pre-LLM architectures that do not include attention mechanisms.\n",
        "\n",
        "# Problems with Pre-LLM Architectures (No Attention)\n",
        "\n",
        "## 1. Poor Long-Range Dependency Modeling\n",
        "- RNNs process input **sequentially**, updating a hidden state as they go.\n",
        "- Earlier inputs must be remembered **only through this hidden state**, which gets \"overwritten\" by new tokens.\n",
        "- Result: **Hard to retain information** from 100s or 1000s of tokens back.\n",
        "- Even with LSTMs/GRUs, longer sequences still suffer from **context loss**.\n",
        "\n",
        "> **Example:** In a paragraph, remembering a subject introduced in sentence one to resolve a pronoun in sentence four is often unreliable.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. No Parallelization – Slow Training\n",
        "- RNNs process tokens **one at a time** → inherently **sequential computation**.\n",
        "- No parallelization across time steps like in attention-based models.\n",
        "- **Result:** Slow training and inference, especially for long sequences.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Fixed Memory Bottleneck\n",
        "- The hidden state is a **fixed-size vector** (e.g., 512 dimensions), regardless of the input sequence length.\n",
        "- All information (from 5 tokens or 500) must be **compressed** into this single vector.\n",
        "- Leads to **information loss** and capacity issues.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Gradient Problems\n",
        "During backpropagation through time (BPTT), gradients can:\n",
        "\n",
        "- **Vanish:** fail to update earlier layers.\n",
        "- **Explode:** become numerically unstable.\n",
        "\n",
        "> This makes training **unstable** and often leads to poor convergence.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. No Explicit Position or Focus\n",
        "- RNNs have no mechanism to **selectively focus** on specific past tokens.\n",
        "- They treat all past inputs **equally** via the hidden state.\n",
        "- This leads to:\n",
        "  - **Poor resolution of co-reference**\n",
        "  - Inability to model relationships between **distant tokens**\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Harder to Interpret\n",
        "- There's no explicit signal for **which part of the input** the model is attending to.\n",
        "- Unlike attention (which provides interpretable weights), **RNN hidden states are opaque**.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Less Transferable Representations\n",
        "- RNNs trained on one task **don’t easily adapt** to other tasks or domains.\n",
        "- Attention models (e.g., Transformers) learn better **contextual embeddings**, which are more **transferable** and general-purpose (e.g., BERT, GPT).\n"
      ],
      "metadata": {
        "id": "gXO5ykLJY5FL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Attending to different parts of the input with self-attention."
      ],
      "metadata": {
        "id": "FBHYRzB3E45a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We will now cover the inner workings of the self-attention mechanism and learn how to code it from ground up.\n",
        "* In self-attetion, the `self` refers to the mechanism's ability to compute attention weights by relating different positions with a single input sequence."
      ],
      "metadata": {
        "id": "9sCb9sJZFET4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 A simple self-attention mechanism without trainable weights"
      ],
      "metadata": {
        "id": "8HsCJjXUF9Yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's begin by implementing a simplified variant of self-attention,free from trainable weights.\n",
        "* The goal of self-attention is to compute a `context vector` for each input element that combines information from all other input.\n",
        "* For example purposes consider an input text like \"You are on a mission\".In this case, each element of the sequence, such as `x(1)`, corresponds to a d-dimensional embedding representing a specific token, like `You`.\n",
        "* In self-attention, our goals is to calculate context vectors `z(i)` for each element `x(i)` in the input sequence.\n",
        "* To illustrate this concept, let's focus on the embedding vector of the second input element, `x(2)` (which corresponds to the token `are`), and the corresponding context vector, `z(2)`.\n",
        "* This enhanced context vector, `z(2)`, is an embedding that contains information about `x(2) ` and all other input elements, `x(1) to x(T)`.\n",
        "* Context vectors play a crucial role in self-attention in whereby they create enriched representations of each element in an input sequence(like a sentence) by incorporating information from all other elements in the sequence.\n",
        "* Now let's try a little bit of code:\n"
      ],
      "metadata": {
        "id": "MAEi1X71IcdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "inputs = torch.tensor(\n",
        "    [\n",
        "        [0.43,0.15,0.88],#You (x^1)\n",
        "        [0.45,0.87,0.65],#are (x^2)\n",
        "        [0.57,0.85,0.64],#on (x^3)\n",
        "        [0.22,0.58,0.33],#a (x^4)\n",
        "        [0.77,0.25,0.10]# mission (x^5)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "V27OekHBPB7Z"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first step of implementing self-attention is to compute the intermediate values `w`, referred to attention scores.\n",
        "* We determine these scores by computing the dott product of the query, `x(2)`, with every other input token:"
      ],
      "metadata": {
        "id": "rpckSWRjP6Pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1] # the second input serves as the query.\n",
        "attn_scores2 = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "  attn_scores2[i] = torch.dot(x_i,query)\n",
        "print(attn_scores2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOrc30t8UAlp",
        "outputId": "20588210-e76c-4266-ce1b-60090775fdb3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.8960, 1.3819, 1.4120, 0.8181, 0.6290])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The next step is to normalize each of the attention scores we computed previously.\n",
        "* The main goal behind normalization is to obtain attention weights that sum up to 1."
      ],
      "metadata": {
        "id": "9MpQ_06_UiGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores2_tmp = attn_scores2 / attn_scores2.sum()\n",
        "print(\"Attention weights:\",attn_scores2_tmp)\n",
        "print(\"Sum:\",attn_scores2_tmp.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnZ04LEDU2G_",
        "outputId": "2f256e70-caa5-4051-9e15-8ae8d9d813c4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1744, 0.2690, 0.2749, 0.1593, 0.1224])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In practice it's more common and advisable to use the softmax function for normalization."
      ],
      "metadata": {
        "id": "MSZDYTNWVN2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_function(x):\n",
        "  return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "attn_weights2_func = softmax_function(attn_scores2)\n",
        "print(\"Attention weights:\",attn_weights2_func)\n",
        "print(\"Sum:\",attn_weights2_func.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB9Qz5tSo3Bm",
        "outputId": "3c5c50de-8dc8-4011-d035-ee063b048ce9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1669, 0.2713, 0.2796, 0.1544, 0.1278])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The best option is use Pytorch implementation of softmax, which has been extensively optimized for performance:"
      ],
      "metadata": {
        "id": "zDDWaklepXWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights2 = torch.softmax(attn_scores2,dim=0)\n",
        "print(\"Attention weigths:\",attn_weights2)\n",
        "print(\"Sum:\",attn_weights2.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3U-gXD8pjEO",
        "outputId": "953e9f35-4125-45e1-9426-d3665ee48513"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weigths: tensor([0.1669, 0.2713, 0.2796, 0.1544, 0.1278])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now that we have calculated the normalized attention weights, we are ready for the final step which is calculating the context vectors `z(2)` by multiplying embedded input tokens, `x(i)`,with the corresponding attention weights and summing the resulting vectors.\n",
        "* So basically, the context vector `z(2)` is the weighted sum of all input vectors, obtained by multiplying each input vector by it's corresponding attention weight:"
      ],
      "metadata": {
        "id": "ZkYV61gNp5FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1]#the second input is the query\n",
        "context_vec2 = torch.zeros(query.shape)\n",
        "for i , x_i in enumerate(inputs):\n",
        "  context_vec2 += attn_weights2[i]*x_i\n",
        "print(context_vec2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRn4KOqDqqcS",
        "outputId": "7af2780b-d513-4e10-8148-e6648f6bf70c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4856, 0.6202, 0.5659])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3.1 Computing attention weights for all input tokens\n"
      ],
      "metadata": {
        "id": "4hB0rRhnrIlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* So far we have computed attention weights and context vector for input 2.\n",
        "* Now let's extend this computation to calculate attention weights and context vectors for all input.\n",
        "* We will follow the three steps below:\n",
        "   * Compute attention scores - Compute the attention scores as dot products between the inputs.\n",
        "   * Compute attention weights - the attention weights are normalized version of attention scores.\n",
        "   * Compute context vectors - the context vectors are computed as weighted sum over the inputs"
      ],
      "metadata": {
        "id": "LJaAhY4IrWVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = torch.empty(6,6)\n",
        "for i, x_i in enumerate(inputs):\n",
        "  for j, x_j in enumerate(inputs):\n",
        "    attn_scores[i,j] = torch.dot(x_i,x_j)\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhkOWuAXszdi",
        "outputId": "cdd89685-ea74-406b-ab94-05c86ba93597"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[9.8180e-01, 8.9600e-01, 9.3580e-01, 4.7200e-01, 4.5660e-01, 1.6876e-07],\n",
            "        [8.9600e-01, 1.3819e+00, 1.4120e+00, 8.1810e-01, 6.2900e-01, 8.4034e+20],\n",
            "        [9.3580e-01, 1.4120e+00, 1.4570e+00, 8.2960e-01, 7.1540e-01, 2.1715e-18],\n",
            "        [4.7200e-01, 8.1810e-01, 8.2960e-01, 4.9370e-01, 3.4740e-01, 4.7851e+22],\n",
            "        [4.5660e-01, 6.2900e-01, 7.1540e-01, 3.4740e-01, 6.6540e-01, 1.9971e+20],\n",
            "        [1.0559e-08, 1.6597e-07, 2.4827e-18, 3.1360e+27, 7.0800e+31, 3.1095e-18]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Each element in the tenosr above represents an attention score between each pair of inputs.\n",
        "* Note that these tensors are unnormalized, we will deal with normalization later.\n",
        "\n",
        "* When computing the preceding attention tensor we used `for` loops.\n",
        "* However `for` loops are generally slow, and we can achieve the same results using matrix multiplication (matmul)."
      ],
      "metadata": {
        "id": "lDGmLOdwuANp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = inputs @ inputs.T\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WydvvXeyusys",
        "outputId": "6dde6807-9102-4b71-e7e7-b0e3b6643087"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9818, 0.8960, 0.9358, 0.4720, 0.4566],\n",
            "        [0.8960, 1.3819, 1.4120, 0.8181, 0.6290],\n",
            "        [0.9358, 1.4120, 1.4570, 0.8296, 0.7154],\n",
            "        [0.4720, 0.8181, 0.8296, 0.4937, 0.3474],\n",
            "        [0.4566, 0.6290, 0.7154, 0.3474, 0.6654]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we normalize each row so that the values in each row sum to 1:"
      ],
      "metadata": {
        "id": "CbzqVHpmu8W6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(attn_scores,dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmJ8PYYlvDkY",
        "outputId": "e638944a-eeeb-423f-e657-4078b8db5bba"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2460, 0.2258, 0.2350, 0.1478, 0.1455],\n",
            "        [0.1669, 0.2713, 0.2796, 0.1544, 0.1278],\n",
            "        [0.1668, 0.2685, 0.2809, 0.1500, 0.1338],\n",
            "        [0.1740, 0.2459, 0.2488, 0.1778, 0.1536],\n",
            "        [0.1782, 0.2117, 0.2308, 0.1597, 0.2196]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In Pytorch, the dim parameter in functions like `torch.softmax` specifies the dimension of the input tensor along which the function will be computed.\n",
        "* By setting `dim =-1`, we are instructing the `softmax` function to apply the normalization along the last dimension of the `att_scores` tensor."
      ],
      "metadata": {
        "id": "3PD1DocJwbNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The third and final step is to use these attention weigths to compute all context vectors via matrix multiplication:"
      ],
      "metadata": {
        "id": "9P7iOCpOxY7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_context_vecs = attn_weights @ inputs\n",
        "print(all_context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ty2N6aN7xmuf",
        "outputId": "6084c1c8-a255-4c58-c71b-0e53c8bf1e5e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4858, 0.5551, 0.5769],\n",
            "        [0.4856, 0.6202, 0.5659],\n",
            "        [0.4887, 0.6178, 0.5640],\n",
            "        [0.4846, 0.5930, 0.5462],\n",
            "        [0.5076, 0.5546, 0.5168]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can double-check that code is correct by comparing the second row with the context vector `z(2)` that we computed previously."
      ],
      "metadata": {
        "id": "94C4d0E8yFZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Previously 2nd context vector:\",context_vec2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk7gLfbOyWxz",
        "outputId": "68303df6-9d0b-4721-f8a3-f81b34bc36e1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previously 2nd context vector: tensor([0.4856, 0.6202, 0.5659])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can see that the previosly calculated `context_vec2` matches the second row in the previous tensor exactly."
      ],
      "metadata": {
        "id": "ZnWxdIofrb0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Self-attention with trainable weights."
      ],
      "metadata": {
        "id": "eDCpLHsXtFbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We will implement self-attention mechanism step by step by introducing the following trainable weight matrices:\n",
        "  * `Wq`\n",
        "  * `Wk`\n",
        "  * `Wv`\n",
        "* These three matrices are used to project the embedded input tokens, `x(i)`, into query, key and value vectors.\n",
        "* Previously we defined the second input element `x(2)` as the query when we computed the simplified attention weigths to compute the context vector `z(2)`. Later we generalized this to compute all context vectors `z(1)....z(T)` for the six-word input sentence `You are on a mission`.\n",
        "* Similary, we start by computing only one context vector, `z(2)` for example usecases.\n",
        "* We will then modify this code to calculate all context vectors.\n",
        "* Let's start defining a few variables:"
      ],
      "metadata": {
        "id": "sqiaSjP4uELX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = inputs[1]# second input element\n",
        "d_in = inputs.shape[1] #input embedding size, d = 3\n",
        "d_out = 2 #output embedding size, d_out =2"
      ],
      "metadata": {
        "id": "ZWo2_54kvmsY"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Note that in GPT-like models, the input and output dimensions are usually the same, but to better follow, we'll use different input `d_in = 3` and output `d_out=2` dimension here.\n",
        "\n",
        "* Next, we initialize the three weight matrices `Wq,Wk,Wv`"
      ],
      "metadata": {
        "id": "rh2G5orJwA83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "Wq = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
        "Wk = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
        "Wv = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)"
      ],
      "metadata": {
        "id": "f-u-Lrtiwe13"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We have set `requires_grad=False` to reduce clutter in the outputs, but if we were to use the weight matrices for model training, we would instead set `requires_grad=True` to update these matrices during model training.\n",
        "* Next, we compute the query, key and value vectors:"
      ],
      "metadata": {
        "id": "cVGyjWMdxSvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = x2 @ Wq\n",
        "key2 = x2 @ Wk\n",
        "value2 = x2 @ Wv\n",
        "print(query2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp3XFnKexyF7",
        "outputId": "d710590e-a0aa-484a-cabc-a196bccd618f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9839, 1.6369])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The output for the query results in a two-dimensional vector since we previously set the number of columns of the corresponding weight matrix, via `d_out`, to 2.\n",
        "\n",
        "* Even though our temporary goal is only to compute the one context vector, `z(2)`, we still require the key value and value vectors for all input elements as they are involved in computing the attention weigths with respect to the query `q(2)`.\n",
        "\n",
        "* We can obtain all keys and vlues via matrix multiplication:"
      ],
      "metadata": {
        "id": "mYTpwT4A04z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @ Wk\n",
        "values = inputs @ Wv\n",
        "print(\"keys.shape:\",keys.shape)\n",
        "print(\"values.shape\",values.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf4UqvL__cRS",
        "outputId": "f725591e-8603-49e3-9978-b95f0b5a7a19"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys.shape: torch.Size([5, 2])\n",
            "values.shape torch.Size([5, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As we can tell from the outputs, we successfully projected the five input token from a three-dimensional onto a two-dimensioanl embedding space.\n",
        "*The second step is to compute the attention scores.\n",
        "* Let's compute the attention score `w22`:"
      ],
      "metadata": {
        "id": "YydaJZAT_uXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys2 = keys[1]\n",
        "attn_scores22 = query2.dot(keys2)\n",
        "print(attn_scores22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCYXqOPXBB6R",
        "outputId": "361597aa-4af2-40a0-dcc5-f813e2b5e5b7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.9225)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We see the result is an unnormalized attention score."
      ],
      "metadata": {
        "id": "5u4LcjT6BmTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Again, we can generalize this computation to all attention scores via matrix multiplication:"
      ],
      "metadata": {
        "id": "tm0ma-NNBT6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores2 = query2 @ keys.T\n",
        "print(attn_scores2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFtYBS6oASnh",
        "outputId": "b72a49aa-4231-4ce1-923f-dab4f7ac1ab1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.5030, 2.9225, 3.0669, 1.6288, 1.6697])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we want to compute the attention weights by scaling the attention scores and using the softmax function.\n",
        "* However, we scale the attention scores by dividing them by the square root of the embedding dimension of the keys (that is taking the square root is mathematically the same as exponentiating by 0.5):"
      ],
      "metadata": {
        "id": "LAxNetcSB7N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dk = keys.shape[-1]\n",
        "attn_weights2 = torch.softmax(attn_scores2/dk**0.5,dim=-1)\n",
        "print(attn_weights2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdL7TufXC1Jf",
        "outputId": "356fe8d7-4d1e-4bc1-99c0-7763df6c0bc1"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2029, 0.2729, 0.3023, 0.1093, 0.1126])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we compute the context vectors"
      ],
      "metadata": {
        "id": "9jy3wjIRDKj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec2 = attn_weights2 @ values\n",
        "print(context_vec2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oByvbL-xDka2",
        "outputId": "55425c4b-fc75-40ef-9a7c-25cc731bebad"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.4139, 0.8871])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* So far we have only computed a single context vector, `z(2)`.\n",
        "* Next, we will generalize the code to compute all context vectors in the input sequence, `z(1) to z(t)`."
      ],
      "metadata": {
        "id": "6NuLmvqnDyAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In transformers, the query (Q) represents what you're looking for, the key (K) represents what each input token offers, and the value (V) contains the actual information to retrieve. The model compares each query to all keys using dot products to calculate attention scores, determining how much focus each token deserves. Then, it uses those scores to weigh the values and produce a context-aware output for each token."
      ],
      "metadata": {
        "id": "dHRfTRJctN6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 implementing a compact self-attention Python class"
      ],
      "metadata": {
        "id": "dvH2cwanttPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class SelfAttention1(nn.Module):\n",
        "  def __init__(self,d_in,d_out):\n",
        "    super().__init__()\n",
        "    self.Wq = nn.Parameter(torch.rand(d_in,d_out))\n",
        "    self.Wk = nn.Parameter(torch.rand(d_in,d_out))\n",
        "    self.Wv = nn.Parameter(torch.rand(d_in,d_out))\n",
        "\n",
        "  def forward(self,x):\n",
        "    keys = x @ self.Wk\n",
        "    queries = x @ self.Wq\n",
        "    values = x @ self.Wv\n",
        "    attn_scores =queries @ keys.T\n",
        "    attn_weigths = torch.softmax(\n",
        "        attn_scores / keys.shape[-1] ** 0.5, dim=-1\n",
        "    )\n",
        "    context_vec = attn_weigths @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "scyemOM5ty-I"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's initialize this class as follows:"
      ],
      "metadata": {
        "id": "LQBsCwTru_Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "sa1 = SelfAttention1(d_in,d_out)\n",
        "print(sa1(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibSazunTvETZ",
        "outputId": "d8483a00-ccd3-4ebf-f3eb-0920e22b0806"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2917, 0.7996],\n",
            "        [0.2983, 0.8155],\n",
            "        [0.2991, 0.8176],\n",
            "        [0.2863, 0.7865],\n",
            "        [0.2840, 0.7809]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Since `inputs` contain five embedding vectors, this reslut in matrix storing five context vectors.\n",
        "\n",
        "* Self-attention involves the trainable weigtht matrices `Wq,Wk and Wq`.\n",
        "* These matrices transform input data into queries,keys and values, respectively, which are  crucial components of attention mechanism.\n",
        "* We can improve `selfattention1` implementation further by utilizing Pytorch's `nn.Linear` layers, which effectively perfrom matrix multiplication when bias units are disabled."
      ],
      "metadata": {
        "id": "nz9R88huvlcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention2(nn.Module):\n",
        "  def __init__(self,d_in,d_out,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.Wq = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.Wk = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.Wv = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "\n",
        "  def forward(self,x):\n",
        "    keys = self.Wk(x)\n",
        "    queries = self.Wq(x)\n",
        "    values = self.Wv(x)\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights= torch.softmax(\n",
        "        attn_scores / keys.shape[-1]** 0.5,dim=-1    )\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "G6gZVabaw49Q"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##instantiating selfattention2\n",
        "torch.manual_seed(789)\n",
        "sa2 = SelfAttention2(d_in,d_out)\n",
        "print(sa2(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg8nyU6H1HDp",
        "outputId": "b643c0f9-7778-4846-8ae8-be9ba6781f29"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0538,  0.1025],\n",
            "        [-0.0553,  0.1005],\n",
            "        [-0.0553,  0.1004],\n",
            "        [-0.0554,  0.1006],\n",
            "        [-0.0555,  0.1006]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Hiding future words with casual attention\n",
        "\n",
        "# 🧠 Masked Attention (in Transformers)\n",
        "\n",
        "## Definition\n",
        "**Masked attention** is a technique used in attention mechanisms—especially in Transformers—to prevent a model from accessing certain positions in a sequence, typically **future tokens**, during training.\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠️ Why It's Used\n",
        "- In **language modeling**, we want to predict the next word using only the **previous words**, not future ones.\n",
        "- Masking ensures **causality**: output at position *t* only depends on positions ≤ *t*.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 How It Works\n",
        "- A **mask matrix** (usually filled with `-inf` or `-1e9`) is added to the attention scores **before** applying softmax.\n",
        "- This forces the softmax to assign **zero probability** to masked (future) positions.\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 Example\n",
        "In GPT-style models (decoder-only Transformers), during training:\n",
        "\n",
        "- Input: The cat sat on\n",
        "- Target: cat sat on the\n",
        "\n",
        "\n",
        "At position 3 (`sat`), it can only attend to: `The`, `cat`.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Key Points\n",
        "- Ensures **autoregressive behavior** (left-to-right generation).\n",
        "- Used in **decoder** blocks, not **encoder** blocks (encoders see the full input).\n",
        "- Implemented via **triangular masks** in code (e.g., using `torch.tril()` in PyTorch).\n",
        "\n"
      ],
      "metadata": {
        "id": "H2UX97jseuUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5.1 implementing casual attention\n",
        "* First, we compute the attention weights using the softmax function."
      ],
      "metadata": {
        "id": "s-17WRcSRD_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = sa2.Wq(inputs) # reuses the query and key weight matrices of SelfAttention2 object from previous section for convenience.\n",
        "keys = sa2.Wk(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weigths = torch.softmax(attn_scores / keys.shape[-1]**0.5,dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbz1sLxwRxNr",
        "outputId": "90ea095b-1372-4a1d-c129-dbfc55c81e98"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2460, 0.2258, 0.2350, 0.1478, 0.1455],\n",
            "        [0.1669, 0.2713, 0.2796, 0.1544, 0.1278],\n",
            "        [0.1668, 0.2685, 0.2809, 0.1500, 0.1338],\n",
            "        [0.1740, 0.2459, 0.2488, 0.1778, 0.1536],\n",
            "        [0.1782, 0.2117, 0.2308, 0.1597, 0.2196]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we implement using Pytorch's `tril` function to create a mask where the values above the diagonal are zero:"
      ],
      "metadata": {
        "id": "h29s1ibzSfD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = attn_scores.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length,context_length))\n",
        "print(mask_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5er4KvP9S461",
        "outputId": "098d4ecd-13db-4988-a0eb-12f712614caa"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now we can multiply this mask with attn_weights to zero-out the values above the diagonal:"
      ],
      "metadata": {
        "id": "6x0zgBg6TJC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masked_simple = attn_weights * mask_simple\n",
        "print(masked_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laGf_IWgTSeA",
        "outputId": "6d90b2dc-478d-442e-d4b4-f8915cd2c41c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2460, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1669, 0.2713, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1668, 0.2685, 0.2809, 0.0000, 0.0000],\n",
            "        [0.1740, 0.2459, 0.2488, 0.1778, 0.0000],\n",
            "        [0.1782, 0.2117, 0.2308, 0.1597, 0.2196]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we renormalized the attention weights to sum up to 1 againn in eachr row. We achieve this by dividingeach element in each row by sum in each row:"
      ],
      "metadata": {
        "id": "QOskvjlpTbhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_sums = masked_simple.sum(dim=-1,keepdim=True)\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "print(masked_simple_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxXw7YmRTttx",
        "outputId": "c1daa1f4-8714-4b54-f8ce-5c7640e824fd"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3809, 0.6191, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2329, 0.3749, 0.3922, 0.0000, 0.0000],\n",
            "        [0.2055, 0.2905, 0.2939, 0.2100, 0.0000],\n",
            "        [0.1782, 0.2117, 0.2308, 0.1597, 0.2196]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The result is an attention weight matrix where the attention weights above the diagonal are zeroed-out, and rows sum to 1.\n",
        "* We can still improve our casual attention implementation.\n",
        "* Let's take a mathematical property of the softmax function and implement the computation of the masked attention weights more efficiently. Whereby the softmax function converts its inputs into a probability distribution.When negative infinity values are present in a row, the softmax fuction treats them as zero probability.\n",
        "\n",
        "* We can implement this more efficiently masking `trick` by creating a mask with 1s above the diagonal and then replacing these 1s with negative infinity(-inf) values:"
      ],
      "metadata": {
        "id": "pmvLc0W3UGaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(),-torch.inf)\n",
        "print(masked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35SULqgxYJzF",
        "outputId": "94038f17-865d-463d-dbf7-ccb5034e074d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2853,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4427, 0.1349,   -inf,   -inf,   -inf],\n",
            "        [0.4551, 0.1390, 0.1731,   -inf,   -inf],\n",
            "        [0.2616, 0.0850, 0.1036, 0.0186,   -inf],\n",
            "        [0.2162, 0.0734, 0.0882, 0.0177, 0.0786]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now all we need to do is apply the softmax function to these masked results:"
      ],
      "metadata": {
        "id": "ggp2V8ZncxEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(masked/ keys.shape[-1]**0.5,dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BB9xTN0c6OP",
        "outputId": "f6a271d9-e5a2-4c64-cb2a-a253b6529547"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5542, 0.4458, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3818, 0.3054, 0.3128, 0.0000, 0.0000],\n",
            "        [0.2763, 0.2439, 0.2471, 0.2327, 0.0000],\n",
            "        [0.2177, 0.1968, 0.1988, 0.1892, 0.1975]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can see based on the output, the values in each row sum to 1, and no further normalization is necessary."
      ],
      "metadata": {
        "id": "jryrYCiidIRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5.2 Masking additional attention weights with dropout.\n",
        "* Dropout is a regularization technique in deep learning designed to prevent overfitting and improve model generalization.\n",
        "* It's a technique whereby randomly selected hidden layer units are ignored during training, effectively `dropping` them out.\n",
        "\n",
        "* In the transformer architecture, including GPT-like models, dropout in the attention mechanism is typically applied at two specific times: after calculating the attention weights or after applying the attention weights.\n",
        "\n",
        "* In the following code exmaple, we sue a dropout rate of 50%, which means masking out half of the attention weights.\n",
        "* We apply Pytorch's dropout implementation first to a 6 x 6 tensor consisting of 1s for simplicity:\n",
        "\n"
      ],
      "metadata": {
        "id": "3HmBM9Reda55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5)# choosing a dropout rate of 50%\n",
        "dummy = torch.ones(6,6)#creating a matrix of 1s\n",
        "print(dropout(dummy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkvE7ENahwlP",
        "outputId": "dad8ad1a-b97c-48f8-8143-722cc1aa6186"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2., 2., 2., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.],\n",
            "        [0., 0., 2., 0., 2., 0.],\n",
            "        [2., 2., 0., 0., 0., 2.],\n",
            "        [2., 0., 0., 0., 0., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* When applying dropout to an attention weight matrix witha rate of 50%, half of the elements in the matrix are randomly set to zero.\n",
        "* To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a factor of `1/0.5 = 2`.\n",
        "* This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains consistent during both training and inference phases.\n",
        "\n",
        "* Next let's apply dropout to the attention weight matrix itself."
      ],
      "metadata": {
        "id": "YiJ1YtciiJjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "print(dropout(attn_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uia8Q2jijCjD",
        "outputId": "f782b93d-0a83-4790-ace0-884a588f6fe2"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [1.1084, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.4878, 0.0000, 0.4654, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.3784, 0.3950]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The resultant attention  weight matrix above now has additional elements zeroed out and remaining 1s rescaled.\n",
        "\n",
        "* Note that the resulting dropout outputs may look different depending on your operatinf system."
      ],
      "metadata": {
        "id": "OKnhhHGajMOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5.3 Implementing a compact casual attention class.\n",
        "\n",
        "* We will now incorporate the casual attention and dropout modifications into the `SelfAttention` Python class we developed in section 3.4.\n",
        "* This modified class will serve as template for developing `multi-head attention`, which is the final attention class we will implement.\n",
        "\n",
        "* Before that, let's ensure that the code can handle batches consisting of more than one input so that `CasualAttention` class supports the batches outputs produced by dataloaders we implemented in section 2"
      ],
      "metadata": {
        "id": "JuxtvZAnjpde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs,inputs),dim=0)\n",
        "print(batch.shape) #two inputs with 5 tokens each;each token has embedding dimension of 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NcZOpgjk3o5",
        "outputId": "d3c2c6cd-6d0d-4118-90bb-0daabaa53e37"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now let's implement the casual attention class which is similar to the `SelfAttention` class we implemented earlier, except that we have included the dropout and casual mask components."
      ],
      "metadata": {
        "id": "xickzXs6mTWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CasualAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.Wq = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.Wk = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.Wv = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout) # compared to the previous SelfAttention1 class, we added a dropout layer.\n",
        "    self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length),diagonal=1))#the register_buffer call is also new addition\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    keys = self.Wk(x)\n",
        "    queries = self.Wq(x)\n",
        "    values = self.Wv(x)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(1,2)#we transpose dimensions 1 and 2, keeping the batch dimension at the first position (0)\n",
        "    attn_scores.masked_fill_(self.mask.bool() [:num_tokens], -torch.inf)\n",
        "    attn_weigths = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    attn_weigths = self.dropout(attn_weigths)\n",
        "    context_vec = attn_weigths @ values\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "ATJYyeIymz3Z"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We have added a `self.register_buffer()` call in the `__init__` method.\n",
        "* The use of `register_buffer` in Pytorch is not strictly necessary for all use cases but offers several advantages here.\n",
        "* For example, when we use the `CasualAttention` class in our LLM, bufffers are automatically moved to the appropriate device(CPU or GPU) along with our model, which will be relevant when training our LLM. This means we do not manually ensure these tensors are on the same device as your model parameters, avoiding device mismatch error.\n",
        "\n",
        "* We can use the `CasualAttention` class as follows:"
      ],
      "metadata": {
        "id": "2UEU1XCFliKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "context_length = batch.shape[1]\n",
        "ca = CasualAttention(d_in,d_out,context_length,0.0)\n",
        "context_vecs = ca(batch)\n",
        "print(\"context_vec.shape:\",context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jpjxzyHm3nj",
        "outputId": "f8bb8f8d-ed19-443c-88d8-fd249b1bb128"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vec.shape: torch.Size([2, 5, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The resulting context vector is a three dimensional tensor where each token is now represented by two-dimensional embedding"
      ],
      "metadata": {
        "id": "2un9AEJynf6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Extending single-head attention to multi-head attention.\n",
        "\n",
        "* Our final task is to extend the previously implemented casual attention class over multiple heads.\n",
        "\n",
        "* The term `multi-head` refers to dividing attention mechanism into multiple `heads`, each operating independently."
      ],
      "metadata": {
        "id": "EPnX9J0YnxZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6.1 Stacking multiple single-head attention layers.\n",
        "\n",
        "* In pratical sense,implementing multi-head attention invloves creating multiple instances of the self-attention mechanism, each with its own weights, and then combining their outputs.\n",
        "* The main idea behindmulti-head attention is to run the attention mechanism multiple times(in parallel) with different, learned linear projections-the results of multiplying  the input data(like query,key,value vectors in attention mechanisms) by a weight matrix.\n",
        "* Now let's get coding:\n"
      ],
      "metadata": {
        "id": "tSQi_ON_pupe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList(\n",
        "        [CasualAttention(d_in,d_out,context_length,dropout,qkv_bias) for _ in range(num_heads)]\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return torch.cat([head(x) for head in self.heads],dim=-1)"
      ],
      "metadata": {
        "id": "bujFLnLqqwYA"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Note if we use this `MultiHeadAttentionWrapper` class with two attention heads (through `num_heads=2`) and `CasualAttention` output dimension (`d_out*num_heads=4`).\n",
        "* To illustrate this further with a concrete example, we can use the `MultiHeadAttentionWrapper` class similar to the `CasualAttention` class before:"
      ],
      "metadata": {
        "id": "EsdHzNx7rmox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "context_length = batch.shape[1]\n",
        "d_in,d_out = 3,2\n",
        "mha = MultiHeadAttentionWrapper(\n",
        "    d_in,d_out,context_length,0.0,num_heads=2\n",
        ")\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\",context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7zlwf4IsVw5",
        "outputId": "9f59f40c-2329-4bfd-da1e-d9d130cc2f6b"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.4401, 0.1068, 0.5429, 0.3272],\n",
            "         [0.4405, 0.2633, 0.3180, 0.2185],\n",
            "         [0.4565, 0.3060, 0.2663, 0.1864],\n",
            "         [0.4013, 0.2941, 0.1963, 0.1485],\n",
            "         [0.3975, 0.2588, 0.2221, 0.1318]],\n",
            "\n",
            "        [[0.4401, 0.1068, 0.5429, 0.3272],\n",
            "         [0.4405, 0.2633, 0.3180, 0.2185],\n",
            "         [0.4565, 0.3060, 0.2663, 0.1864],\n",
            "         [0.4013, 0.2941, 0.1963, 0.1485],\n",
            "         [0.3975, 0.2588, 0.2221, 0.1318]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 5, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first dimension of the resulting `context_vecs` tensor is 2 since we have two input texts (the input texts are duplicated, which is why the context vectors are exactly the same for those).\n",
        "* The second dimension refers to the 5 tokens in each input.\n",
        "* The third dimension refers to the fourth dimensional embedding of each token."
      ],
      "metadata": {
        "id": "R_yxZTfhziZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6.2 Implementing multi-head attention with weight splits\n",
        "\n",
        "* Till now, we have created a  `MultiheadAttentionWrapper` to implement multi-head attention by stacking multiple single-head attention modules.\n",
        "* Instead of maintaing two seperate classes `MultiHeadAttentionWrapper` and `CausalAttention`, we can combine these concepts into a single `MultiHeadAttention` class.\n",
        "* Let's take a look at the `MultiHeadAttention` class before we discuss it further."
      ],
      "metadata": {
        "id": "r_uLDRd10YKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert (d_out % num_heads == 0 ), \\\n",
        "       \"d_out must be divisible by num_heads\"\n",
        "    self.out = d_out\n",
        "    self.num_heads =num_heads\n",
        "    self.head_dim = d_out // num_heads #reduces the projection dim to match the desired output dim\n",
        "    self.Wq = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.Wk = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.Wv = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out,d_out)#using a linear layer to combine head outputs\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    keys = self.Wk(x)\n",
        "    queries = self.Wq(x)\n",
        "    values = self.Wv(x)\n",
        "    keys = keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    values = values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    queries = queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    keys = keys.transpse(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "    attn_scores = queries @ keys.tranpose(2,3)\n",
        "    mask_bool = self.mask()[:num_tokens,:num_tokens]\n",
        "    attn_scores.masked_fill_(mask_bool,-torch.inf)\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5,dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context_vec = (attn_weights @ values).transpose(1,2)\n",
        "    context_vec = context_vec.reshape(b,num_tokens,self.d_out)\n",
        "    context_vec = self.out_proj(context_vec)\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "RoPaBI871rNK"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The splitting of the query, key and value tensors is achieved through tensor reshaping and transposing operations using Pytorch's `.view` and `.transpose`.\n",
        "\n",
        "* The key operation is to split the `d_out` dimension into `num_heads` and `head_dim`, where `head_dim=d_out/num_heads`.\n",
        "\n",
        "* The tensors are then transposed to bring the `num_heads` dimension, resulting in a shape of `(b,num_head,num_tokens,head_dim)`. This process of tranposition is crucial for correctly aligning the queries, keys, and values across the different heads and performing batched matrix multiplications efficiently.\n",
        "\n",
        "* To illustrate this batched matrix multiplication, suppose we have the following tensor:"
      ],
      "metadata": {
        "id": "DRTclgil9FXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([\n",
        "    [[0.2865,0.6789,0.2255,0.8734],\n",
        "    [0.8993,0.0390,0.6827,0.4556],\n",
        "    [0.7178,0.6877,0.9145,0.2340]],\n",
        "\n",
        "    [\n",
        "        [0.0345,0.3564,0.1245,0.5334],\n",
        "        [0.4066,0.2568,0.4545,0.9785],\n",
        "        [0.4606,0.5159,0.4442,0.5786]\n",
        "    ]\n",
        "])\n"
      ],
      "metadata": {
        "id": "cGm01-AQ_pD6"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now we perform a batched matmul between the tensor itself and a view of the tensor where we transposed the last two dimensions, `num_tokens` and `head_dim`:"
      ],
      "metadata": {
        "id": "eb0qdt7IAv4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(a @ a.transpose(1,2))"
      ],
      "metadata": {
        "id": "FoKJTeV80fBr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f08483-4c64-4129-f40f-b41fd7bad0ec"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1.3567, 0.8360, 1.0831],\n",
            "         [0.8360, 1.4839, 1.4033],\n",
            "         [1.0831, 1.4033, 1.8792]],\n",
            "\n",
            "        [[0.4282, 0.6841, 0.5637],\n",
            "         [0.6841, 1.3953, 1.0878],\n",
            "         [0.5637, 1.0878, 1.0104]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Implementing a GPT model from scratch to generate text."
      ],
      "metadata": {
        "id": "Tyaw14FQIYBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Coding an LLM architecture\n",
        "* In this step we will code the architecture of gpt-2 because it the only gpt model whose architecture which is opensource:\n"
      ],
      "metadata": {
        "id": "U2gcFdCMIhKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\":50257,\n",
        "    \"context_length\":1024,\n",
        "    \"emb_dim\":768,\n",
        "    \"num_heads\":12,\n",
        "    \"num_layers\":12,\n",
        "    \"drop_rate\":0.1,\n",
        "    \"qkv_bias\":False\n",
        "}"
      ],
      "metadata": {
        "id": "YPyItcFsI_w0"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `vocab_size`-refers to a vocabulary of 50,257 words, as used by the BPE tokeninzer.\n",
        "* `context_length`- denotes the maximum number of inut tokens the model can handle via  the positional embeddings.\n",
        "* `emb_dim`-represnets the embedding size, transforming each token into a 768-dimensional vector.\n",
        "`num_heads`- indicates the count of attention heads in the multi-head attention mechanism.\n",
        "`num_layers`- specifies the numbers of transformer blocks in the model.\n",
        "`drop_rate` indicates the intensity of the dropout mechanism to prevent overfitting.\n",
        "`qkv_bias`- determines whether to include a bias vector in the `Linear` layer of the multi-head attention for query,key and value computations.\n"
      ],
      "metadata": {
        "id": "GxQ7qTBPJbZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing GPT Architecture\n"
      ],
      "metadata": {
        "id": "63fn7hPFnJ8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## step 1 placeholder backbone.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicGPTModel(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[\n",
        "            BasicTransformerBlock(cfg)\n",
        "            for _ in range(cfg[\"num_layers\"])\n",
        "        ]\n",
        "    )# using a placeholder for transformerblock\n",
        "    self.final_norm =BasicLayerNorm(cfg[\"emb_dim\"])#using a placeholder for layernorm\n",
        "    self.out_head = nn.Linear(\n",
        "        cfg[\"emb_dim\"],\n",
        "        cfg[\"vocab_size\"],\n",
        "        bias=False\n",
        "    )\n",
        "\n",
        "  def forward(self,in_idx):\n",
        "    batch_size,seq_len = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(\n",
        "        torch.arange(seq_len,device=in_idx.device)\n",
        "    )\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "class BasicTransformerBlock(nn.Module):# a simple placeholder class that will be replaced by a real  TransformerBlock later.\n",
        " def __init__(self,cfg):\n",
        "  super().__init__()\n",
        "\n",
        " def forward(self,x):\n",
        "   return x\n",
        "\n",
        "class BasicLayerNorm(nn.Module):\n",
        "  def __init__(self,normalized_shape, eps=1e-5):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "HoHvoefWm_G0"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `BasicGPTModel` class defines a simplified version of a GPT-LIKE Model using Pytorch's neural network (nn.Module).\n",
        "* The model's architecture in the `BasicGPTModel` class consist of token and positional embeddings, dropout, a series of transformer blocks, a final layer normalization and a linear output layer.\n",
        "* The `method` method describes the data flow through the model: it computes token and positional embeddings for the input indices, applies normalization, and finally produces logits with the liner output layer.\n",
        "* Now we tokenize a batch consisiting of two text inputs for the GPT model using the tiktoken tokenizer.\n"
      ],
      "metadata": {
        "id": "pE1IG0CYr5_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "batch = []\n",
        "txt1 = \"A transformer is a neural network architecture that uses \"\n",
        "txt2 = \"A transformer is a device that transfers electrical energy \"\n",
        "\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch,dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwogxeP7uQAF",
        "outputId": "5556e1a2-db9e-4f62-f0a2-95e5300d4607"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   32, 47385,   318,   257, 17019,  3127, 10959,   326,  3544,   220],\n",
            "        [   32, 47385,   318,   257,  3335,   326, 16395, 12278,  2568,   220]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next, we initialize a new 124-million-parameter `BasicGPTModel` instance and feed it the tokenized `batch`:"
      ],
      "metadata": {
        "id": "b9W0ESqdxeuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = BasicGPTModel(GPT_CONFIG_124M)\n",
        "logits = model(batch)\n",
        "print(\"Output shape:\",logits.shape)\n",
        "print(logits)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8E0i1zqyH3T",
        "outputId": "ce1cb7e5-313c-4b4a-adf1-f85ec5ef00dd"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 10, 50257])\n",
            "tensor([[[ 0.3944, -0.0408, -0.2424,  ..., -0.2212,  0.0131,  1.3445],\n",
            "         [-0.6883,  1.1449, -0.3125,  ..., -0.0058,  0.0797,  0.7782],\n",
            "         [ 0.7116, -0.4044, -0.1252,  ..., -0.3596, -0.0923, -0.3402],\n",
            "         ...,\n",
            "         [-0.1160,  0.7408,  0.5574,  ..., -1.7610, -0.9843, -0.0680],\n",
            "         [ 0.6679, -0.8315,  0.4512,  ..., -0.8140, -0.2951, -0.1050],\n",
            "         [ 0.2066, -1.7154,  0.6513,  ...,  0.1503,  0.9521,  0.3501]],\n",
            "\n",
            "        [[ 0.3859, -0.5115, -0.2036,  ..., -0.1633, -0.0996,  1.0689],\n",
            "         [-0.5117,  1.2063, -1.5755,  ..., -0.2097,  0.1184,  0.8775],\n",
            "         [ 0.2851,  0.0699, -0.4033,  ..., -0.4429,  0.1969, -1.2305],\n",
            "         ...,\n",
            "         [-0.4840, -0.3921, -0.8128,  ..., -0.1775, -0.9464,  0.3534],\n",
            "         [ 0.4340, -0.1734,  0.7860,  ..., -0.0755, -1.1405, -0.1249],\n",
            "         [ 0.6304, -0.6307,  0.5974,  ...,  0.7942,  0.3494,  0.3275]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The output tensor has two rows corresponding to the two text samples.Each text consist of 10 tokens; each token is a 50,257-dimensional vector, which matches the size of the tokenizer's vocabulary.\n",
        "* The embedding has 50,257 dimensions because each of these dimensions refers to a unique token in the vocabulary."
      ],
      "metadata": {
        "id": "6R95m7Kny6PP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Normalizing activations with LayerNorm\n",
        "\n",
        "* The main idea behind layernorm is to adjust the activations (outputs) of a neural network layer to have a mean of 0 and variance 1, also known as unit variance.\n",
        "* This adjustment speed up the convergence  to effective weights and ensures consistent, reliable training.\n",
        "* Let's create absic neural network layer with five inputs and six outputs that we apply to two input examples:"
      ],
      "metadata": {
        "id": "rwCqHiZ9zemI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "batch_example = torch.randn(2,5)\n",
        "layer = nn.Sequential(nn.Linear(5,6),nn.ReLU())\n",
        "out = layer(batch_example)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLtmQ7MT055p",
        "outputId": "0dd518d1-4b05-4e43-c90a-50b70b328fde"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
            "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
            "       grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Before applying layernorm to these outputs, let's examine the mean and variance:"
      ],
      "metadata": {
        "id": "9XorUQtM1WbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = out.mean(dim=-1,keepdim=True)\n",
        "var = out.var(dim=-1,keepdim=True)\n",
        "print(\"Mean:\\n\",mean)\n",
        "print(\"Variance:\\n\",var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEl8EyQD1dx-",
        "outputId": "a0c4dc3c-1104-4df8-9b69-ce0350e6d8c9"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[0.1324],\n",
            "        [0.2170]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[0.0231],\n",
            "        [0.0398]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first row in the mean tensor tensor here contains the mean value for the input row, and the sencond output row contains the mean for the second input row.\n",
        "* Using `keepdim=True` in operation like mean or variance calculation ensures that the output tensor retains the same number of dimensions as the input tensor, even though the operation reduces  the tensor along the dimension specified via `dim`.\n",
        "* Now,let's apply layer norm to the layer outputs we obtained earlier:"
      ],
      "metadata": {
        "id": "IS5A2w2i13cM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_norm = (out-mean) / torch.sqrt(var)\n",
        "mean = out_norm.mean(dim=-1,keepdim=True)\n",
        "var = out_norm.var(dim=-1,keepdim=True)\n",
        "print(\"Normalized layer outputs:\\n\",out_norm)\n",
        "print(\"Mean:\\n\",mean)\n",
        "print(\"Variance:\\n\",var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PTlfOQf27on",
        "outputId": "95c130ec-2600-44ab-ccc9-166eee669b63"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized layer outputs:\n",
            " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
            "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "Mean:\n",
            " tensor([[9.9341e-09],\n",
            "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As we can see based on the results, the normalized layer outputs,which now also contain negative values, have 0 mean and a variance of 1.\n",
        "* Now let's encapsulate this process in a PyTorch module that we can use in the GPT model later."
      ],
      "metadata": {
        "id": "PxpKUtM537FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean = x.mean(dim=-1,keepdim=True)\n",
        "    var = x.var(dim=-1,keepdim=True,unbiased=False)\n",
        "    norm_x = (x-mean) /torch.sqrt(var + self.eps)\n",
        "    return self.scale * norm_x + self.shift\n"
      ],
      "metadata": {
        "id": "vZj_63aU4VXB"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The variable `eps` is a small constant (epsilon) added to the variance to prevent division by zero during normalization.\n",
        "* The `scale` and `shift` are two trainable parameters that the LLM automatically adjusts during training if it is determined that doing so wou;d improve the model's performance on its training task.\n",
        "* Let's the `LayerNorm` module in practice and apply it to the batch input:"
      ],
      "metadata": {
        "id": "_TCC0x0G6SJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layernorm = LayerNorm(emb_dim=5)\n",
        "out_layernorm = layernorm(batch_example)\n",
        "var = out_layernorm.var(dim=-1,unbiased=False,keepdim=True)\n",
        "print(\"Mean:\\n\",mean)\n",
        "print(\"Variance:\\n\",var)"
      ],
      "metadata": {
        "id": "Nbdgb1qn7I6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e43e2dd8-e03a-45b6-b291-e47434942ff1"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[9.9341e-09],\n",
            "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The results show that the layer normalization code works as expected and normalized the values of each of two inputs suchs that they have a mean of 0 and a variance of 1.\n",
        "* Now let's move on to the next step where we look into the GELU activation function, which is one of the activation functions used in LLMs."
      ],
      "metadata": {
        "id": "jfRhcoSV_58S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Implementing a feed forward network with GELU activations.\n",
        "* Usually, the ReLU activation function has been commonly ued in deep learning due to its simplicity and effectiveness across various neural network architectures.\n",
        "* However, other activation functions like GELU (Gaussian error linear unit) and SwiGLU (Swish-gated linear unit).\n",
        "* These two offer improved performance for deep learning models, unlike the simpler ReLU.\n",
        "* Let's implement the GELU via code:"
      ],
      "metadata": {
        "id": "Fohjz5jDAgnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return 0.5*x * (\n",
        "        1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi))* (x + 0.04475 * torch.pow(x,3))\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "bqlS2LWGBjxm"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now let's differentiate between GELU function with ReLU function via plotting their curves:"
      ],
      "metadata": {
        "id": "pjTfh962CJDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "gelu, relu = GELU() , nn.ReLU()\n",
        "\n",
        "x = torch.linspace(-3,3,100)\n",
        "y_gelu,y_relu = gelu(x), relu(x)\n",
        "plt.figure(figsize=(10,6))\n",
        "for i, (y,label) in enumerate(zip([y_gelu,y_relu],[\"GELU\",\"ReLU\"]),1):\n",
        "  plt.subplot(1,2,i)\n",
        "  plt.plot(x,y)\n",
        "  plt.title(f\"{label} activation function\")\n",
        "  plt.xlabel(\"x\")\n",
        "  plt.ylabel(f\"{label}(x)\")\n",
        "  plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "DcO17fPlCX-R",
        "outputId": "6bd12327-ba09-4986-b1e5-00ec006ae6ae"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The smoothness of GELU can lead to better optimization properties during training, as it allows for more adjustments to the model's parameters.\n",
        "* The ReLU has a sharp corner at zero, which can sometimes make optimization harder, especially in networks that are very deep or have complex architectures.\n",
        "* Next, let's use the GELU function to implement the small neural network module, `FeedForward` that we will be using in the LLM's transformer block later.\n"
      ],
      "metadata": {
        "id": "AXYJQG6NDfsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg[\"emb_dim\"],4*cfg[\"emb_dim\"]),\n",
        "        GELU(),\n",
        "        nn.Linear(4* cfg[\"emb_dim\"],cfg[\"emb_dim\"]),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "e-hGTiREGhIM"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now let's initialize a new `FeedForward` module with a token embedding size of 768 and feed it a batch input with two samples and three tokens each:"
      ],
      "metadata": {
        "id": "WbkNz55KHZoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ffn = FeedForward(GPT_CONFIG_124M)\n",
        "x = torch.rand(2,3,768)\n",
        "out = ffn(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQJMOtAiHsCw",
        "outputId": "ca6fdac1-eefc-4d0c-9f16-0251beb8c3b7"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The `FeedForward` module plays a crucial role in enhancing the model's ability to learn from and generalize the data.\n",
        "* Although the input and output dimensions of this module is the same, it internally expands the embedding dimesion into a higher-dimensional space through the first linear layer.\n",
        "* This expansion is followed by a nonlinear GELU activation and then a contraction back to the original dimension with the second linear transformation.\n",
        "\n",
        "\n",
        "* Next, we will go over the concept of shortcut connections that we insert between different layers of a neural network, which are important for improving the training performance in deep neural network architectures."
      ],
      "metadata": {
        "id": "asvGMZaDIEXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Adding shortcut connections.\n",
        "* Originally, shortcut connections were proposed for deep networks in computer vision to mitigate the challenge of vanishing gradients.\n",
        "* The vanishing gradient problem refers to the issue where gradients become progressively smaller as they propagate backward through the layers, making it difficult to effectively train earlier layers.\n",
        "* These connections play a crucial role in preserving the flow of gradients during the backward pass in training.\n",
        "* Now let's see how add shortcut connections in `forward` method:"
      ],
      "metadata": {
        "id": "XHGJt2ZmJeYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyNeuralNetwork(nn.Module):\n",
        "  def __init__(self,layer_sizes,use_shortcut):\n",
        "    super().__init__()\n",
        "    self.use_shortcut = use_shortcut\n",
        "    self.layers = nn.ModuleList([ #implements five layers\n",
        "        nn.Sequential(\n",
        "            nn.Linear(layer_sizes[0],layer_sizes[1]),\n",
        "            GELU(),\n",
        "        ),\n",
        "        nn.Sequential(\n",
        "            nn.Linear(layer_sizes[1],layer_sizes[2]),\n",
        "            GELU(),\n",
        "        ),\n",
        "        nn.Sequential(\n",
        "            nn.Linear(layer_sizes[2],layer_sizes[3]),\n",
        "            GELU(),\n",
        "        ),\n",
        "       nn.Sequential(\n",
        "            nn.Linear(layer_sizes[3],layer_sizes[4]),\n",
        "            GELU(),\n",
        "       ),\n",
        "       nn.Sequential(\n",
        "            nn.Linear(layer_sizes[4],layer_sizes[5]),\n",
        "            GELU(),\n",
        "       )\n",
        "\n",
        "\n",
        "\n",
        "    ])\n",
        "\n",
        "  def forward(self,x):\n",
        "    for layer in self.layers:\n",
        "      layer_output = layer(x) #compute the output of the current layer\n",
        "      if self.use_shortcut and x.shape == layer_output.shape: # check if shortcut can be applied\n",
        "        x = x + layer_output\n",
        "      else:\n",
        "        x = layer_output\n",
        "    return x"
      ],
      "metadata": {
        "id": "fnh3RjxzL0nn"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This code implement a deep neural network with five layers, each consisting of `Linear` layer and `GELU` activation function.\n",
        "* In the forward pass, we iteratively pass the input through the layers and optionally add connections if the `self.shortcut` attribute is set to `True`.\n",
        "\n",
        "* Let's use this code to initialize a neural network without shortcut connections.\n",
        "* Each layer is initialized such that it accepts an example with three input values and returns three output values"
      ],
      "metadata": {
        "id": "30KPqQQgO9ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes = [3,3,3,3,3,1]\n",
        "sample_input = torch.tensor([[1.,0.,-1]])\n",
        "torch.manual_seed(123)\n",
        "model_without_shortcut = DummyNeuralNetwork(layer_sizes,use_shortcut=False)"
      ],
      "metadata": {
        "id": "3_emC7mEP_-j"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's implement a function that computes the gradients in the model's backward pass:"
      ],
      "metadata": {
        "id": "iu32JxWEIyzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradients(model,x):\n",
        "  output = model(x) #forward pass\n",
        "  target = torch.tensor([[0.]])\n",
        "\n",
        "  loss = nn.MSELoss()\n",
        "  loss = loss(output,target)#calculates loss based on close the target and output are\n",
        "\n",
        "  loss.backward()#backward pass to calculate the gradients\n",
        "\n",
        "  for name, param in model.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
      ],
      "metadata": {
        "id": "S4RYqqVII6_T"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's use the `gradients` function and apply it to the model without skip connections:"
      ],
      "metadata": {
        "id": "sW1YM9zLKkQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradients(model_without_shortcut,sample_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMl25oFlKtAq",
        "outputId": "b61dfe0c-d649-4755-9860-6adced98f99e"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 0.00020173616940155625\n",
            "layers.1.0.weight has gradient mean of 0.00012011189392069355\n",
            "layers.2.0.weight has gradient mean of 0.0007152058533392847\n",
            "layers.3.0.weight has gradient mean of 0.0013988724676892161\n",
            "layers.4.0.weight has gradient mean of 0.0050496445037424564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The output of this function shows, the gradients become smaller as we progress from the last layer to the first layer, this phenomenon is called the `vanishing gradient problem`.\n",
        "\n",
        "* Now, let's instantiate a model with skip connections to see how different is:"
      ],
      "metadata": {
        "id": "GpEM_DZ-K3sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model_with_shortcut = DummyNeuralNetwork(layer_sizes,use_shortcut=True)\n",
        "gradients(model_with_shortcut,sample_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HRXsPTQMSKS",
        "outputId": "84b82e9c-b079-4522-bdf0-f4e70ec32cba"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 0.2217087745666504\n",
            "layers.1.0.weight has gradient mean of 0.20695072412490845\n",
            "layers.2.0.weight has gradient mean of 0.3289870023727417\n",
            "layers.3.0.weight has gradient mean of 0.2665843367576599\n",
            "layers.4.0.weight has gradient mean of 1.3259057998657227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The layer layer (layer 4) still has a larger gradient than the others, but, the gradient value stabilizes as we progress toward the first layer (layer 0) and doesn't shrink drastically.\n",
        "\n",
        "* In summary, the shortcut connections are important for overcoming the limitation posed by the vanishing gradient problem in deep neural networks."
      ],
      "metadata": {
        "id": "_5skmqcpMlBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Connecting attention and linear layers in a transformer block.\n",
        "\n",
        "* Now let's implement the `transformer block`, a fundemental building block GPT and other LLM architectures.\n",
        "* This block, combines concepts like multi-head attention,layer normalization,dropout,feed forward layers and GELU activations."
      ],
      "metadata": {
        "id": "wGWg87hkNNBq"
      }
    }
  ]
}