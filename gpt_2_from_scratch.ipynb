{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO/PqdLRExm10QJBLEyJXQ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cohegen/gpt2-from-scratch/blob/main/gpt_2_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Introduction:Understanding Large Language Models through GPT-2"
      ],
      "metadata": {
        "id": "1S-EKQRfmZTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Large Language models are a class of deep learning models designed to understand and generate human-like text.\n",
        "* They're built using the transformer architecture and are widely used in applications like chatbots,translation and text generation\n"
      ],
      "metadata": {
        "id": "jLIMlxHQmfIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Understanding word embeddings"
      ],
      "metadata": {
        "id": "PJ9DloqAnn-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Deep neural net models, including LLMs, cannot process raw text directly.\n",
        "* Since text is categorical, it is not compatible with the mathematical operations used to implement and train neural networks.Therefore we need a way to represent words as continous-valued vectors.\n",
        "* This concept of converting data into a vector format is referred to embedding.\n",
        "* In simple terms, an embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a convert nonnumeric data into format that neural networks can process.\n",
        "* Instead of treating each word as a unique symbol, embeddings map words into a continuous vector space where similar words are close together.\n",
        "* For example, the vectors for \"king\" and \"queen\" or \"run\" and \"jog\" will be near each other because they share similar meanings. These embeddings are learned from data and capture semantic relationships, making them a fundamental building block in modern NLP models like GPT-2."
      ],
      "metadata": {
        "id": "DAPAiIK8nxUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Tokenizing text"
      ],
      "metadata": {
        "id": "ca9i_4jHpou2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* LLMs take text as inputs but wait before these words are mapped into embeddings, the go through a stage known as tokenization.\n",
        "* Tokenization is the process of breaking text into smaller units called tokens- which can be words,subwords, or even characters.\n",
        "* This step is crucial because language models like GPT-2  don't understand raw text; they work with numbers.\n",
        "* Tokenization converts texts into sequence of tokens that can be mapped to numerical IDs\n",
        "* The text we'll tokenizer for LLM training is `The dante's inferno dataset`.\n",
        "* You can find this dataset from the official project gutenberg website.\n",
        "* Let's get coding"
      ],
      "metadata": {
        "id": "ytG-DzRYpvl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## downloading dante's inferno dataset from project gutenberg website.\n",
        "import urllib.request\n",
        "\n",
        "url = 'https://www.gutenberg.org/cache/epub/41537/pg41537.txt'\n",
        "file_path = \"dante's-inferno.txt\"\n",
        "urllib.request.urlretrieve(url,file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfxwYwh7s06C",
        "outputId": "d1db2b11-5b44-41f5-979e-410dc1bd6a74"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"dante's-inferno.txt\", <http.client.HTTPMessage at 0x7c9d7f12ce10>)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's load `dante's-inferno.txt` file using python file handling utilities."
      ],
      "metadata": {
        "id": "qLe6SX9ltY2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"dante's-inferno.txt\",'r',encoding='utf-8') as f:\n",
        "  raw_text = f.read()\n",
        "print(\"Total number of characters:\",len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB1fKG9lt0Bg",
        "outputId": "1167d24e-5408-4cc3-e981-e42bd24849d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 700670\n",
            "ï»¿The Project Gutenberg eBook of The Divine Comedy of Dante Alighieri: The Inferno\n",
            "    \n",
            "This ebook i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Our goal is to tokenize this 700,670 character long story into individual words and special characters that we can turn into embeddings for LLM training.\n",
        "* Which option do we have in splitting the texts, in this short story we can use python's regular expression library `re` for illustration purposes."
      ],
      "metadata": {
        "id": "zeI8coswwqAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using some simple text we can use `re.split` command with the following syntax to split a text on whitespaces characters."
      ],
      "metadata": {
        "id": "4SCkdqw3xXeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"Hello, world. I am Antonius, nice to meet you.\"\n",
        "result = re.split(r'(\\s)',text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwYj9HT5xpoy",
        "outputId": "5ade065a-1235-4de1-e709-a5263b354f75"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'I', ' ', 'am', ' ', 'Antonius,', ' ', 'nice', ' ', 'to', ' ', 'meet', ' ', 'you.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This simple tokenization scheme mostly works for seperating the use case text into individual words, however some are still connected to punctuation characters that we want to have as seperate entities.\n",
        "* We have also avoided a step in tokenization in which we make all text inputs lowercase because capitalization helps LLMs distinguish between proper nouns and common nouns, understand sentence structure,and learn to geneate text with proper capitalization.\n",
        "\n",
        "\n",
        "* Let's modify the regular expression splits on whitespaces `(\\s)`, commas, and periods ([,.])."
      ],
      "metadata": {
        "id": "RWR70LcTyISL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.] | \\s)',text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49BXSzWtzbSv",
        "outputId": "33239f1d-1eb1-407c-e3ae-bc454c62122a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ', ', 'world', '. ', 'I am Antonius', ', ', 'nice to meet you.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization works well here but let's increase the diversity of punctuation which our dummy tokenizer can work on e.g question marks, double-slashes."
      ],
      "metadata": {
        "id": "c_qS0b5wzzof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is attention is all you need-- a good research paper?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|\\s+|--)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Tjb_wkLX0Fj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76d02fa0-5416-4bbe-e6f9-ce617c9553b7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'attention', ' ', 'is', ' ', 'all', ' ', 'you', ' ', 'need', '--', '', ' ', 'a', ' ', 'good', ' ', 'research', ' ', 'paper', '?', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a basic tokenizer working, let's apply it to Dante's Inferno."
      ],
      "metadata": {
        "id": "9uIVD1ljQL0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text = re.split(r'([,.:;?_!\"()\\']|\\s+|--)',raw_text)\n",
        "preprocessed_text = [item.strip() for item in preprocessed_text if item.strip()]\n",
        "print(len(preprocessed_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUXqlxjuQXEW",
        "outputId": "a1f42206-4fe7-4ee7-ef1f-7162e75ff7aa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "145180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The print statement outputs `145180` tokens in this text(without whitespaces).\n",
        "Now let's print the first 70 tokens for quick visual check"
      ],
      "metadata": {
        "id": "_LU-kz9rQ5C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed_text[:70])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEdsNmjRRMV5",
        "outputId": "cf717332-f9ff-4a98-a66e-66c0cc975c7a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'Divine', 'Comedy', 'of', 'Dante', 'Alighieri', ':', 'The', 'Inferno', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Converting tokens into token IDs"
      ],
      "metadata": {
        "id": "GsRtx5XDRaNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now, let's convert these tokens from a Python string to an integer representation to produce the token IDs.\n",
        "* This step is an intermediate step before converting the token IDs into embedding vectors.\n",
        "\n",
        "* Since we have tokenized Dante's Inferno and assigned it to Python variable called `preprocessed_text`, let's create a list of all unique tokens and sort them alphabetically to determine the vocabulary size."
      ],
      "metadata": {
        "id": "6DzQ9lNBRerK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed_text))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNS-EkiYSrdf",
        "outputId": "0f291199-f407-436c-c269-5930455ca339"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* After finding out the `vocab_size=13909`, we create the vocabulary and print its first 500 entries."
      ],
      "metadata": {
        "id": "FYYQeWgcS9x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab  ={token:integer for integer,token in enumerate(all_words)}\n",
        "for i,item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i >= 500:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qKdtdt3TMa9",
        "outputId": "95dcc9b9-a70c-4fa0-c72b-b59f4ee4291e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "('#41537]', 2)\n",
            "('$1', 3)\n",
            "('$5', 4)\n",
            "(\"'\", 5)\n",
            "('(', 6)\n",
            "(')', 7)\n",
            "('***', 8)\n",
            "(',', 9)\n",
            "('-', 10)\n",
            "('--', 11)\n",
            "('.', 12)\n",
            "('000', 13)\n",
            "('1', 14)\n",
            "('10', 15)\n",
            "('100', 16)\n",
            "('101', 17)\n",
            "('102', 18)\n",
            "('103', 19)\n",
            "('1037', 20)\n",
            "('104', 21)\n",
            "('105', 22)\n",
            "('106', 23)\n",
            "('107', 24)\n",
            "('108', 25)\n",
            "('1085', 26)\n",
            "('109', 27)\n",
            "('10th', 28)\n",
            "('11', 29)\n",
            "('110', 30)\n",
            "('1106', 31)\n",
            "('111', 32)\n",
            "('1115', 33)\n",
            "('112', 34)\n",
            "('113', 35)\n",
            "('114', 36)\n",
            "('115', 37)\n",
            "('1152-1190', 38)\n",
            "('116', 39)\n",
            "('117', 40)\n",
            "('118', 41)\n",
            "('1180', 42)\n",
            "('1185', 43)\n",
            "('119', 44)\n",
            "('1193', 45)\n",
            "('1198', 46)\n",
            "('12', 47)\n",
            "('120', 48)\n",
            "('121', 49)\n",
            "('1215', 50)\n",
            "('122', 51)\n",
            "('1220', 52)\n",
            "('123', 53)\n",
            "('1238', 54)\n",
            "('1239', 55)\n",
            "('124', 56)\n",
            "('1248', 57)\n",
            "('1249', 58)\n",
            "('125', 59)\n",
            "('1250', 60)\n",
            "('1252', 61)\n",
            "('1258', 62)\n",
            "('1259', 63)\n",
            "('126', 64)\n",
            "('1260', 65)\n",
            "('1261', 66)\n",
            "('1264', 67)\n",
            "('1265', 68)\n",
            "('1266', 69)\n",
            "('1267', 70)\n",
            "('1268', 71)\n",
            "('1269', 72)\n",
            "('127', 73)\n",
            "('1270', 74)\n",
            "('1271', 75)\n",
            "('1273', 76)\n",
            "('1275', 77)\n",
            "('1277', 78)\n",
            "('1278', 79)\n",
            "('128', 80)\n",
            "('1280', 81)\n",
            "('1281', 82)\n",
            "('1282', 83)\n",
            "('1283', 84)\n",
            "('1284', 85)\n",
            "('1285', 86)\n",
            "('1286', 87)\n",
            "('1288', 88)\n",
            "('1289', 89)\n",
            "('129', 90)\n",
            "('1290', 91)\n",
            "('1291', 92)\n",
            "('1292', 93)\n",
            "('1293', 94)\n",
            "('1294', 95)\n",
            "('1295', 96)\n",
            "('1296', 97)\n",
            "('1297', 98)\n",
            "('1298', 99)\n",
            "('1299', 100)\n",
            "('13', 101)\n",
            "('130', 102)\n",
            "('1300', 103)\n",
            "('1301', 104)\n",
            "('1301-1302', 105)\n",
            "('1301-2', 106)\n",
            "('1302', 107)\n",
            "('1303', 108)\n",
            "('1304', 109)\n",
            "('1305', 110)\n",
            "('1306', 111)\n",
            "('1307', 112)\n",
            "('1308', 113)\n",
            "('1309', 114)\n",
            "('131', 115)\n",
            "('1310', 116)\n",
            "('1311', 117)\n",
            "('1312', 118)\n",
            "('1313', 119)\n",
            "('1313-1316', 120)\n",
            "('1314', 121)\n",
            "('1315', 122)\n",
            "('1316', 123)\n",
            "('132', 124)\n",
            "('1320', 125)\n",
            "('1321', 126)\n",
            "('1326', 127)\n",
            "('1327', 128)\n",
            "('133', 129)\n",
            "('1332', 130)\n",
            "('1333', 131)\n",
            "('134', 132)\n",
            "('1343', 133)\n",
            "('135', 134)\n",
            "('1350', 135)\n",
            "('136', 136)\n",
            "('137', 137)\n",
            "('1370', 138)\n",
            "('138', 139)\n",
            "('139', 140)\n",
            "('14', 141)\n",
            "('140', 142)\n",
            "('141', 143)\n",
            "('142', 144)\n",
            "('143', 145)\n",
            "('144', 146)\n",
            "('145', 147)\n",
            "('1465', 148)\n",
            "('147', 149)\n",
            "('148', 150)\n",
            "('149', 151)\n",
            "('1496', 152)\n",
            "('15', 153)\n",
            "('150', 154)\n",
            "('1500', 155)\n",
            "('151', 156)\n",
            "('153', 157)\n",
            "('1539', 158)\n",
            "('154', 159)\n",
            "('155', 160)\n",
            "('156', 161)\n",
            "('16', 162)\n",
            "('161', 163)\n",
            "('168', 164)\n",
            "('17', 165)\n",
            "('176', 166)\n",
            "('18', 167)\n",
            "('1823', 168)\n",
            "('1826', 169)\n",
            "('1839', 170)\n",
            "('184', 171)\n",
            "('1854', 172)\n",
            "('1861', 173)\n",
            "('1864', 174)\n",
            "('1865', 175)\n",
            "('1879', 176)\n",
            "('1880', 177)\n",
            "('1882', 178)\n",
            "('1884', 179)\n",
            "('19', 180)\n",
            "('192', 181)\n",
            "('199', 182)\n",
            "('1st', 183)\n",
            "('2', 184)\n",
            "('20', 185)\n",
            "('20%', 186)\n",
            "('200', 187)\n",
            "('2001', 188)\n",
            "('2012', 189)\n",
            "('2024', 190)\n",
            "('203', 191)\n",
            "('209', 192)\n",
            "('21', 193)\n",
            "('213', 194)\n",
            "('217', 195)\n",
            "('22', 196)\n",
            "('225', 197)\n",
            "('23', 198)\n",
            "('233', 199)\n",
            "('24', 200)\n",
            "('241', 201)\n",
            "('249', 202)\n",
            "('25', 203)\n",
            "('25th', 204)\n",
            "('26', 205)\n",
            "('260', 206)\n",
            "('264', 207)\n",
            "('268', 208)\n",
            "('269', 209)\n",
            "('26th', 210)\n",
            "('27', 211)\n",
            "('279', 212)\n",
            "('27th', 213)\n",
            "('28', 214)\n",
            "('29', 215)\n",
            "('2d', 216)\n",
            "('3', 217)\n",
            "('30', 218)\n",
            "('30th', 219)\n",
            "('31', 220)\n",
            "('312', 221)\n",
            "('32', 222)\n",
            "('33', 223)\n",
            "('34', 224)\n",
            "('349', 225)\n",
            "('35', 226)\n",
            "('353', 227)\n",
            "('36', 228)\n",
            "('37', 229)\n",
            "('38', 230)\n",
            "('39', 231)\n",
            "('3d', 232)\n",
            "('4', 233)\n",
            "('40', 234)\n",
            "('41', 235)\n",
            "('42', 236)\n",
            "('43', 237)\n",
            "('44', 238)\n",
            "('45', 239)\n",
            "('46', 240)\n",
            "('47', 241)\n",
            "('48', 242)\n",
            "('482', 243)\n",
            "('49', 244)\n",
            "('496', 245)\n",
            "('4th', 246)\n",
            "('5', 247)\n",
            "('50', 248)\n",
            "('501', 249)\n",
            "('51', 250)\n",
            "('52', 251)\n",
            "('53', 252)\n",
            "('54', 253)\n",
            "('55', 254)\n",
            "('552', 255)\n",
            "('56', 256)\n",
            "('57', 257)\n",
            "('58', 258)\n",
            "('586', 259)\n",
            "('59', 260)\n",
            "('596-1887', 261)\n",
            "('5th', 262)\n",
            "('6', 263)\n",
            "('60', 264)\n",
            "('61', 265)\n",
            "('62', 266)\n",
            "('63', 267)\n",
            "('64', 268)\n",
            "('64-6221541', 269)\n",
            "('65', 270)\n",
            "('6500', 271)\n",
            "('66', 272)\n",
            "('67', 273)\n",
            "('68', 274)\n",
            "('69', 275)\n",
            "('6th', 276)\n",
            "('7', 277)\n",
            "('70', 278)\n",
            "('71', 279)\n",
            "('710', 280)\n",
            "('72', 281)\n",
            "('73', 282)\n",
            "('74', 283)\n",
            "('75', 284)\n",
            "('76', 285)\n",
            "('77', 286)\n",
            "('78', 287)\n",
            "('79', 288)\n",
            "('7th', 289)\n",
            "('8', 290)\n",
            "('80', 291)\n",
            "('801', 292)\n",
            "('809', 293)\n",
            "('81', 294)\n",
            "('82', 295)\n",
            "('83', 296)\n",
            "('84', 297)\n",
            "('84116', 298)\n",
            "('85', 299)\n",
            "('86', 300)\n",
            "('87', 301)\n",
            "('88', 302)\n",
            "('883', 303)\n",
            "('89', 304)\n",
            "('8th', 305)\n",
            "('9', 306)\n",
            "('90', 307)\n",
            "('91', 308)\n",
            "('92', 309)\n",
            "('93', 310)\n",
            "('94', 311)\n",
            "('95', 312)\n",
            "('96', 313)\n",
            "('97', 314)\n",
            "('98', 315)\n",
            "('99', 316)\n",
            "('9th', 317)\n",
            "(':', 318)\n",
            "(';', 319)\n",
            "('?', 320)\n",
            "('A', 321)\n",
            "('ACTUAL', 322)\n",
            "('AGREE', 323)\n",
            "('AGREEMENT', 324)\n",
            "('ALIGHIERI', 325)\n",
            "('AN', 326)\n",
            "('AND', 327)\n",
            "('ANY', 328)\n",
            "('ANYTHING', 329)\n",
            "('ASCIIâ', 330)\n",
            "('Abashed', 331)\n",
            "('Abati', 332)\n",
            "('Abbagliato', 333)\n",
            "('Abbey', 334)\n",
            "('Abbot', 335)\n",
            "('Abel', 336)\n",
            "('Abode', 337)\n",
            "('About', 338)\n",
            "('Above', 339)\n",
            "('Abraham', 340)\n",
            "('Abram', 341)\n",
            "('Abruzzi', 342)\n",
            "('Absalom', 343)\n",
            "('Accord', 344)\n",
            "('According', 345)\n",
            "('Accorso', 346)\n",
            "('Accorso[474]', 347)\n",
            "('Accounts', 348)\n",
            "('Accursed', 349)\n",
            "('Accurst', 350)\n",
            "('AccursÃ¨d', 351)\n",
            "('Acheron', 352)\n",
            "('Acheron[451]', 353)\n",
            "('Achilles', 354)\n",
            "('Achilles[781]', 355)\n",
            "('Acquacheta', 356)\n",
            "('Acquacheta[491]', 357)\n",
            "('Acquasparta', 358)\n",
            "('Acre', 359)\n",
            "('Across', 360)\n",
            "('Acting', 361)\n",
            "('Adam', 362)\n",
            "('Adam[769]', 363)\n",
            "('Add', 364)\n",
            "('Adding', 365)\n",
            "('Additional', 366)\n",
            "('Adelasia', 367)\n",
            "('Adelsberg', 368)\n",
            "('Adige', 369)\n",
            "('Adige[393]', 370)\n",
            "('Adimari', 371)\n",
            "('Adriatic', 372)\n",
            "('Afresh', 373)\n",
            "('Africa', 374)\n",
            "('Africanus', 375)\n",
            "('After', 376)\n",
            "('After[282]', 377)\n",
            "('Again', 378)\n",
            "('Against', 379)\n",
            "('Age', 380)\n",
            "('Ages', 381)\n",
            "('Agli', 382)\n",
            "('Aglow', 383)\n",
            "('Agnello', 384)\n",
            "('Ago', 385)\n",
            "('Agrigentum', 386)\n",
            "('Ah', 387)\n",
            "('Ahithophel', 388)\n",
            "('Aim', 389)\n",
            "('Alack', 390)\n",
            "('Alardo', 391)\n",
            "('Alas', 392)\n",
            "('Alberic', 393)\n",
            "('Alberic[850]', 394)\n",
            "('Alberigo', 395)\n",
            "('Albert', 396)\n",
            "('Albert[84]', 397)\n",
            "('Alberti', 398)\n",
            "('Alberto', 399)\n",
            "('Alchemists', 400)\n",
            "('Aldighieri', 401)\n",
            "('Aldighiero', 402)\n",
            "('Aldobrand', 403)\n",
            "('Aldobrandi', 404)\n",
            "('Aldobrando', 405)\n",
            "('Alecto', 406)\n",
            "('Aleppe', 407)\n",
            "('Alert', 408)\n",
            "('Alessio', 409)\n",
            "('Alexander', 410)\n",
            "('Alexander[437]', 411)\n",
            "('Ali', 412)\n",
            "('Ali[723]', 413)\n",
            "('Alichin', 414)\n",
            "('Alichin[591]', 415)\n",
            "('Alichino', 416)\n",
            "('Alighieri', 417)\n",
            "('Alike', 418)\n",
            "('All', 419)\n",
            "('Allegorico', 420)\n",
            "('Alliances', 421)\n",
            "('Allowing', 422)\n",
            "('Allured', 423)\n",
            "('Almighty', 424)\n",
            "('Almost', 425)\n",
            "('Alone', 426)\n",
            "('Along', 427)\n",
            "('Aloof', 428)\n",
            "('Alphonso', 429)\n",
            "('Alps', 430)\n",
            "('Alps[436]', 431)\n",
            "('Already', 432)\n",
            "('Also', 433)\n",
            "('Although', 434)\n",
            "('Always', 435)\n",
            "('Alyscampo', 436)\n",
            "('Am', 437)\n",
            "('Amazement', 438)\n",
            "('Ambassador', 439)\n",
            "('Amen', 440)\n",
            "('Amidei', 441)\n",
            "('Among', 442)\n",
            "('Amor', 443)\n",
            "('AmphiaraÃ¼s', 444)\n",
            "('AmphiarÃ¤us', 445)\n",
            "('Amphion', 446)\n",
            "('Amphion[798]', 447)\n",
            "('AmpÃ¨re', 448)\n",
            "('An', 449)\n",
            "('Anagni', 450)\n",
            "('Anastasius', 451)\n",
            "('Anastasius[376]', 452)\n",
            "('Anaxagoras', 453)\n",
            "('Anchises', 454)\n",
            "('Ancona', 455)\n",
            "('And', 456)\n",
            "('Andrea', 457)\n",
            "('Andrews', 458)\n",
            "('Angelo', 459)\n",
            "('Angels', 460)\n",
            "('Anger', 461)\n",
            "('Angiolel', 462)\n",
            "('Angiolello', 463)\n",
            "('Animal', 464)\n",
            "('Anjou', 465)\n",
            "('Annas', 466)\n",
            "('Anonimo', 467)\n",
            "('Another', 468)\n",
            "('Anselm', 469)\n",
            "('Anselmuccio', 470)\n",
            "('Antenor', 471)\n",
            "('Antenora', 472)\n",
            "('Antioch', 473)\n",
            "('Antiochus', 474)\n",
            "('Antonio', 475)\n",
            "('AntÃ¦us', 476)\n",
            "('AntÃ¦us[790]', 477)\n",
            "('Anxious', 478)\n",
            "('Any', 479)\n",
            "('Apennine', 480)\n",
            "('Apennines', 481)\n",
            "('Apia', 482)\n",
            "('Apocalypse', 483)\n",
            "('Apollo', 484)\n",
            "('Apostle', 485)\n",
            "('Apothecaries', 486)\n",
            "('April', 487)\n",
            "('Apuana', 488)\n",
            "('Apulia', 489)\n",
            "('Apulian', 490)\n",
            "('Apulians', 491)\n",
            "('Aqua', 492)\n",
            "('Aquarius', 493)\n",
            "('Aquarius[630]', 494)\n",
            "('Aquinas', 495)\n",
            "('Aquitaine', 496)\n",
            "('Arab', 497)\n",
            "('Arabian', 498)\n",
            "('Arabic', 499)\n",
            "('Arabs', 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We see that, the dictionary contains individual tokens associated with unique integer labels.\n",
        "* Next we will apply this vocabulary to convert new text into token IDs."
      ],
      "metadata": {
        "id": "ds8yJw0mVwBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's create a tokenizer class in Python with encode method that splits texts into tokens and carries out string-to-integer mapping to produce token IDs via vocabularies.\n",
        "* We also include `decode` method that carries out the reverse integer-to-string mapping to convert the token IDs back into text."
      ],
      "metadata": {
        "id": "ipA-UkCTWLV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "class SimpleTokenizer1:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|\\s|--)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self,ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "pvUK4aLGW3f5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's test this class using a sample paragraph from Dante's Inferno"
      ],
      "metadata": {
        "id": "BDAD37w8ZSSM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c170d54",
        "outputId": "ae0bec6f-599b-404d-bab8-4326d9a8390f"
      },
      "source": [
        "##instantiating the SimpleTokenizer1\n",
        "tokenizer = SimpleTokenizer1(vocab)\n",
        "text = \"\"\"CANTO XXXIV. The Ninth Circle--the Fourth Ring or Judecca, the deepest point\n",
        "of the Inferno and the Centre of the Universe--it is the place\n",
        "of those treacherous to their Lords or Benefactors--Lucifer with\n",
        "Judas, Brutus, and Cassius hanging from his mouths--passage\n",
        "through the Centre of the Earth--ascent from the depths to the\n",
        "light of the stars in the Southern Hemisphere,\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[763, 3412, 12, 3024, 2221, 942, 11, 12706, 1433, 2637, 10090, 1830, 9, 12706, 6385, 10524, 10012, 12706, 1763, 4624, 12706, 883, 10012, 12706, 3193, 11, 8933, 8924, 12706, 10451, 10012, 12769, 12969, 12857, 12710, 1958, 10090, 658, 11, 1969, 13708, 1829, 9, 739, 9, 4624, 843, 8244, 7856, 8442, 9748, 11, 10256, 12806, 12706, 883, 10012, 12706, 1233, 11, 4781, 7856, 12706, 6484, 12857, 12706, 9222, 10012, 12706, 12239, 8672, 12706, 2889, 1663, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's see if we can turn these token IDs back into text using the `decode` method."
      ],
      "metadata": {
        "id": "kv3wPfmJZf02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wldJqWwNZrMI",
        "outputId": "13655dfc-4ca4-4324-9c10-1d63f03f6007"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CANTO XXXIV. The Ninth Circle -- the Fourth Ring or Judecca, the deepest point of the Inferno and the Centre of the Universe -- it is the place of those treacherous to their Lords or Benefactors -- Lucifer with Judas, Brutus, and Cassius hanging from his mouths -- passage through the Centre of the Earth -- ascent from the depths to the light of the stars in the Southern Hemisphere,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* So I guess i works now let's apply it to a new text sample not contained in the training set:"
      ],
      "metadata": {
        "id": "e7iHq_sXZ3NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, welcome to Bogota\"\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "Gh5JHcKXaEiW",
        "outputId": "95c5cca3-642c-421c-f891-5289abb643a0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-31-2531998187.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, welcome to Bogota\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-28-2450852655.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.:;?_!\"()\\']|\\s|--)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-28-2450852655.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.:;?_!\"()\\']|\\s|--)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can see that after we execute this code we get the above error.\n",
        "* We're getting error because the word `Hello` was not used in `Dante's Inferno` story.\n",
        "* Hence, it is not contained in the vocabulary.\n",
        "* This highlights the need to consider large and diverse training sets to extend the vocabulary when working on LLMs.\n",
        "\n",
        "* Next we'll try to test the tokenizer further on text that contain unkown words and also special tokens that can be used to provide further context for an LLM during training."
      ],
      "metadata": {
        "id": "6lIr-dQramq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Adding special context tokens"
      ],
      "metadata": {
        "id": "AzUesKJPb7_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Our task now is to modify the tokenizer to handle unknown words.\n",
        "* And also need to address the usage and additional of special context tokens that can enhance a model's understading of context or other information.\n",
        "* These special tokens iclude markers for unknown words and document boundaries.\n",
        "\n",
        "* Let'smodify the vocabulary to include these two special tokens , `<unk>` and `<|endoftext|>`, by adding them to our list of all unique words."
      ],
      "metadata": {
        "id": "3Acu6GU1cDgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed_text)))\n",
        "all_tokens.append(\"<|endoftext|>\") # Append as individual string\n",
        "all_tokens.append(\"<|unk|>\") # Append as individual string\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhhvHCNLdZ56",
        "outputId": "256e4dda-645c-478f-ce28-c21d60251230"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now let's print the last 500 enties of the updated vocabulary:"
      ],
      "metadata": {
        "id": "MCRs3JDfdqgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-500:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDQpxMEkd1BZ",
        "outputId": "981305f6-3afa-409e-bbfa-c265467d9082"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('visage', 13411)\n",
            "('visible', 13412)\n",
            "('vision', 13413)\n",
            "('visions', 13414)\n",
            "('visit', 13415)\n",
            "('visited', 13416)\n",
            "('visiting', 13417)\n",
            "('vital', 13418)\n",
            "('vitally', 13419)\n",
            "('vitiate', 13420)\n",
            "('vivid', 13421)\n",
            "('vividly', 13422)\n",
            "('vizor', 13423)\n",
            "('vobis', 13424)\n",
            "('vocal', 13425)\n",
            "('vocation', 13426)\n",
            "('vogue', 13427)\n",
            "('voice', 13428)\n",
            "('voices', 13429)\n",
            "('void', 13430)\n",
            "('vol', 13431)\n",
            "('volition', 13432)\n",
            "('volume', 13433)\n",
            "('voluntarily', 13434)\n",
            "('volunteer', 13435)\n",
            "('volunteers', 13436)\n",
            "('vomiting', 13437)\n",
            "('voracity', 13438)\n",
            "('voted', 13439)\n",
            "('vouches', 13440)\n",
            "('vow', 13441)\n",
            "('voyage', 13442)\n",
            "('vulgar', 13443)\n",
            "('wafted', 13444)\n",
            "('wage', 13445)\n",
            "('waged', 13446)\n",
            "('wager', 13447)\n",
            "('wages', 13448)\n",
            "('wagged', 13449)\n",
            "('waging', 13450)\n",
            "('wags', 13451)\n",
            "('wail', 13452)\n",
            "('wailed', 13453)\n",
            "('wailed[530]', 13454)\n",
            "('wailing', 13455)\n",
            "('wailings', 13456)\n",
            "('wails', 13457)\n",
            "('waist', 13458)\n",
            "('wait', 13459)\n",
            "('waiting', 13460)\n",
            "('waits', 13461)\n",
            "('wake', 13462)\n",
            "('waketh', 13463)\n",
            "('waking', 13464)\n",
            "('walk', 13465)\n",
            "('walks', 13466)\n",
            "('wall', 13467)\n",
            "('wall-painting', 13468)\n",
            "('wall[651]', 13469)\n",
            "('walled', 13470)\n",
            "('wallet', 13471)\n",
            "('wallow', 13472)\n",
            "('walls', 13473)\n",
            "('wand', 13474)\n",
            "('wander', 13475)\n",
            "('wandered', 13476)\n",
            "('wanderer', 13477)\n",
            "('wandering', 13478)\n",
            "('wanderings', 13479)\n",
            "('wanders', 13480)\n",
            "('waned', 13481)\n",
            "('wanness', 13482)\n",
            "('want', 13483)\n",
            "('wanted', 13484)\n",
            "('wanting', 13485)\n",
            "('war', 13486)\n",
            "('war-bell', 13487)\n",
            "('war-cries', 13488)\n",
            "('ward', 13489)\n",
            "('warders', 13490)\n",
            "('wards', 13491)\n",
            "('warehouses', 13492)\n",
            "('warlike', 13493)\n",
            "('warm', 13494)\n",
            "('warmed', 13495)\n",
            "('warmest', 13496)\n",
            "('warmly', 13497)\n",
            "('warms', 13498)\n",
            "('warmth', 13499)\n",
            "('warned', 13500)\n",
            "('warning', 13501)\n",
            "('warns', 13502)\n",
            "('warranties', 13503)\n",
            "('warrior', 13504)\n",
            "('wars', 13505)\n",
            "('wary', 13506)\n",
            "('was', 13507)\n",
            "('wash', 13508)\n",
            "('washed', 13509)\n",
            "('wasps', 13510)\n",
            "('wast', 13511)\n",
            "('waste', 13512)\n",
            "('waste[396]', 13513)\n",
            "('wasted', 13514)\n",
            "('watch', 13515)\n",
            "('watch-dog', 13516)\n",
            "('watched', 13517)\n",
            "('watchfulness', 13518)\n",
            "('watching', 13519)\n",
            "('water', 13520)\n",
            "('water-brooks', 13521)\n",
            "('waterfall', 13522)\n",
            "('waterfalls', 13523)\n",
            "('waters', 13524)\n",
            "('watery', 13525)\n",
            "('wave', 13526)\n",
            "('waver', 13527)\n",
            "('wavered', 13528)\n",
            "('wavers', 13529)\n",
            "('waves', 13530)\n",
            "('waving', 13531)\n",
            "('wax', 13532)\n",
            "('waxed', 13533)\n",
            "('waxen', 13534)\n",
            "('way', 13535)\n",
            "('ways', 13536)\n",
            "('we', 13537)\n",
            "('weak', 13538)\n",
            "('weaken', 13539)\n",
            "('weakness', 13540)\n",
            "('wealth', 13541)\n",
            "('wealthier', 13542)\n",
            "('wealthy', 13543)\n",
            "('weapon', 13544)\n",
            "('wear', 13545)\n",
            "('wearers', 13546)\n",
            "('wearied', 13547)\n",
            "('weariness', 13548)\n",
            "('wearing', 13549)\n",
            "('wears', 13550)\n",
            "('weary', 13551)\n",
            "('weather', 13552)\n",
            "('weathered', 13553)\n",
            "('weaver', 13554)\n",
            "('weavers', 13555)\n",
            "('web', 13556)\n",
            "('website', 13557)\n",
            "('wed', 13558)\n",
            "('wedded', 13559)\n",
            "('wedding', 13560)\n",
            "('weeks', 13561)\n",
            "('ween', 13562)\n",
            "('weep', 13563)\n",
            "('weeping', 13564)\n",
            "('weeps', 13565)\n",
            "('weet', 13566)\n",
            "('weighed', 13567)\n",
            "('weighs', 13568)\n",
            "('weight', 13569)\n",
            "('weight[397]', 13570)\n",
            "('weights', 13571)\n",
            "('weighty', 13572)\n",
            "('welcome', 13573)\n",
            "('welcomed', 13574)\n",
            "('welcoming', 13575)\n",
            "('welfare', 13576)\n",
            "('well', 13577)\n",
            "('well-ascertained', 13578)\n",
            "('well-born', 13579)\n",
            "('well-defined', 13580)\n",
            "('well-fought', 13581)\n",
            "('well-informed', 13582)\n",
            "('well-known', 13583)\n",
            "('well-peeled', 13584)\n",
            "('well-wooded', 13585)\n",
            "('well[516]', 13586)\n",
            "('wellbeing', 13587)\n",
            "('wellnigh', 13588)\n",
            "('wends', 13589)\n",
            "('went', 13590)\n",
            "('wept', 13591)\n",
            "('were', 13592)\n",
            "('wert', 13593)\n",
            "('west', 13594)\n",
            "('western', 13595)\n",
            "('westwards', 13596)\n",
            "('whale', 13597)\n",
            "('what', 13598)\n",
            "('what[733]', 13599)\n",
            "('whatever', 13600)\n",
            "('whatsoever', 13601)\n",
            "('wheel', 13602)\n",
            "('wheeled', 13603)\n",
            "('wheeling', 13604)\n",
            "('wheels', 13605)\n",
            "('whelmed', 13606)\n",
            "('whelmÃ¨d', 13607)\n",
            "('when', 13608)\n",
            "('when[288]', 13609)\n",
            "('whence', 13610)\n",
            "('whene', 13611)\n",
            "('whenever', 13612)\n",
            "('where', 13613)\n",
            "('where[859]', 13614)\n",
            "('whereby', 13615)\n",
            "('wherefore', 13616)\n",
            "('wherein', 13617)\n",
            "('whereon', 13618)\n",
            "('wheresoe', 13619)\n",
            "('whereupon', 13620)\n",
            "('wherever', 13621)\n",
            "('whether', 13622)\n",
            "('which', 13623)\n",
            "('while', 13624)\n",
            "('whilom', 13625)\n",
            "('whip', 13626)\n",
            "('whipped', 13627)\n",
            "('whirling', 13628)\n",
            "('whirls', 13629)\n",
            "('whirlwind', 13630)\n",
            "('whist', 13631)\n",
            "('whistle', 13632)\n",
            "('whit', 13633)\n",
            "('white', 13634)\n",
            "('whither', 13635)\n",
            "('who', 13636)\n",
            "('whoever', 13637)\n",
            "('whole', 13638)\n",
            "('wholeness', 13639)\n",
            "('wholesome', 13640)\n",
            "('wholly', 13641)\n",
            "('whom', 13642)\n",
            "('whom[463]', 13643)\n",
            "('whore', 13644)\n",
            "('whose', 13645)\n",
            "('whoso', 13646)\n",
            "('why', 13647)\n",
            "('wick', 13648)\n",
            "('wicked', 13649)\n",
            "('wickedness', 13650)\n",
            "('wide', 13651)\n",
            "('widely', 13652)\n",
            "('widened', 13653)\n",
            "('widening', 13654)\n",
            "('wider', 13655)\n",
            "('widespread', 13656)\n",
            "('widest', 13657)\n",
            "('widow', 13658)\n",
            "('width', 13659)\n",
            "('wield', 13660)\n",
            "('wielded', 13661)\n",
            "('wife', 13662)\n",
            "('wight', 13663)\n",
            "('wild', 13664)\n",
            "('wilderness', 13665)\n",
            "('wile', 13666)\n",
            "('wiles', 13667)\n",
            "('will', 13668)\n",
            "('willed', 13669)\n",
            "('willed[252]', 13670)\n",
            "('willing', 13671)\n",
            "('willingly', 13672)\n",
            "('willingness', 13673)\n",
            "('wills', 13674)\n",
            "('wilt', 13675)\n",
            "('wily', 13676)\n",
            "('win', 13677)\n",
            "('winces', 13678)\n",
            "('wind', 13679)\n",
            "('windmill', 13680)\n",
            "('window', 13681)\n",
            "('windpipe', 13682)\n",
            "('winds', 13683)\n",
            "('wine', 13684)\n",
            "('wine-bibbers', 13685)\n",
            "('wing', 13686)\n",
            "('winged', 13687)\n",
            "('wings', 13688)\n",
            "('winning', 13689)\n",
            "('wins', 13690)\n",
            "('winter', 13691)\n",
            "('winter-tide', 13692)\n",
            "('winter-time', 13693)\n",
            "('wintry', 13694)\n",
            "('wipe', 13695)\n",
            "('wiped', 13696)\n",
            "('wisdom', 13697)\n",
            "('wise', 13698)\n",
            "('wisely', 13699)\n",
            "('wiser', 13700)\n",
            "('wisest', 13701)\n",
            "('wish', 13702)\n",
            "('wished', 13703)\n",
            "('wishes', 13704)\n",
            "('wishing', 13705)\n",
            "('wit', 13706)\n",
            "('witch', 13707)\n",
            "('with', 13708)\n",
            "('withdraw', 13709)\n",
            "('withdrawn', 13710)\n",
            "('withdrew', 13711)\n",
            "('withes', 13712)\n",
            "('withhold', 13713)\n",
            "('within', 13714)\n",
            "('without', 13715)\n",
            "('withstanding', 13716)\n",
            "('withstood', 13717)\n",
            "('witless', 13718)\n",
            "('witness', 13719)\n",
            "('witnessed', 13720)\n",
            "('witnessing', 13721)\n",
            "('wits', 13722)\n",
            "('witty', 13723)\n",
            "('wives', 13724)\n",
            "('wizards', 13725)\n",
            "('woe', 13726)\n",
            "('woeful', 13727)\n",
            "('woes', 13728)\n",
            "('woful', 13729)\n",
            "('woke', 13730)\n",
            "('wolf', 13731)\n",
            "('wolf-cubs', 13732)\n",
            "('wolves', 13733)\n",
            "('woman', 13734)\n",
            "('woman-like', 13735)\n",
            "('womanhood', 13736)\n",
            "('womankind', 13737)\n",
            "('womanly', 13738)\n",
            "('women', 13739)\n",
            "('won', 13740)\n",
            "('wonder', 13741)\n",
            "('wonderful', 13742)\n",
            "('wondering', 13743)\n",
            "('wonders', 13744)\n",
            "('wondrous', 13745)\n",
            "('wont', 13746)\n",
            "('wonted', 13747)\n",
            "('wood', 13748)\n",
            "('wood[161]', 13749)\n",
            "('wooden', 13750)\n",
            "('woodland', 13751)\n",
            "('woods', 13752)\n",
            "('woody', 13753)\n",
            "('woollens', 13754)\n",
            "('word', 13755)\n",
            "('words', 13756)\n",
            "('words[372]', 13757)\n",
            "('wore', 13758)\n",
            "('work', 13759)\n",
            "('worked', 13760)\n",
            "('worker', 13761)\n",
            "('working', 13762)\n",
            "('workmanship', 13763)\n",
            "('works', 13764)\n",
            "('workshops', 13765)\n",
            "('world', 13766)\n",
            "('world[828]', 13767)\n",
            "('worldly', 13768)\n",
            "('worlds', 13769)\n",
            "('worm', 13770)\n",
            "('worms', 13771)\n",
            "('worn', 13772)\n",
            "('worrying', 13773)\n",
            "('worse', 13774)\n",
            "('worser', 13775)\n",
            "('worship', 13776)\n",
            "('worshipped', 13777)\n",
            "('worst', 13778)\n",
            "('worsted', 13779)\n",
            "('worth', 13780)\n",
            "('worthier', 13781)\n",
            "('worthiest', 13782)\n",
            "('worthiness', 13783)\n",
            "('worthy', 13784)\n",
            "('would', 13785)\n",
            "('wouldest', 13786)\n",
            "('wouldst', 13787)\n",
            "('wound', 13788)\n",
            "('wounded', 13789)\n",
            "('wounds', 13790)\n",
            "('wrapped', 13791)\n",
            "('wrath', 13792)\n",
            "('wrathful', 13793)\n",
            "('wrenched', 13794)\n",
            "('wrested', 13795)\n",
            "('wrestle', 13796)\n",
            "('wretch', 13797)\n",
            "('wretched', 13798)\n",
            "('wretchedness', 13799)\n",
            "('wretches', 13800)\n",
            "('wriggle', 13801)\n",
            "('wrings', 13802)\n",
            "('writ', 13803)\n",
            "('write', 13804)\n",
            "('writer', 13805)\n",
            "('writers', 13806)\n",
            "('writes', 13807)\n",
            "('writhed', 13808)\n",
            "('writhes', 13809)\n",
            "('writhing', 13810)\n",
            "('writing', 13811)\n",
            "('writings', 13812)\n",
            "('written', 13813)\n",
            "('wrong', 13814)\n",
            "('wronged', 13815)\n",
            "('wrote', 13816)\n",
            "('wrought', 13817)\n",
            "('wrung', 13818)\n",
            "('www', 13819)\n",
            "('x', 13820)\n",
            "('xi', 13821)\n",
            "('xii', 13822)\n",
            "('xiii', 13823)\n",
            "('xiii^a', 13824)\n",
            "('xiv', 13825)\n",
            "('xix', 13826)\n",
            "('xv', 13827)\n",
            "('xvi', 13828)\n",
            "('xvii', 13829)\n",
            "('xviii', 13830)\n",
            "('xx', 13831)\n",
            "('xxi', 13832)\n",
            "('xxii', 13833)\n",
            "('xxiii', 13834)\n",
            "('xxiv', 13835)\n",
            "('xxix', 13836)\n",
            "('xxv', 13837)\n",
            "('xxvi', 13838)\n",
            "('xxvii', 13839)\n",
            "('xxviii', 13840)\n",
            "('xxx', 13841)\n",
            "('xxxi', 13842)\n",
            "('xxxii', 13843)\n",
            "('xxxiii', 13844)\n",
            "('xxxiv', 13845)\n",
            "('yards', 13846)\n",
            "('yawned', 13847)\n",
            "('yawning', 13848)\n",
            "('yawns', 13849)\n",
            "('ye', 13850)\n",
            "('yea', 13851)\n",
            "('year', 13852)\n",
            "('year[804]', 13853)\n",
            "('yearned', 13854)\n",
            "('years', 13855)\n",
            "('yelled', 13856)\n",
            "('yellow', 13857)\n",
            "('yells', 13858)\n",
            "('yelp', 13859)\n",
            "('yelping', 13860)\n",
            "('yestermorn', 13861)\n",
            "('yesternight', 13862)\n",
            "('yet', 13863)\n",
            "('yield', 13864)\n",
            "('yielded', 13865)\n",
            "('yieldeth', 13866)\n",
            "('yielding', 13867)\n",
            "('yields', 13868)\n",
            "('yoke', 13869)\n",
            "('yon', 13870)\n",
            "('yonder', 13871)\n",
            "('yore', 13872)\n",
            "('yore[876]', 13873)\n",
            "('you', 13874)\n",
            "('you[362]', 13875)\n",
            "('young', 13876)\n",
            "('younger', 13877)\n",
            "('youngest', 13878)\n",
            "('your', 13879)\n",
            "('yours', 13880)\n",
            "('yourself', 13881)\n",
            "('yourselves', 13882)\n",
            "('youth', 13883)\n",
            "('youthful', 13884)\n",
            "('zabi', 13885)\n",
            "('zeal', 13886)\n",
            "('zeniths', 13887)\n",
            "('zone', 13888)\n",
            "('zone[517]', 13889)\n",
            "('Â£1500', 13890)\n",
            "('Â£500', 13891)\n",
            "('Ãgean', 13892)\n",
            "('Ãgina', 13893)\n",
            "('Ãgina[748]', 13894)\n",
            "('Ãn', 13895)\n",
            "('Ãneas', 13896)\n",
            "('Ãneid', 13897)\n",
            "('Ãsop', 13898)\n",
            "('âAS-ISâ', 13899)\n",
            "('âDefects', 13900)\n",
            "('âInformation', 13901)\n",
            "('âPlain', 13902)\n",
            "('âProject', 13903)\n",
            "('âRight', 13904)\n",
            "('âthe', 13905)\n",
            "('â', 13906)\n",
            "('â¢', 13907)\n",
            "('\\ufeffThe', 13908)\n",
            "('<|endoftext|>', 13909)\n",
            "('<|unk|>', 13910)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Based on the output we can confirm that the special tokens have indeed successfully incorporated into the vocabulary.\n",
        "\n",
        "* Now let's update our tokenizer."
      ],
      "metadata": {
        "id": "BXaHoOmMeGTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer2:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "    preprocessed = [\n",
        "        item.strip() for item in preprocessed if item.strip()\n",
        "    ]\n",
        "    preprocessed = [\n",
        "        item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed\n",
        "    ] #replaces unknown words by <|unk|> tokens\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self,ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r\"\\s+([,.:;?_!\\\"()'])\", r\"\\1\", text) # replaces spaces before the specified punctuation.\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "yPEGuG_see_x"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now test our new tokenizer.\n",
        "* For this we'll use text samples that we concatenate from two different from two independent and unrelated sentences."
      ],
      "metadata": {
        "id": "zcXuEiDQe6n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"In electrical engineering, a transformer is a passive component that transfers electrical energy from one electrical circuit to another circuit, or multiple circuits.\"\n",
        "text2 = \"In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.\"\n",
        "text =  \"<|endoftext|>\".join((text1,text2))\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsTb7BxXfr1x",
        "outputId": "bcd916f5-9a1d-4b31-c25f-36c4604183db"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In electrical engineering, a transformer is a passive component that transfers electrical energy from one electrical circuit to another circuit, or multiple circuits.<|endoftext|>In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now let's tokenize the sample text using `SimpleTokenizer2`on the vocab we previously created in 2.2"
      ],
      "metadata": {
        "id": "gfdZUYWngSVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizer2(vocab)\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txlPGfsCggpG",
        "outputId": "ea2368d1-f7d6-455e-c245-ef2308a8689e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1751, 13910, 7072, 9, 4316, 13910, 8924, 4316, 13910, 13910, 12704, 13910, 13910, 7065, 7856, 10049, 13910, 5630, 12857, 4647, 5630, 9, 10090, 13910, 5631, 12, 13910, 6382, 9141, 9, 13910, 8924, 4612, 13910, 4986, 10046, 12706, 13910, 4861, 13910, 9, 8672, 13623, 12694, 8924, 6053, 12857, 13910, 11197, 5414, 13910, 9, 4624, 6889, 13910, 8924, 6053, 8870, 4316, 13910, 13910, 13910, 7856, 4316, 13755, 13910, 12584, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can see that the list of token IDs contain `13909` for the `<|endoftext|>` seperator token as well as  several `13910` tokens, which are used for unknown words.\n",
        "\n",
        "\n",
        "* Let's detokenize the text for a quick sanity check."
      ],
      "metadata": {
        "id": "0TsxIRtmgyCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenizer.encode(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXtHikzxhOjv",
        "outputId": "0063e9b2-87b2-4e95-e31e-9d0cafddc6c4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In <|unk|> engineering, a <|unk|> is a <|unk|> <|unk|> that <|unk|> <|unk|> energy from one <|unk|> circuit to another circuit, or <|unk|> circuits. <|unk|> deep learning, <|unk|> is an <|unk|> based on the <|unk|> attention <|unk|>, in which text is converted to <|unk|> representations called <|unk|>, and each <|unk|> is converted into a <|unk|> <|unk|> <|unk|> from a word <|unk|> table.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Based on comparing this detokenized text with the original input text, we know that the training dataset, Dante's Inferno, does not contain the words `electrical,architecture etc`.\n",
        "\n",
        "* Depending on the LLM, researchers also consider additional special tokens such as the following:\n",
        "\n",
        "  * [BOS](beginning of sequence) - This tokens marks the start of a text. It signifies to the LLM where a piece of content begins.\n",
        "  * [EOS] (end of sequence) - This token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts similar to `<|endoftext|>`.\n",
        "  * [PAD] (padding) - When training LLMs with batch sizes larger than one, the batch might contain text of varying lengths.To ensure all text have the same lengths, the shorter texts are extended or `padded`using thr [PAD] token, up to the length of the longest text in the batch."
      ],
      "metadata": {
        "id": "fYpEjKK9hbB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Byte pair encoding"
      ],
      "metadata": {
        "id": "ANSYAfN2jyV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Byte Pair Encoding (BPE) tokenizer was used in training LLMs like GPT-2,GPT-3,and the original model used in ChatGPT."
      ],
      "metadata": {
        "id": "Ldv7HpLNj32x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We first need to download a library that does bpe called `tiktoken` using the code below:"
      ],
      "metadata": {
        "id": "osuh6p8AkPFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu2SFdPXkeBf",
        "outputId": "d623536b-e0bd-4dd9-a649-edd079558465"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Once installed, we instantiate the BPE tokenizer for tiktoken as follows:"
      ],
      "metadata": {
        "id": "DH50n1dTkoHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "EVNh4EkXkxAo"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}