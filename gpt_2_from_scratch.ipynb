{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXtcZGgG2uRku7OUm0x8rn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Introduction:Understanding Large Language Models through GPT-2"
      ],
      "metadata": {
        "id": "1S-EKQRfmZTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Large Language models are a class of deep learning models designed to understand and generate human-like text.\n",
        "* They're built using the transformer architecture and are widely used in applications like chatbots,translation and text generation\n"
      ],
      "metadata": {
        "id": "jLIMlxHQmfIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Understanding word embeddings"
      ],
      "metadata": {
        "id": "PJ9DloqAnn-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Deep neural net models, including LLMs, cannot process raw text directly.\n",
        "* Since text is categorical, it is not compatible with the mathematical operations used to implement and train neural networks.Therefore we need a way to represent words as continous-valued vectors.\n",
        "* This concept of converting data into a vector format is referred to embedding.\n",
        "* In simple terms, an embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a convert nonnumeric data into format that neural networks can process.\n",
        "* Instead of treating each word as a unique symbol, embeddings map words into a continuous vector space where similar words are close together.\n",
        "* For example, the vectors for \"king\" and \"queen\" or \"run\" and \"jog\" will be near each other because they share similar meanings. These embeddings are learned from data and capture semantic relationships, making them a fundamental building block in modern NLP models like GPT-2."
      ],
      "metadata": {
        "id": "DAPAiIK8nxUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Tokenizing text"
      ],
      "metadata": {
        "id": "ca9i_4jHpou2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* LLMs take text as inputs but wait before these words are mapped into embeddings, the go through a stage known as tokenization.\n",
        "* Tokenization is the process of breaking text into smaller units called tokens- which can be words,subwords, or even characters.\n",
        "* This step is crucial because language models like GPT-2  don't understand raw text; they work with numbers.\n",
        "* Tokenization converts texts into sequence of tokens that can be mapped to numerical IDs\n",
        "* The text we'll tokenizer for LLM training is `The dante's inferno dataset`.\n",
        "* You can find this dataset from the official project gutenberg website.\n",
        "* Let's get coding"
      ],
      "metadata": {
        "id": "ytG-DzRYpvl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## downloading dante's inferno dataset from project gutenberg website.\n",
        "import urllib.request\n",
        "\n",
        "url = 'https://www.gutenberg.org/cache/epub/41537/pg41537.txt'\n",
        "file_path = \"dante's-inferno.txt\"\n",
        "urllib.request.urlretrieve(url,file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfxwYwh7s06C",
        "outputId": "18efd732-eee3-4da0-ac10-4cb5c2577ee7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"dante's-inferno.txt\", <http.client.HTTPMessage at 0x7890c771d090>)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's load `dante's-inferno.txt` file using python file handling utilities."
      ],
      "metadata": {
        "id": "qLe6SX9ltY2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"dante's-inferno.txt\",'r',encoding='utf-8') as f:\n",
        "  raw_text = f.read()\n",
        "print(\"Total number of characters:\",len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB1fKG9lt0Bg",
        "outputId": "34f9e1f0-6300-4522-dd87-4204e966ba6b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 700670\n",
            "ï»¿The Project Gutenberg eBook of The Divine Comedy of Dante Alighieri: The Inferno\n",
            "    \n",
            "This ebook i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Our goal is to tokenize this 700,670 character long story into individual words and special characters that we can turn into embeddings for LLM training.\n",
        "* Which option do we have in splitting the texts, in this short story we can use python's regular expression library `re` for illustration purposes."
      ],
      "metadata": {
        "id": "zeI8coswwqAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using some simple text we can use `re.split` command with the following syntax to split a text on whitespaces characters."
      ],
      "metadata": {
        "id": "4SCkdqw3xXeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"Hello, world. I am Antonius, nice to meet you.\"\n",
        "result = re.split(r'(\\s)',text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwYj9HT5xpoy",
        "outputId": "6370d64a-b594-490c-ece7-614fbded5dc8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'I', ' ', 'am', ' ', 'Antonius,', ' ', 'nice', ' ', 'to', ' ', 'meet', ' ', 'you.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This simple tokenization scheme mostly works for seperating the use case text into individual words, however some are still connected to punctuation characters that we want to have as seperate entities.\n",
        "* We have also avoided a step in tokenization in which we make all text inputs lowercase because capitalization helps LLMs distinguish between proper nouns and common nouns, understand sentence structure,and learn to geneate text with proper capitalization.\n",
        "\n",
        "\n",
        "* Let's modify the regular expression splits on whitespaces `(\\s)`, commas, and periods ([,.])."
      ],
      "metadata": {
        "id": "RWR70LcTyISL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.] | \\s)',text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49BXSzWtzbSv",
        "outputId": "53904fb4-ca7c-4825-c0db-eb182c81da13"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ', ', 'world', '. ', 'I am Antonius', ', ', 'nice to meet you.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization works well here but let's increase the diversity of punctuation which our dummy tokenizer can work on e.g question marks, double-slashes."
      ],
      "metadata": {
        "id": "c_qS0b5wzzof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is attention is all you need-- a good research paper?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|\\s+|--)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Tjb_wkLX0Fj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04e52bee-38ae-4e6b-bcdb-0c863b843f34"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'attention', ' ', 'is', ' ', 'all', ' ', 'you', ' ', 'need', '--', '', ' ', 'a', ' ', 'good', ' ', 'research', ' ', 'paper', '?', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a basic tokenizer working, let's apply it to Dante's Inferno."
      ],
      "metadata": {
        "id": "9uIVD1ljQL0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text = re.split(r'([,.:;?_!\"()\\']|\\s+|--)',raw_text)\n",
        "preprocessed_text = [item.strip() for item in preprocessed_text if item.strip()]\n",
        "print(len(preprocessed_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUXqlxjuQXEW",
        "outputId": "d69f7662-bdcf-400e-b23c-4e1f844a2d51"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "145180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The print statement outputs `145180` tokens in this text(without whitespaces).\n",
        "Now let's print the first 70 tokens for quick visual check"
      ],
      "metadata": {
        "id": "_LU-kz9rQ5C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed_text[:70])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEdsNmjRRMV5",
        "outputId": "1909e5fa-aab7-4aff-a793-8046a48d7ed6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'Divine', 'Comedy', 'of', 'Dante', 'Alighieri', ':', 'The', 'Inferno', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Converting tokens into token IDs"
      ],
      "metadata": {
        "id": "GsRtx5XDRaNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now, let's convert these tokens from a Python string to an integer representation to produce the token IDs.\n",
        "* This step is an intermediate step before converting the token IDs into embedding vectors.\n",
        "\n",
        "* Since we have tokenized Dante's Inferno and assigned it to Python variable called `preprocessed_text`, let's create a list of all unique tokens and sort them alphabetically to determine the vocabulary size."
      ],
      "metadata": {
        "id": "6DzQ9lNBRerK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed_text))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNS-EkiYSrdf",
        "outputId": "b2883603-1d5a-4af8-c438-5f5b6abc805d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* After finding out the `vocab_size=13909`, we create the vocabulary and print its first 500 entries."
      ],
      "metadata": {
        "id": "FYYQeWgcS9x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab  ={token:integer for integer,token in enumerate(all_words)}\n",
        "for i,item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i >= 500:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qKdtdt3TMa9",
        "outputId": "f3813f59-1933-456f-bff9-2b97dbc05ac9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "('#41537]', 2)\n",
            "('$1', 3)\n",
            "('$5', 4)\n",
            "(\"'\", 5)\n",
            "('(', 6)\n",
            "(')', 7)\n",
            "('***', 8)\n",
            "(',', 9)\n",
            "('-', 10)\n",
            "('--', 11)\n",
            "('.', 12)\n",
            "('000', 13)\n",
            "('1', 14)\n",
            "('10', 15)\n",
            "('100', 16)\n",
            "('101', 17)\n",
            "('102', 18)\n",
            "('103', 19)\n",
            "('1037', 20)\n",
            "('104', 21)\n",
            "('105', 22)\n",
            "('106', 23)\n",
            "('107', 24)\n",
            "('108', 25)\n",
            "('1085', 26)\n",
            "('109', 27)\n",
            "('10th', 28)\n",
            "('11', 29)\n",
            "('110', 30)\n",
            "('1106', 31)\n",
            "('111', 32)\n",
            "('1115', 33)\n",
            "('112', 34)\n",
            "('113', 35)\n",
            "('114', 36)\n",
            "('115', 37)\n",
            "('1152-1190', 38)\n",
            "('116', 39)\n",
            "('117', 40)\n",
            "('118', 41)\n",
            "('1180', 42)\n",
            "('1185', 43)\n",
            "('119', 44)\n",
            "('1193', 45)\n",
            "('1198', 46)\n",
            "('12', 47)\n",
            "('120', 48)\n",
            "('121', 49)\n",
            "('1215', 50)\n",
            "('122', 51)\n",
            "('1220', 52)\n",
            "('123', 53)\n",
            "('1238', 54)\n",
            "('1239', 55)\n",
            "('124', 56)\n",
            "('1248', 57)\n",
            "('1249', 58)\n",
            "('125', 59)\n",
            "('1250', 60)\n",
            "('1252', 61)\n",
            "('1258', 62)\n",
            "('1259', 63)\n",
            "('126', 64)\n",
            "('1260', 65)\n",
            "('1261', 66)\n",
            "('1264', 67)\n",
            "('1265', 68)\n",
            "('1266', 69)\n",
            "('1267', 70)\n",
            "('1268', 71)\n",
            "('1269', 72)\n",
            "('127', 73)\n",
            "('1270', 74)\n",
            "('1271', 75)\n",
            "('1273', 76)\n",
            "('1275', 77)\n",
            "('1277', 78)\n",
            "('1278', 79)\n",
            "('128', 80)\n",
            "('1280', 81)\n",
            "('1281', 82)\n",
            "('1282', 83)\n",
            "('1283', 84)\n",
            "('1284', 85)\n",
            "('1285', 86)\n",
            "('1286', 87)\n",
            "('1288', 88)\n",
            "('1289', 89)\n",
            "('129', 90)\n",
            "('1290', 91)\n",
            "('1291', 92)\n",
            "('1292', 93)\n",
            "('1293', 94)\n",
            "('1294', 95)\n",
            "('1295', 96)\n",
            "('1296', 97)\n",
            "('1297', 98)\n",
            "('1298', 99)\n",
            "('1299', 100)\n",
            "('13', 101)\n",
            "('130', 102)\n",
            "('1300', 103)\n",
            "('1301', 104)\n",
            "('1301-1302', 105)\n",
            "('1301-2', 106)\n",
            "('1302', 107)\n",
            "('1303', 108)\n",
            "('1304', 109)\n",
            "('1305', 110)\n",
            "('1306', 111)\n",
            "('1307', 112)\n",
            "('1308', 113)\n",
            "('1309', 114)\n",
            "('131', 115)\n",
            "('1310', 116)\n",
            "('1311', 117)\n",
            "('1312', 118)\n",
            "('1313', 119)\n",
            "('1313-1316', 120)\n",
            "('1314', 121)\n",
            "('1315', 122)\n",
            "('1316', 123)\n",
            "('132', 124)\n",
            "('1320', 125)\n",
            "('1321', 126)\n",
            "('1326', 127)\n",
            "('1327', 128)\n",
            "('133', 129)\n",
            "('1332', 130)\n",
            "('1333', 131)\n",
            "('134', 132)\n",
            "('1343', 133)\n",
            "('135', 134)\n",
            "('1350', 135)\n",
            "('136', 136)\n",
            "('137', 137)\n",
            "('1370', 138)\n",
            "('138', 139)\n",
            "('139', 140)\n",
            "('14', 141)\n",
            "('140', 142)\n",
            "('141', 143)\n",
            "('142', 144)\n",
            "('143', 145)\n",
            "('144', 146)\n",
            "('145', 147)\n",
            "('1465', 148)\n",
            "('147', 149)\n",
            "('148', 150)\n",
            "('149', 151)\n",
            "('1496', 152)\n",
            "('15', 153)\n",
            "('150', 154)\n",
            "('1500', 155)\n",
            "('151', 156)\n",
            "('153', 157)\n",
            "('1539', 158)\n",
            "('154', 159)\n",
            "('155', 160)\n",
            "('156', 161)\n",
            "('16', 162)\n",
            "('161', 163)\n",
            "('168', 164)\n",
            "('17', 165)\n",
            "('176', 166)\n",
            "('18', 167)\n",
            "('1823', 168)\n",
            "('1826', 169)\n",
            "('1839', 170)\n",
            "('184', 171)\n",
            "('1854', 172)\n",
            "('1861', 173)\n",
            "('1864', 174)\n",
            "('1865', 175)\n",
            "('1879', 176)\n",
            "('1880', 177)\n",
            "('1882', 178)\n",
            "('1884', 179)\n",
            "('19', 180)\n",
            "('192', 181)\n",
            "('199', 182)\n",
            "('1st', 183)\n",
            "('2', 184)\n",
            "('20', 185)\n",
            "('20%', 186)\n",
            "('200', 187)\n",
            "('2001', 188)\n",
            "('2012', 189)\n",
            "('2024', 190)\n",
            "('203', 191)\n",
            "('209', 192)\n",
            "('21', 193)\n",
            "('213', 194)\n",
            "('217', 195)\n",
            "('22', 196)\n",
            "('225', 197)\n",
            "('23', 198)\n",
            "('233', 199)\n",
            "('24', 200)\n",
            "('241', 201)\n",
            "('249', 202)\n",
            "('25', 203)\n",
            "('25th', 204)\n",
            "('26', 205)\n",
            "('260', 206)\n",
            "('264', 207)\n",
            "('268', 208)\n",
            "('269', 209)\n",
            "('26th', 210)\n",
            "('27', 211)\n",
            "('279', 212)\n",
            "('27th', 213)\n",
            "('28', 214)\n",
            "('29', 215)\n",
            "('2d', 216)\n",
            "('3', 217)\n",
            "('30', 218)\n",
            "('30th', 219)\n",
            "('31', 220)\n",
            "('312', 221)\n",
            "('32', 222)\n",
            "('33', 223)\n",
            "('34', 224)\n",
            "('349', 225)\n",
            "('35', 226)\n",
            "('353', 227)\n",
            "('36', 228)\n",
            "('37', 229)\n",
            "('38', 230)\n",
            "('39', 231)\n",
            "('3d', 232)\n",
            "('4', 233)\n",
            "('40', 234)\n",
            "('41', 235)\n",
            "('42', 236)\n",
            "('43', 237)\n",
            "('44', 238)\n",
            "('45', 239)\n",
            "('46', 240)\n",
            "('47', 241)\n",
            "('48', 242)\n",
            "('482', 243)\n",
            "('49', 244)\n",
            "('496', 245)\n",
            "('4th', 246)\n",
            "('5', 247)\n",
            "('50', 248)\n",
            "('501', 249)\n",
            "('51', 250)\n",
            "('52', 251)\n",
            "('53', 252)\n",
            "('54', 253)\n",
            "('55', 254)\n",
            "('552', 255)\n",
            "('56', 256)\n",
            "('57', 257)\n",
            "('58', 258)\n",
            "('586', 259)\n",
            "('59', 260)\n",
            "('596-1887', 261)\n",
            "('5th', 262)\n",
            "('6', 263)\n",
            "('60', 264)\n",
            "('61', 265)\n",
            "('62', 266)\n",
            "('63', 267)\n",
            "('64', 268)\n",
            "('64-6221541', 269)\n",
            "('65', 270)\n",
            "('6500', 271)\n",
            "('66', 272)\n",
            "('67', 273)\n",
            "('68', 274)\n",
            "('69', 275)\n",
            "('6th', 276)\n",
            "('7', 277)\n",
            "('70', 278)\n",
            "('71', 279)\n",
            "('710', 280)\n",
            "('72', 281)\n",
            "('73', 282)\n",
            "('74', 283)\n",
            "('75', 284)\n",
            "('76', 285)\n",
            "('77', 286)\n",
            "('78', 287)\n",
            "('79', 288)\n",
            "('7th', 289)\n",
            "('8', 290)\n",
            "('80', 291)\n",
            "('801', 292)\n",
            "('809', 293)\n",
            "('81', 294)\n",
            "('82', 295)\n",
            "('83', 296)\n",
            "('84', 297)\n",
            "('84116', 298)\n",
            "('85', 299)\n",
            "('86', 300)\n",
            "('87', 301)\n",
            "('88', 302)\n",
            "('883', 303)\n",
            "('89', 304)\n",
            "('8th', 305)\n",
            "('9', 306)\n",
            "('90', 307)\n",
            "('91', 308)\n",
            "('92', 309)\n",
            "('93', 310)\n",
            "('94', 311)\n",
            "('95', 312)\n",
            "('96', 313)\n",
            "('97', 314)\n",
            "('98', 315)\n",
            "('99', 316)\n",
            "('9th', 317)\n",
            "(':', 318)\n",
            "(';', 319)\n",
            "('?', 320)\n",
            "('A', 321)\n",
            "('ACTUAL', 322)\n",
            "('AGREE', 323)\n",
            "('AGREEMENT', 324)\n",
            "('ALIGHIERI', 325)\n",
            "('AN', 326)\n",
            "('AND', 327)\n",
            "('ANY', 328)\n",
            "('ANYTHING', 329)\n",
            "('ASCIIâ', 330)\n",
            "('Abashed', 331)\n",
            "('Abati', 332)\n",
            "('Abbagliato', 333)\n",
            "('Abbey', 334)\n",
            "('Abbot', 335)\n",
            "('Abel', 336)\n",
            "('Abode', 337)\n",
            "('About', 338)\n",
            "('Above', 339)\n",
            "('Abraham', 340)\n",
            "('Abram', 341)\n",
            "('Abruzzi', 342)\n",
            "('Absalom', 343)\n",
            "('Accord', 344)\n",
            "('According', 345)\n",
            "('Accorso', 346)\n",
            "('Accorso[474]', 347)\n",
            "('Accounts', 348)\n",
            "('Accursed', 349)\n",
            "('Accurst', 350)\n",
            "('AccursÃ¨d', 351)\n",
            "('Acheron', 352)\n",
            "('Acheron[451]', 353)\n",
            "('Achilles', 354)\n",
            "('Achilles[781]', 355)\n",
            "('Acquacheta', 356)\n",
            "('Acquacheta[491]', 357)\n",
            "('Acquasparta', 358)\n",
            "('Acre', 359)\n",
            "('Across', 360)\n",
            "('Acting', 361)\n",
            "('Adam', 362)\n",
            "('Adam[769]', 363)\n",
            "('Add', 364)\n",
            "('Adding', 365)\n",
            "('Additional', 366)\n",
            "('Adelasia', 367)\n",
            "('Adelsberg', 368)\n",
            "('Adige', 369)\n",
            "('Adige[393]', 370)\n",
            "('Adimari', 371)\n",
            "('Adriatic', 372)\n",
            "('Afresh', 373)\n",
            "('Africa', 374)\n",
            "('Africanus', 375)\n",
            "('After', 376)\n",
            "('After[282]', 377)\n",
            "('Again', 378)\n",
            "('Against', 379)\n",
            "('Age', 380)\n",
            "('Ages', 381)\n",
            "('Agli', 382)\n",
            "('Aglow', 383)\n",
            "('Agnello', 384)\n",
            "('Ago', 385)\n",
            "('Agrigentum', 386)\n",
            "('Ah', 387)\n",
            "('Ahithophel', 388)\n",
            "('Aim', 389)\n",
            "('Alack', 390)\n",
            "('Alardo', 391)\n",
            "('Alas', 392)\n",
            "('Alberic', 393)\n",
            "('Alberic[850]', 394)\n",
            "('Alberigo', 395)\n",
            "('Albert', 396)\n",
            "('Albert[84]', 397)\n",
            "('Alberti', 398)\n",
            "('Alberto', 399)\n",
            "('Alchemists', 400)\n",
            "('Aldighieri', 401)\n",
            "('Aldighiero', 402)\n",
            "('Aldobrand', 403)\n",
            "('Aldobrandi', 404)\n",
            "('Aldobrando', 405)\n",
            "('Alecto', 406)\n",
            "('Aleppe', 407)\n",
            "('Alert', 408)\n",
            "('Alessio', 409)\n",
            "('Alexander', 410)\n",
            "('Alexander[437]', 411)\n",
            "('Ali', 412)\n",
            "('Ali[723]', 413)\n",
            "('Alichin', 414)\n",
            "('Alichin[591]', 415)\n",
            "('Alichino', 416)\n",
            "('Alighieri', 417)\n",
            "('Alike', 418)\n",
            "('All', 419)\n",
            "('Allegorico', 420)\n",
            "('Alliances', 421)\n",
            "('Allowing', 422)\n",
            "('Allured', 423)\n",
            "('Almighty', 424)\n",
            "('Almost', 425)\n",
            "('Alone', 426)\n",
            "('Along', 427)\n",
            "('Aloof', 428)\n",
            "('Alphonso', 429)\n",
            "('Alps', 430)\n",
            "('Alps[436]', 431)\n",
            "('Already', 432)\n",
            "('Also', 433)\n",
            "('Although', 434)\n",
            "('Always', 435)\n",
            "('Alyscampo', 436)\n",
            "('Am', 437)\n",
            "('Amazement', 438)\n",
            "('Ambassador', 439)\n",
            "('Amen', 440)\n",
            "('Amidei', 441)\n",
            "('Among', 442)\n",
            "('Amor', 443)\n",
            "('AmphiaraÃ¼s', 444)\n",
            "('AmphiarÃ¤us', 445)\n",
            "('Amphion', 446)\n",
            "('Amphion[798]', 447)\n",
            "('AmpÃ¨re', 448)\n",
            "('An', 449)\n",
            "('Anagni', 450)\n",
            "('Anastasius', 451)\n",
            "('Anastasius[376]', 452)\n",
            "('Anaxagoras', 453)\n",
            "('Anchises', 454)\n",
            "('Ancona', 455)\n",
            "('And', 456)\n",
            "('Andrea', 457)\n",
            "('Andrews', 458)\n",
            "('Angelo', 459)\n",
            "('Angels', 460)\n",
            "('Anger', 461)\n",
            "('Angiolel', 462)\n",
            "('Angiolello', 463)\n",
            "('Animal', 464)\n",
            "('Anjou', 465)\n",
            "('Annas', 466)\n",
            "('Anonimo', 467)\n",
            "('Another', 468)\n",
            "('Anselm', 469)\n",
            "('Anselmuccio', 470)\n",
            "('Antenor', 471)\n",
            "('Antenora', 472)\n",
            "('Antioch', 473)\n",
            "('Antiochus', 474)\n",
            "('Antonio', 475)\n",
            "('AntÃ¦us', 476)\n",
            "('AntÃ¦us[790]', 477)\n",
            "('Anxious', 478)\n",
            "('Any', 479)\n",
            "('Apennine', 480)\n",
            "('Apennines', 481)\n",
            "('Apia', 482)\n",
            "('Apocalypse', 483)\n",
            "('Apollo', 484)\n",
            "('Apostle', 485)\n",
            "('Apothecaries', 486)\n",
            "('April', 487)\n",
            "('Apuana', 488)\n",
            "('Apulia', 489)\n",
            "('Apulian', 490)\n",
            "('Apulians', 491)\n",
            "('Aqua', 492)\n",
            "('Aquarius', 493)\n",
            "('Aquarius[630]', 494)\n",
            "('Aquinas', 495)\n",
            "('Aquitaine', 496)\n",
            "('Arab', 497)\n",
            "('Arabian', 498)\n",
            "('Arabic', 499)\n",
            "('Arabs', 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We see that, the dictionary contains individual tokens associated with unique integer labels.\n",
        "* Next we will apply this vocabulary to convert new text into token IDs."
      ],
      "metadata": {
        "id": "ds8yJw0mVwBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's create a tokenizer class in Python with encode method that splits texts into tokens and carries out string-to-integer mapping to produce token IDs via vocabularies.\n",
        "* We also include `decode` method that carries out the reverse integer-to-string mapping to convert the token IDs back into text."
      ],
      "metadata": {
        "id": "ipA-UkCTWLV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "class SimpleTokenizer1:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|\\s|--)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self,ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "pvUK4aLGW3f5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's test this class using a sample paragraph from Dante's Inferno"
      ],
      "metadata": {
        "id": "BDAD37w8ZSSM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c170d54",
        "outputId": "e7b1e79f-45ae-4a60-d00e-03187d0f1fc2"
      },
      "source": [
        "##instantiating the SimpleTokenizer1\n",
        "tokenizer = SimpleTokenizer1(vocab)\n",
        "text = \"\"\"CANTO XXXIV. The Ninth Circle--the Fourth Ring or Judecca, the deepest point\n",
        "of the Inferno and the Centre of the Universe--it is the place\n",
        "of those treacherous to their Lords or Benefactors--Lucifer with\n",
        "Judas, Brutus, and Cassius hanging from his mouths--passage\n",
        "through the Centre of the Earth--ascent from the depths to the\n",
        "light of the stars in the Southern Hemisphere,\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[763, 3412, 12, 3024, 2221, 942, 11, 12706, 1433, 2637, 10090, 1830, 9, 12706, 6385, 10524, 10012, 12706, 1763, 4624, 12706, 883, 10012, 12706, 3193, 11, 8933, 8924, 12706, 10451, 10012, 12769, 12969, 12857, 12710, 1958, 10090, 658, 11, 1969, 13708, 1829, 9, 739, 9, 4624, 843, 8244, 7856, 8442, 9748, 11, 10256, 12806, 12706, 883, 10012, 12706, 1233, 11, 4781, 7856, 12706, 6484, 12857, 12706, 9222, 10012, 12706, 12239, 8672, 12706, 2889, 1663, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's see if we can turn these token IDs back into text using the `decode` method."
      ],
      "metadata": {
        "id": "kv3wPfmJZf02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wldJqWwNZrMI",
        "outputId": "cc5e08b0-5c87-4207-a5ed-85f4c199b37a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CANTO XXXIV. The Ninth Circle -- the Fourth Ring or Judecca, the deepest point of the Inferno and the Centre of the Universe -- it is the place of those treacherous to their Lords or Benefactors -- Lucifer with Judas, Brutus, and Cassius hanging from his mouths -- passage through the Centre of the Earth -- ascent from the depths to the light of the stars in the Southern Hemisphere,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* So I guess i works now let's apply it to a new text sample not contained in the training set:"
      ],
      "metadata": {
        "id": "e7iHq_sXZ3NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, welcome to Bogota\"\n",
        "##print(tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "Gh5JHcKXaEiW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can see that after we execute this code we get the above error.\n",
        "* We're getting error because the word `Hello` was not used in `Dante's Inferno` story.\n",
        "* Hence, it is not contained in the vocabulary.\n",
        "* This highlights the need to consider large and diverse training sets to extend the vocabulary when working on LLMs.\n",
        "\n",
        "* Next we'll try to test the tokenizer further on text that contain unkown words and also special tokens that can be used to provide further context for an LLM during training."
      ],
      "metadata": {
        "id": "6lIr-dQramq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Adding special context tokens"
      ],
      "metadata": {
        "id": "AzUesKJPb7_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Our task now is to modify the tokenizer to handle unknown words.\n",
        "* And also need to address the usage and additional of special context tokens that can enhance a model's understading of context or other information.\n",
        "* These special tokens iclude markers for unknown words and document boundaries.\n",
        "\n",
        "* Let'smodify the vocabulary to include these two special tokens , `<unk>` and `<|endoftext|>`, by adding them to our list of all unique words."
      ],
      "metadata": {
        "id": "3Acu6GU1cDgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed_text)))\n",
        "all_tokens.append(\"<|endoftext|>\") # Append as individual string\n",
        "all_tokens.append(\"<|unk|>\") # Append as individual string\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhhvHCNLdZ56",
        "outputId": "b06f1bc9-683d-4863-8b31-1031641dbd96"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now let's print the last 500 enties of the updated vocabulary:"
      ],
      "metadata": {
        "id": "MCRs3JDfdqgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-500:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDQpxMEkd1BZ",
        "outputId": "3a8f9556-baab-4340-8e8a-4ea461b8a919"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('visage', 13411)\n",
            "('visible', 13412)\n",
            "('vision', 13413)\n",
            "('visions', 13414)\n",
            "('visit', 13415)\n",
            "('visited', 13416)\n",
            "('visiting', 13417)\n",
            "('vital', 13418)\n",
            "('vitally', 13419)\n",
            "('vitiate', 13420)\n",
            "('vivid', 13421)\n",
            "('vividly', 13422)\n",
            "('vizor', 13423)\n",
            "('vobis', 13424)\n",
            "('vocal', 13425)\n",
            "('vocation', 13426)\n",
            "('vogue', 13427)\n",
            "('voice', 13428)\n",
            "('voices', 13429)\n",
            "('void', 13430)\n",
            "('vol', 13431)\n",
            "('volition', 13432)\n",
            "('volume', 13433)\n",
            "('voluntarily', 13434)\n",
            "('volunteer', 13435)\n",
            "('volunteers', 13436)\n",
            "('vomiting', 13437)\n",
            "('voracity', 13438)\n",
            "('voted', 13439)\n",
            "('vouches', 13440)\n",
            "('vow', 13441)\n",
            "('voyage', 13442)\n",
            "('vulgar', 13443)\n",
            "('wafted', 13444)\n",
            "('wage', 13445)\n",
            "('waged', 13446)\n",
            "('wager', 13447)\n",
            "('wages', 13448)\n",
            "('wagged', 13449)\n",
            "('waging', 13450)\n",
            "('wags', 13451)\n",
            "('wail', 13452)\n",
            "('wailed', 13453)\n",
            "('wailed[530]', 13454)\n",
            "('wailing', 13455)\n",
            "('wailings', 13456)\n",
            "('wails', 13457)\n",
            "('waist', 13458)\n",
            "('wait', 13459)\n",
            "('waiting', 13460)\n",
            "('waits', 13461)\n",
            "('wake', 13462)\n",
            "('waketh', 13463)\n",
            "('waking', 13464)\n",
            "('walk', 13465)\n",
            "('walks', 13466)\n",
            "('wall', 13467)\n",
            "('wall-painting', 13468)\n",
            "('wall[651]', 13469)\n",
            "('walled', 13470)\n",
            "('wallet', 13471)\n",
            "('wallow', 13472)\n",
            "('walls', 13473)\n",
            "('wand', 13474)\n",
            "('wander', 13475)\n",
            "('wandered', 13476)\n",
            "('wanderer', 13477)\n",
            "('wandering', 13478)\n",
            "('wanderings', 13479)\n",
            "('wanders', 13480)\n",
            "('waned', 13481)\n",
            "('wanness', 13482)\n",
            "('want', 13483)\n",
            "('wanted', 13484)\n",
            "('wanting', 13485)\n",
            "('war', 13486)\n",
            "('war-bell', 13487)\n",
            "('war-cries', 13488)\n",
            "('ward', 13489)\n",
            "('warders', 13490)\n",
            "('wards', 13491)\n",
            "('warehouses', 13492)\n",
            "('warlike', 13493)\n",
            "('warm', 13494)\n",
            "('warmed', 13495)\n",
            "('warmest', 13496)\n",
            "('warmly', 13497)\n",
            "('warms', 13498)\n",
            "('warmth', 13499)\n",
            "('warned', 13500)\n",
            "('warning', 13501)\n",
            "('warns', 13502)\n",
            "('warranties', 13503)\n",
            "('warrior', 13504)\n",
            "('wars', 13505)\n",
            "('wary', 13506)\n",
            "('was', 13507)\n",
            "('wash', 13508)\n",
            "('washed', 13509)\n",
            "('wasps', 13510)\n",
            "('wast', 13511)\n",
            "('waste', 13512)\n",
            "('waste[396]', 13513)\n",
            "('wasted', 13514)\n",
            "('watch', 13515)\n",
            "('watch-dog', 13516)\n",
            "('watched', 13517)\n",
            "('watchfulness', 13518)\n",
            "('watching', 13519)\n",
            "('water', 13520)\n",
            "('water-brooks', 13521)\n",
            "('waterfall', 13522)\n",
            "('waterfalls', 13523)\n",
            "('waters', 13524)\n",
            "('watery', 13525)\n",
            "('wave', 13526)\n",
            "('waver', 13527)\n",
            "('wavered', 13528)\n",
            "('wavers', 13529)\n",
            "('waves', 13530)\n",
            "('waving', 13531)\n",
            "('wax', 13532)\n",
            "('waxed', 13533)\n",
            "('waxen', 13534)\n",
            "('way', 13535)\n",
            "('ways', 13536)\n",
            "('we', 13537)\n",
            "('weak', 13538)\n",
            "('weaken', 13539)\n",
            "('weakness', 13540)\n",
            "('wealth', 13541)\n",
            "('wealthier', 13542)\n",
            "('wealthy', 13543)\n",
            "('weapon', 13544)\n",
            "('wear', 13545)\n",
            "('wearers', 13546)\n",
            "('wearied', 13547)\n",
            "('weariness', 13548)\n",
            "('wearing', 13549)\n",
            "('wears', 13550)\n",
            "('weary', 13551)\n",
            "('weather', 13552)\n",
            "('weathered', 13553)\n",
            "('weaver', 13554)\n",
            "('weavers', 13555)\n",
            "('web', 13556)\n",
            "('website', 13557)\n",
            "('wed', 13558)\n",
            "('wedded', 13559)\n",
            "('wedding', 13560)\n",
            "('weeks', 13561)\n",
            "('ween', 13562)\n",
            "('weep', 13563)\n",
            "('weeping', 13564)\n",
            "('weeps', 13565)\n",
            "('weet', 13566)\n",
            "('weighed', 13567)\n",
            "('weighs', 13568)\n",
            "('weight', 13569)\n",
            "('weight[397]', 13570)\n",
            "('weights', 13571)\n",
            "('weighty', 13572)\n",
            "('welcome', 13573)\n",
            "('welcomed', 13574)\n",
            "('welcoming', 13575)\n",
            "('welfare', 13576)\n",
            "('well', 13577)\n",
            "('well-ascertained', 13578)\n",
            "('well-born', 13579)\n",
            "('well-defined', 13580)\n",
            "('well-fought', 13581)\n",
            "('well-informed', 13582)\n",
            "('well-known', 13583)\n",
            "('well-peeled', 13584)\n",
            "('well-wooded', 13585)\n",
            "('well[516]', 13586)\n",
            "('wellbeing', 13587)\n",
            "('wellnigh', 13588)\n",
            "('wends', 13589)\n",
            "('went', 13590)\n",
            "('wept', 13591)\n",
            "('were', 13592)\n",
            "('wert', 13593)\n",
            "('west', 13594)\n",
            "('western', 13595)\n",
            "('westwards', 13596)\n",
            "('whale', 13597)\n",
            "('what', 13598)\n",
            "('what[733]', 13599)\n",
            "('whatever', 13600)\n",
            "('whatsoever', 13601)\n",
            "('wheel', 13602)\n",
            "('wheeled', 13603)\n",
            "('wheeling', 13604)\n",
            "('wheels', 13605)\n",
            "('whelmed', 13606)\n",
            "('whelmÃ¨d', 13607)\n",
            "('when', 13608)\n",
            "('when[288]', 13609)\n",
            "('whence', 13610)\n",
            "('whene', 13611)\n",
            "('whenever', 13612)\n",
            "('where', 13613)\n",
            "('where[859]', 13614)\n",
            "('whereby', 13615)\n",
            "('wherefore', 13616)\n",
            "('wherein', 13617)\n",
            "('whereon', 13618)\n",
            "('wheresoe', 13619)\n",
            "('whereupon', 13620)\n",
            "('wherever', 13621)\n",
            "('whether', 13622)\n",
            "('which', 13623)\n",
            "('while', 13624)\n",
            "('whilom', 13625)\n",
            "('whip', 13626)\n",
            "('whipped', 13627)\n",
            "('whirling', 13628)\n",
            "('whirls', 13629)\n",
            "('whirlwind', 13630)\n",
            "('whist', 13631)\n",
            "('whistle', 13632)\n",
            "('whit', 13633)\n",
            "('white', 13634)\n",
            "('whither', 13635)\n",
            "('who', 13636)\n",
            "('whoever', 13637)\n",
            "('whole', 13638)\n",
            "('wholeness', 13639)\n",
            "('wholesome', 13640)\n",
            "('wholly', 13641)\n",
            "('whom', 13642)\n",
            "('whom[463]', 13643)\n",
            "('whore', 13644)\n",
            "('whose', 13645)\n",
            "('whoso', 13646)\n",
            "('why', 13647)\n",
            "('wick', 13648)\n",
            "('wicked', 13649)\n",
            "('wickedness', 13650)\n",
            "('wide', 13651)\n",
            "('widely', 13652)\n",
            "('widened', 13653)\n",
            "('widening', 13654)\n",
            "('wider', 13655)\n",
            "('widespread', 13656)\n",
            "('widest', 13657)\n",
            "('widow', 13658)\n",
            "('width', 13659)\n",
            "('wield', 13660)\n",
            "('wielded', 13661)\n",
            "('wife', 13662)\n",
            "('wight', 13663)\n",
            "('wild', 13664)\n",
            "('wilderness', 13665)\n",
            "('wile', 13666)\n",
            "('wiles', 13667)\n",
            "('will', 13668)\n",
            "('willed', 13669)\n",
            "('willed[252]', 13670)\n",
            "('willing', 13671)\n",
            "('willingly', 13672)\n",
            "('willingness', 13673)\n",
            "('wills', 13674)\n",
            "('wilt', 13675)\n",
            "('wily', 13676)\n",
            "('win', 13677)\n",
            "('winces', 13678)\n",
            "('wind', 13679)\n",
            "('windmill', 13680)\n",
            "('window', 13681)\n",
            "('windpipe', 13682)\n",
            "('winds', 13683)\n",
            "('wine', 13684)\n",
            "('wine-bibbers', 13685)\n",
            "('wing', 13686)\n",
            "('winged', 13687)\n",
            "('wings', 13688)\n",
            "('winning', 13689)\n",
            "('wins', 13690)\n",
            "('winter', 13691)\n",
            "('winter-tide', 13692)\n",
            "('winter-time', 13693)\n",
            "('wintry', 13694)\n",
            "('wipe', 13695)\n",
            "('wiped', 13696)\n",
            "('wisdom', 13697)\n",
            "('wise', 13698)\n",
            "('wisely', 13699)\n",
            "('wiser', 13700)\n",
            "('wisest', 13701)\n",
            "('wish', 13702)\n",
            "('wished', 13703)\n",
            "('wishes', 13704)\n",
            "('wishing', 13705)\n",
            "('wit', 13706)\n",
            "('witch', 13707)\n",
            "('with', 13708)\n",
            "('withdraw', 13709)\n",
            "('withdrawn', 13710)\n",
            "('withdrew', 13711)\n",
            "('withes', 13712)\n",
            "('withhold', 13713)\n",
            "('within', 13714)\n",
            "('without', 13715)\n",
            "('withstanding', 13716)\n",
            "('withstood', 13717)\n",
            "('witless', 13718)\n",
            "('witness', 13719)\n",
            "('witnessed', 13720)\n",
            "('witnessing', 13721)\n",
            "('wits', 13722)\n",
            "('witty', 13723)\n",
            "('wives', 13724)\n",
            "('wizards', 13725)\n",
            "('woe', 13726)\n",
            "('woeful', 13727)\n",
            "('woes', 13728)\n",
            "('woful', 13729)\n",
            "('woke', 13730)\n",
            "('wolf', 13731)\n",
            "('wolf-cubs', 13732)\n",
            "('wolves', 13733)\n",
            "('woman', 13734)\n",
            "('woman-like', 13735)\n",
            "('womanhood', 13736)\n",
            "('womankind', 13737)\n",
            "('womanly', 13738)\n",
            "('women', 13739)\n",
            "('won', 13740)\n",
            "('wonder', 13741)\n",
            "('wonderful', 13742)\n",
            "('wondering', 13743)\n",
            "('wonders', 13744)\n",
            "('wondrous', 13745)\n",
            "('wont', 13746)\n",
            "('wonted', 13747)\n",
            "('wood', 13748)\n",
            "('wood[161]', 13749)\n",
            "('wooden', 13750)\n",
            "('woodland', 13751)\n",
            "('woods', 13752)\n",
            "('woody', 13753)\n",
            "('woollens', 13754)\n",
            "('word', 13755)\n",
            "('words', 13756)\n",
            "('words[372]', 13757)\n",
            "('wore', 13758)\n",
            "('work', 13759)\n",
            "('worked', 13760)\n",
            "('worker', 13761)\n",
            "('working', 13762)\n",
            "('workmanship', 13763)\n",
            "('works', 13764)\n",
            "('workshops', 13765)\n",
            "('world', 13766)\n",
            "('world[828]', 13767)\n",
            "('worldly', 13768)\n",
            "('worlds', 13769)\n",
            "('worm', 13770)\n",
            "('worms', 13771)\n",
            "('worn', 13772)\n",
            "('worrying', 13773)\n",
            "('worse', 13774)\n",
            "('worser', 13775)\n",
            "('worship', 13776)\n",
            "('worshipped', 13777)\n",
            "('worst', 13778)\n",
            "('worsted', 13779)\n",
            "('worth', 13780)\n",
            "('worthier', 13781)\n",
            "('worthiest', 13782)\n",
            "('worthiness', 13783)\n",
            "('worthy', 13784)\n",
            "('would', 13785)\n",
            "('wouldest', 13786)\n",
            "('wouldst', 13787)\n",
            "('wound', 13788)\n",
            "('wounded', 13789)\n",
            "('wounds', 13790)\n",
            "('wrapped', 13791)\n",
            "('wrath', 13792)\n",
            "('wrathful', 13793)\n",
            "('wrenched', 13794)\n",
            "('wrested', 13795)\n",
            "('wrestle', 13796)\n",
            "('wretch', 13797)\n",
            "('wretched', 13798)\n",
            "('wretchedness', 13799)\n",
            "('wretches', 13800)\n",
            "('wriggle', 13801)\n",
            "('wrings', 13802)\n",
            "('writ', 13803)\n",
            "('write', 13804)\n",
            "('writer', 13805)\n",
            "('writers', 13806)\n",
            "('writes', 13807)\n",
            "('writhed', 13808)\n",
            "('writhes', 13809)\n",
            "('writhing', 13810)\n",
            "('writing', 13811)\n",
            "('writings', 13812)\n",
            "('written', 13813)\n",
            "('wrong', 13814)\n",
            "('wronged', 13815)\n",
            "('wrote', 13816)\n",
            "('wrought', 13817)\n",
            "('wrung', 13818)\n",
            "('www', 13819)\n",
            "('x', 13820)\n",
            "('xi', 13821)\n",
            "('xii', 13822)\n",
            "('xiii', 13823)\n",
            "('xiii^a', 13824)\n",
            "('xiv', 13825)\n",
            "('xix', 13826)\n",
            "('xv', 13827)\n",
            "('xvi', 13828)\n",
            "('xvii', 13829)\n",
            "('xviii', 13830)\n",
            "('xx', 13831)\n",
            "('xxi', 13832)\n",
            "('xxii', 13833)\n",
            "('xxiii', 13834)\n",
            "('xxiv', 13835)\n",
            "('xxix', 13836)\n",
            "('xxv', 13837)\n",
            "('xxvi', 13838)\n",
            "('xxvii', 13839)\n",
            "('xxviii', 13840)\n",
            "('xxx', 13841)\n",
            "('xxxi', 13842)\n",
            "('xxxii', 13843)\n",
            "('xxxiii', 13844)\n",
            "('xxxiv', 13845)\n",
            "('yards', 13846)\n",
            "('yawned', 13847)\n",
            "('yawning', 13848)\n",
            "('yawns', 13849)\n",
            "('ye', 13850)\n",
            "('yea', 13851)\n",
            "('year', 13852)\n",
            "('year[804]', 13853)\n",
            "('yearned', 13854)\n",
            "('years', 13855)\n",
            "('yelled', 13856)\n",
            "('yellow', 13857)\n",
            "('yells', 13858)\n",
            "('yelp', 13859)\n",
            "('yelping', 13860)\n",
            "('yestermorn', 13861)\n",
            "('yesternight', 13862)\n",
            "('yet', 13863)\n",
            "('yield', 13864)\n",
            "('yielded', 13865)\n",
            "('yieldeth', 13866)\n",
            "('yielding', 13867)\n",
            "('yields', 13868)\n",
            "('yoke', 13869)\n",
            "('yon', 13870)\n",
            "('yonder', 13871)\n",
            "('yore', 13872)\n",
            "('yore[876]', 13873)\n",
            "('you', 13874)\n",
            "('you[362]', 13875)\n",
            "('young', 13876)\n",
            "('younger', 13877)\n",
            "('youngest', 13878)\n",
            "('your', 13879)\n",
            "('yours', 13880)\n",
            "('yourself', 13881)\n",
            "('yourselves', 13882)\n",
            "('youth', 13883)\n",
            "('youthful', 13884)\n",
            "('zabi', 13885)\n",
            "('zeal', 13886)\n",
            "('zeniths', 13887)\n",
            "('zone', 13888)\n",
            "('zone[517]', 13889)\n",
            "('Â£1500', 13890)\n",
            "('Â£500', 13891)\n",
            "('Ãgean', 13892)\n",
            "('Ãgina', 13893)\n",
            "('Ãgina[748]', 13894)\n",
            "('Ãn', 13895)\n",
            "('Ãneas', 13896)\n",
            "('Ãneid', 13897)\n",
            "('Ãsop', 13898)\n",
            "('âAS-ISâ', 13899)\n",
            "('âDefects', 13900)\n",
            "('âInformation', 13901)\n",
            "('âPlain', 13902)\n",
            "('âProject', 13903)\n",
            "('âRight', 13904)\n",
            "('âthe', 13905)\n",
            "('â', 13906)\n",
            "('â¢', 13907)\n",
            "('\\ufeffThe', 13908)\n",
            "('<|endoftext|>', 13909)\n",
            "('<|unk|>', 13910)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Based on the output we can confirm that the special tokens have indeed successfully incorporated into the vocabulary.\n",
        "\n",
        "* Now let's update our tokenizer."
      ],
      "metadata": {
        "id": "BXaHoOmMeGTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer2:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "    preprocessed = [\n",
        "        item.strip() for item in preprocessed if item.strip()\n",
        "    ]\n",
        "    preprocessed = [\n",
        "        item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed\n",
        "    ] #replaces unknown words by <|unk|> tokens\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self,ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r\"\\s+([,.:;?_!\\\"()'])\", r\"\\1\", text) # replaces spaces before the specified punctuation.\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "yPEGuG_see_x"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now test our new tokenizer.\n",
        "* For this we'll use text samples that we concatenate from two different from two independent and unrelated sentences."
      ],
      "metadata": {
        "id": "zcXuEiDQe6n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"In electrical engineering, a transformer is a passive component that transfers electrical energy from one electrical circuit to another circuit, or multiple circuits.\"\n",
        "text2 = \"In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.\"\n",
        "text =  \"<|endoftext|>\".join((text1,text2))\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsTb7BxXfr1x",
        "outputId": "a711649f-d6ef-4b84-e367-176d63288b53"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In electrical engineering, a transformer is a passive component that transfers electrical energy from one electrical circuit to another circuit, or multiple circuits.<|endoftext|>In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now let's tokenize the sample text using `SimpleTokenizer2`on the vocab we previously created in 2.2"
      ],
      "metadata": {
        "id": "gfdZUYWngSVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizer2(vocab)\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txlPGfsCggpG",
        "outputId": "4ba79c18-44bd-4b10-ee78-f4d3a63b04cb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1751, 13910, 7072, 9, 4316, 13910, 8924, 4316, 13910, 13910, 12704, 13910, 13910, 7065, 7856, 10049, 13910, 5630, 12857, 4647, 5630, 9, 10090, 13910, 5631, 12, 13910, 6382, 9141, 9, 13910, 8924, 4612, 13910, 4986, 10046, 12706, 13910, 4861, 13910, 9, 8672, 13623, 12694, 8924, 6053, 12857, 13910, 11197, 5414, 13910, 9, 4624, 6889, 13910, 8924, 6053, 8870, 4316, 13910, 13910, 13910, 7856, 4316, 13755, 13910, 12584, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can see that the list of token IDs contain `13909` for the `<|endoftext|>` seperator token as well as  several `13910` tokens, which are used for unknown words.\n",
        "\n",
        "\n",
        "* Let's detokenize the text for a quick sanity check."
      ],
      "metadata": {
        "id": "0TsxIRtmgyCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenizer.encode(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXtHikzxhOjv",
        "outputId": "e87784de-b518-484f-e283-4720393d3028"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In <|unk|> engineering, a <|unk|> is a <|unk|> <|unk|> that <|unk|> <|unk|> energy from one <|unk|> circuit to another circuit, or <|unk|> circuits. <|unk|> deep learning, <|unk|> is an <|unk|> based on the <|unk|> attention <|unk|>, in which text is converted to <|unk|> representations called <|unk|>, and each <|unk|> is converted into a <|unk|> <|unk|> <|unk|> from a word <|unk|> table.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Based on comparing this detokenized text with the original input text, we know that the training dataset, Dante's Inferno, does not contain the words `electrical,architecture etc`.\n",
        "\n",
        "* Depending on the LLM, researchers also consider additional special tokens such as the following:\n",
        "\n",
        "  * [BOS](beginning of sequence) - This tokens marks the start of a text. It signifies to the LLM where a piece of content begins.\n",
        "  * [EOS] (end of sequence) - This token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts similar to `<|endoftext|>`.\n",
        "  * [PAD] (padding) - When training LLMs with batch sizes larger than one, the batch might contain text of varying lengths.To ensure all text have the same lengths, the shorter texts are extended or `padded`using thr [PAD] token, up to the length of the longest text in the batch."
      ],
      "metadata": {
        "id": "fYpEjKK9hbB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Byte pair encoding"
      ],
      "metadata": {
        "id": "ANSYAfN2jyV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Byte Pair Encoding (BPE) tokenizer was used in training LLMs like GPT-2,GPT-3,and the original model used in ChatGPT."
      ],
      "metadata": {
        "id": "Ldv7HpLNj32x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We first need to download a library that does bpe called `tiktoken` using the code below:"
      ],
      "metadata": {
        "id": "osuh6p8AkPFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu2SFdPXkeBf",
        "outputId": "6e45c07e-841b-48e4-f80e-b990ed30ac47"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Once installed, we instantiate the BPE tokenizer for tiktoken as follows:"
      ],
      "metadata": {
        "id": "DH50n1dTkoHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "EVNh4EkXkxAo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Example usage of tiktoken tokenizer is similar to `SimpleTokenizer2`."
      ],
      "metadata": {
        "id": "DRVjLbUkF64p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello,do you like coding? <|endoftext|> In the nvidia auditorium\"\n",
        "\n",
        "integers = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxkTjcuAGGWf",
        "outputId": "e20c35f2-48e8-4d39-cbf3-1514358da37f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 4598, 345, 588, 19617, 30, 220, 50256, 554, 262, 299, 21744, 30625, 1505]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can also convert the token IDs back into text using decode method."
      ],
      "metadata": {
        "id": "HbEDYe_IGtuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jDMLF6VG39W",
        "outputId": "df80ea79-21ea-42fe-b8c9-ddf0d0e9d930"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello,do you like coding? <|endoftext|> In the nvidia auditorium\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Data sampling with a sliding window"
      ],
      "metadata": {
        "id": "2W9uk58aHlFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The next step in creating the embeddings for the LLM is to generate the input-target pairs which are essential for training an LLM.\n",
        "* LLMs are pretrained by predicting the next word in a text.\n",
        "* Let's implement a dataloader that fetches the input-target pairs from training dataset using a sliding window approach.\n",
        "* To start this process, we will tokenize the whole `Dante's Inferno` story using BPE tokenizer."
      ],
      "metadata": {
        "id": "v0D47l1HHqRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"dante's-inferno.txt\",'r',encoding='utf-8') as f:\n",
        "  raw_text = f.read()\n",
        "encoded_text = tokenizer.encode(raw_text)\n",
        "print(len(encoded_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ski4wo1ZI9pN",
        "outputId": "e17e6b40-1eb6-4281-d3b5-695f3cf59ff6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "203798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can see that after executing the code above we that our dataset has `203798` tokens.\n",
        "* Now let's remove the first 500 tokens from the dataset for demonstration purposes."
      ],
      "metadata": {
        "id": "8RUxM-nNJaBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sample = encoded_text[500:]"
      ],
      "metadata": {
        "id": "kWgGKGMkJweg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* One of the easiest and most intuitive ways to create input-target pairs for the next-word prediction task is to create 2 variables `x` and `y`,where `x`contains the input tokens and `y` contains the targets,which are inputs shifted by 1:"
      ],
      "metadata": {
        "id": "iBYPC7hJJ_jX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4\n",
        "x = encoded_sample[:context_size]\n",
        "y = encoded_sample[1:context_size+1]\n",
        "print(f\"x:{x}\")\n",
        "print(f\"y:       {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN8lC4kWKfwv",
        "outputId": "f98720a5-4ea3-4efa-d7f8-7d9d6ba226be"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:[561, 475, 1165, 198]\n",
            "y:       [475, 1165, 198, 77]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* By processing the inputs along with targets,which are inputs are shifted by one position, we can create the next-word prediction task."
      ],
      "metadata": {
        "id": "aBGTJ1cfK2yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,context_size+1):\n",
        "  context = encoded_sample[:i]\n",
        "  desired = encoded_sample[i]\n",
        "  print(context,\"------>\",desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i15qFrfwLugH",
        "outputId": "d515edf1-6451-4240-8d15-06e9f327bc9b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[561] ------> 475\n",
            "[561, 475] ------> 1165\n",
            "[561, 475, 1165] ------> 198\n",
            "[561, 475, 1165, 198] ------> 77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Everything left to the arrow refers to the input an LLM would receive, and token ID on the right side of arrow represents the target token ID that the LLM is supossed to predict.\n",
        "* Now let's do the reverse where we convert token IDs into text:"
      ],
      "metadata": {
        "id": "Y4jMP1jjMf0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,context_size+1):\n",
        "  context = encoded_sample[:i]\n",
        "  desired = encoded_sample[i]\n",
        "  print(tokenizer.decode(context),\"------>\",tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCUrQbnhNCed",
        "outputId": "7e2812bd-f58f-491f-aeed-69a5bf94dc96"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " would ------>  but\n",
            " would but ------>  too\n",
            " would but too ------> \n",
            "\n",
            " would but too\n",
            " ------> n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We have now create the input-target pairs that we can use for LLM training\n",
        "\n",
        "* There's only one more task before we can turn tokens into embeddings.\n",
        "* That is implementing an efficient dataloader that iterates over the input dataset and returns the inputs ad targets as Pytorch tensor, which can be thought as multidimensional arrays.\n",
        "\n",
        "* For the efficient data loader implementation,we'll use Pytorch's built-in `Dataset` and `DataLoader` classes."
      ],
      "metadata": {
        "id": "T9rr8zeXNn9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class GPTDataset1(Dataset):\n",
        "  def __init__(self,txt,tokenizer,max_length,stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt)#tokenizing the entire text\n",
        "\n",
        "    for i in range(0,len(token_ids)- max_length,stride):#using sliding window to chunk the book into overlapping sequenced of max_length\n",
        "      input_sample = token_ids[i:i + max_length]\n",
        "      target_sample = token_ids[i+1:i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_sample))\n",
        "      self.target_ids.append(torch.tensor(target_sample))\n",
        "\n",
        "  def __len__(self):#returns the total number of rows in the dataset\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self,idx):#returns a single row fromthe dataset\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "EnDP-0lcA6Xq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The class above is based on Pytorch `Dataset` class and defines how individual rows are fetched from the dataset, where each row consists of a number of token IDs (based on a `max_length`) assigned to an `input_sample`tensor.\n",
        "* The `target_chunk ` tenosr contains the corresponding targets.\n",
        "\n"
      ],
      "metadata": {
        "id": "x0YZZMLAC8zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader1(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding('gpt2')#intializing the tokenizer\n",
        "  dataset = GPTDataset1(txt,tokenizer,max_length,stride)#creating the dataset\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last=drop_last,#drop_last= True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training.\n",
        "      num_workers=num_workers#number of CPU processes to use for preprocessing.\n",
        "\n",
        "      )\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "RQ-qeEBJDwua"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's test the `dataloader` with a batch of size 1 for an LLM with a context size of 4 :"
      ],
      "metadata": {
        "id": "vDFrAWeOISzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"dante's-inferno.txt\",'r',encoding='utf-8') as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "dataloader = create_dataloader1(\n",
        "    raw_text,batch_size=1,max_length=4,stride=1,shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc5o_zjnIfqe",
        "outputId": "6f421272-c5e3-412e-c056-fef4d88fcf7b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[171, 119, 123, 464]]), tensor([[ 119,  123,  464, 4935]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first_batch variable contains two tensors: the first stores the input token IDs, and the second contains the target token IDs.\n",
        "* Since the `max_length` is set to 4, each of the two tensors contain 4 token Ids.\n",
        "\n",
        "* To understand the meaninng of `stride=1`, let's fetch another batch from this dataset:"
      ],
      "metadata": {
        "id": "OrXWFsBbJTmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fv-waSZJ1N4",
        "outputId": "44353cee-e958-4ff0-dd98-3e43c8cd7311"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 119,  123,  464, 4935]]), tensor([[  123,   464,  4935, 20336]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* If we compare the first and second batches, we see that the second batch's token IDs are shifted by one position.\n",
        "\n",
        "* Let's try using a batch size that is greater than one:"
      ],
      "metadata": {
        "id": "PVKbqDgeJ77r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader1(\n",
        "    raw_text,batch_size=8,max_length=4,stride=4,\n",
        "    shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\",inputs)\n",
        "print(\"\\nTargets:\\n\",targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyGgdPbTKXky",
        "outputId": "d0aea256-c170-4eae-d06f-d80a1dd79269"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[  171,   119,   123,   464],\n",
            "        [ 4935, 20336, 46566,   286],\n",
            "        [  383, 13009, 22329,   286],\n",
            "        [34898,   978,   394, 29864],\n",
            "        [   25,   383, 32458,   198],\n",
            "        [  220,   220,   220,   220],\n",
            "        [  198,  1212, 47179,   318],\n",
            "        [  329,   262,   779,   286]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  119,   123,   464,  4935],\n",
            "        [20336, 46566,   286,   383],\n",
            "        [13009, 22329,   286, 34898],\n",
            "        [  978,   394, 29864,    25],\n",
            "        [  383, 32458,   198,   220],\n",
            "        [  220,   220,   220,   198],\n",
            "        [ 1212, 47179,   318,   329],\n",
            "        [  262,   779,   286,  2687]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Note that we have increased the stride to 4 to utilize the dataset fully.\n",
        "* This avoids any overlapping between the batches since more overlapping could lead to increased overfitting."
      ],
      "metadata": {
        "id": "kaWDOx_rLFQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 Creating token embeddings"
      ],
      "metadata": {
        "id": "kn23Ag9mLaMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The last step in preparing the input text for LLM training is to convert the token IDs into embedding vectors.\n",
        "* A continous vector representation, or embedding, is necessary since GPT-like LLMs are deep neural networks trained with backpropagation algorithm.\n",
        "\n",
        "* Let's try to see how the token ID to embedding vector conversion works with a hands-on example.\n",
        "* Suppose have the following four input tokens with ids 12,15,1,10:"
      ],
      "metadata": {
        "id": "FCn-xBbQLeeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([1,2,3,4,5])"
      ],
      "metadata": {
        "id": "lON6669iMqGx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For simplicity purposes we have a small vocabulary of only 6 words, and we want to create embeddings of size 3(in gpt-3 , the embedding size is 12,288 dimensions):"
      ],
      "metadata": {
        "id": "xam1hjaLMxjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size  = 6\n",
        "output_dim= 3"
      ],
      "metadata": {
        "id": "WqPssTQxNETr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using `vocab_size` and `output_dim`, we can instantiate an embedding layer in Pytorch, setting the random seed to `42` for reproducibility purposes:"
      ],
      "metadata": {
        "id": "9-78OFKKNIqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k1vFMFgNZlS",
        "outputId": "ce325d39-61cb-46eb-9e60-09060176af7a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 1.9269,  1.4873, -0.4974],\n",
            "        [ 0.4396, -0.7581,  1.0783],\n",
            "        [ 0.8008,  1.6806,  0.3559],\n",
            "        [-0.6866,  0.6105,  1.3347],\n",
            "        [-0.2316,  0.0418, -0.2516],\n",
            "        [ 0.8599, -0.3097, -0.3957]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We see that the print statements prints the embedding layer's underlying weight matrix.\n",
        "* The weight matrix of the embedding layer contains random values. These values are optimized during LLM training as part of the LLM  optimization itself.\n",
        "* We can also that the weight matrix has six rows and 3 columns.\n",
        "* There is one row for each of the six possible tokens in the vocabulary, and there is one column for each of the three embedding dimensions.\n",
        "* Now let's test it using a token ID to obtain the embedding vector:"
      ],
      "metadata": {
        "id": "Na0dY0EBNsRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([5])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VHn7bzBOnX7",
        "outputId": "0a09dd2d-a7e1-4e9b-b5de-2feeec2c489c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.8599, -0.3097, -0.3957]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We have seen how to convert a single token ID into a three-dimensional embedding vector.Let'snow apply it to all the four input ids"
      ],
      "metadata": {
        "id": "9cSOZJvgPaYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfolBvH7QIwA",
        "outputId": "92ffbb28-3ba0-49e8-908a-8608b5f6fdb9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4396, -0.7581,  1.0783],\n",
            "        [ 0.8008,  1.6806,  0.3559],\n",
            "        [-0.6866,  0.6105,  1.3347],\n",
            "        [-0.2316,  0.0418, -0.2516],\n",
            "        [ 0.8599, -0.3097, -0.3957]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 Encoding word positions"
      ],
      "metadata": {
        "id": "_0Xj5vOSQQZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In general, token embeddings are suitable for an input for an LLM.\n",
        "* However, a minor shortcoming of LLMs is that their self-attention mechanism(see section 3) does not have a notion of position or order for the tokens within a sequence.\n",
        "\n",
        "* The way the previously introduced embedding layer works is that the same token ID always gets mapped to the same vector representation, regardless of where the token ID is positioned in the input sequence.\n",
        "* The deterministic,position-independent embedding of the token ID is good for reproducibility purposes.\n",
        "* However, since the self-attention mechanism of LLMs itself is also position-agnostic, it is helpful to inject additional position infromation into the LLM.\n",
        "* To acheive this, we can use two broad categories of position aware embeddings they are:\n",
        "  * Relative positional embeddings.\n",
        "  * Absolute positional embeddings.\n",
        "* Absolute postional embeddings are directly associated with specific postions in a sequence.For each position in the input sequence, a unique embedding is added to the token's embedding to convey its exact location.\n",
        "* These embeddings are added to the token embeddings before feeding into the transformer.\n",
        "\n",
        "* Relative postional embeddings - Instead of encoding absolute positions, it encodes relative distances between tokens.\n",
        "* For example, the model learns what it means for one token to be two positions before another, rather than being at position 5 or 10.\n",
        "\n",
        "* OpenAI's GPT models use absolute positional embeddings that are optimized during the training process rather than being fixed or predefined like the postional encodings in the original transformer model."
      ],
      "metadata": {
        "id": "R8WsBKgkQUq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Previously we focused on very small embedding sizes for simplicity.\n",
        "* Let's now consider more realistic and useful embedding sizes and encode the input token into a 256-dimensional vector representation, which is smaller than ChatGPT models these days.\n",
        "* Furthermore, we assume that the token IDs were created by the BPE tokenizer we implemented earlier, which has a vocabulary size of 50,257:"
      ],
      "metadata": {
        "id": "4U4XSJ7UNn6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
      ],
      "metadata": {
        "id": "-dUOSjoNP7sn"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using the previous `token_embedding_layer`, if we sample data from the data loader, we embed each token in each batch into a 256-dimensional vector.\n",
        "* If we have a batch size of 8 with four tokens each, the result will be an 8x4x256 tensor.\n",
        "* Let's instantiate the dataloader we have previously created:"
      ],
      "metadata": {
        "id": "s3Fp0hAjQJ4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader1(\n",
        "    raw_text,batch_size=8, max_length=max_length,\n",
        "    stride=max_length,shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Token IDs:\\n\",inputs)\n",
        "print(\"\\nInputs shape:\\n\",inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bXvWqFmQ6EW",
        "outputId": "b08a044c-62c8-4c33-b2fe-2482dcc628e0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[  171,   119,   123,   464],\n",
            "        [ 4935, 20336, 46566,   286],\n",
            "        [  383, 13009, 22329,   286],\n",
            "        [34898,   978,   394, 29864],\n",
            "        [   25,   383, 32458,   198],\n",
            "        [  220,   220,   220,   220],\n",
            "        [  198,  1212, 47179,   318],\n",
            "        [  329,   262,   779,   286]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can see, the token ID tensor is 8 x 4 dimensional, meaning that the data batch consists of eight text samples with four tokens each.\n",
        "\n",
        "* Let's now use the embedding layer to embed these token IDs into 256-dimensional vectors:"
      ],
      "metadata": {
        "id": "7VF-uXPIRhX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQIz3sJ6SMSU",
        "outputId": "e225d925-85b8-42a0-e5f6-989f73f5836a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The 8 x 4 x 256-dimensional tensor output shows that each token ID is now embedded as a 256-dimensional vector:\n",
        "\n",
        "* For a GPT model's absolute embedding approach, we just need to create another embedding layer that has the same embedding dimension as the `token_embedding_layer`:"
      ],
      "metadata": {
        "id": "k7uyA2srSZqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length,output_dim)\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehzK6A_MTnKJ",
        "outputId": "0544b524-1777-403e-f01c-3d0789daee08"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The input to the `pos_embeddings` is usually a placeholder vector `torch.arange(context_length)`,which contains a sequence of numbers 0,1,2..up to the maximum input length -1.\n",
        "* The `context_length` is a variable that represents the supported input size of the LLM.\n",
        "\n",
        "* As we can see, the positional embedding tensor consists of four 256-dimensional vectors.\n",
        "* We can now add these directly to the token embeddings, where Pytorch will add the 4 x 256- dimensional `pos_embeddings` tensor to eacg 4 x 256-dimensional token embedding tensor in each eight batchs:"
      ],
      "metadata": {
        "id": "OnAVAbDyU_io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmUbC4lBV2xA",
        "outputId": "ddd8b3e1-db10-4258-ff7d-c8d6a2283479"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The embeddings are typically added (not concatenated) because:\n",
        "  * `Efficiency`: Addition maintains the same dimensionality, keeping the model size manageable.\n",
        "\n",
        "  * `Information integration`: The model learns to encode both semantic content (what the token means) and positional context (where it appears) in the same representational space\n",
        "\n",
        "  * `Flexible learning`: During training, the model can learn how semantic and positional information should interact\n",
        "\n"
      ],
      "metadata": {
        "id": "O-4XIfTnXc25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Hands programming attention mechanisms\n",
        "\n",
        "* Now, we will look at an integral part of the LLM architecture itself , this is the attentional mechanisms in isolation and focus on them at a mechanistic level."
      ],
      "metadata": {
        "id": "geOrTQEQX-6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 The problem with modelling long sequences.\n",
        "* Before we dive into the `self-attention` mechanism at the core of LLMs, let's consider the probelm with pre-LLM architectures that do not include attention mechanisms.\n",
        "\n",
        "# Problems with Pre-LLM Architectures (No Attention)\n",
        "\n",
        "## 1. Poor Long-Range Dependency Modeling\n",
        "- RNNs process input **sequentially**, updating a hidden state as they go.\n",
        "- Earlier inputs must be remembered **only through this hidden state**, which gets \"overwritten\" by new tokens.\n",
        "- Result: **Hard to retain information** from 100s or 1000s of tokens back.\n",
        "- Even with LSTMs/GRUs, longer sequences still suffer from **context loss**.\n",
        "\n",
        "> **Example:** In a paragraph, remembering a subject introduced in sentence one to resolve a pronoun in sentence four is often unreliable.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. No Parallelization â Slow Training\n",
        "- RNNs process tokens **one at a time** â inherently **sequential computation**.\n",
        "- No parallelization across time steps like in attention-based models.\n",
        "- **Result:** Slow training and inference, especially for long sequences.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Fixed Memory Bottleneck\n",
        "- The hidden state is a **fixed-size vector** (e.g., 512 dimensions), regardless of the input sequence length.\n",
        "- All information (from 5 tokens or 500) must be **compressed** into this single vector.\n",
        "- Leads to **information loss** and capacity issues.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Gradient Problems\n",
        "During backpropagation through time (BPTT), gradients can:\n",
        "\n",
        "- **Vanish:** fail to update earlier layers.\n",
        "- **Explode:** become numerically unstable.\n",
        "\n",
        "> This makes training **unstable** and often leads to poor convergence.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. No Explicit Position or Focus\n",
        "- RNNs have no mechanism to **selectively focus** on specific past tokens.\n",
        "- They treat all past inputs **equally** via the hidden state.\n",
        "- This leads to:\n",
        "  - **Poor resolution of co-reference**\n",
        "  - Inability to model relationships between **distant tokens**\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Harder to Interpret\n",
        "- There's no explicit signal for **which part of the input** the model is attending to.\n",
        "- Unlike attention (which provides interpretable weights), **RNN hidden states are opaque**.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Less Transferable Representations\n",
        "- RNNs trained on one task **donât easily adapt** to other tasks or domains.\n",
        "- Attention models (e.g., Transformers) learn better **contextual embeddings**, which are more **transferable** and general-purpose (e.g., BERT, GPT).\n"
      ],
      "metadata": {
        "id": "gXO5ykLJY5FL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Attending to different parts of the input with self-attention."
      ],
      "metadata": {
        "id": "FBHYRzB3E45a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We will now cover the inner workings of the self-attention mechanism and learn how to code it from ground up.\n",
        "* In self-attetion, the `self` refers to the mechanism's ability to compute attention weights by relating different positions with a single input sequence."
      ],
      "metadata": {
        "id": "9sCb9sJZFET4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 A simple self-attention mechanism without trainable weights"
      ],
      "metadata": {
        "id": "8HsCJjXUF9Yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's begin by implementing a simplified variant of self-attention,free from trainable weights.\n",
        "* The goal of self-attention is to compute a `context vector` for each input element that combines information from all other input.\n",
        "* For example purposes consider an input text like \"You are on a mission\".In this case, each element of the sequence, such as `x(1)`, corresponds to a d-dimensional embedding representing a specific token, like `You`.\n",
        "* In self-attention, our goals is to calculate context vectors `z(i)` for each element `x(i)` in the input sequence.\n",
        "* To illustrate this concept, let's focus on the embedding vector of the second input element, `x(2)` (which corresponds to the token `are`), and the corresponding context vector, `z(2)`.\n",
        "* This enhanced context vector, `z(2)`, is an embedding that contains information about `x(2) ` and all other input elements, `x(1) to x(T)`.\n",
        "* Context vectors play a crucial role in self-attention in whereby they create enriched representations of each element in an input sequence(like a sentence) by incorporating information from all other elements in the sequence.\n",
        "* Now let's try a little bit of code:\n"
      ],
      "metadata": {
        "id": "MAEi1X71IcdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "inputs = torch.tensor(\n",
        "    [\n",
        "        [0.43,0.15,0.88],#You (x^1)\n",
        "        [0.45,0.87,0.65],#are (x^2)\n",
        "        [0.57,0.85,0.64],#on (x^3)\n",
        "        [0.22,0.58,0.33],#a (x^4)\n",
        "        [0.77,0.25,0.10]# mission (x^5)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "V27OekHBPB7Z"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first step of implementing self-attention is to compute the intermediate values `w`, referred to attention scores.\n",
        "* We determine these scores by computing the dott product of the query, `x(2)`, with every other input token:"
      ],
      "metadata": {
        "id": "rpckSWRjP6Pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1] # the second input serves as the query.\n",
        "attn_scores2 = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "  attn_scores2[i] = torch.dot(x_i,query)\n",
        "print(attn_scores2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOrc30t8UAlp",
        "outputId": "d6d2c11a-2607-416d-b129-9271d270bf57"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.8960, 1.3819, 1.4120, 0.8181, 0.6290])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The next step is to normalize each of the attention scores we computed previously.\n",
        "* The main goal behind normalization is to obtain attention weights that sum up to 1."
      ],
      "metadata": {
        "id": "9MpQ_06_UiGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores2_tmp = attn_scores2 / attn_scores2.sum()\n",
        "print(\"Attention weights:\",attn_scores2_tmp)\n",
        "print(\"Sum:\",attn_scores2_tmp.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnZ04LEDU2G_",
        "outputId": "6babe9c0-b4bb-4745-db71-dda935f155a7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1744, 0.2690, 0.2749, 0.1593, 0.1224])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In practice it's more common and advisable to use the softmax function for normalization."
      ],
      "metadata": {
        "id": "MSZDYTNWVN2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_function(x):\n",
        "  return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "attn_weights2_func = softmax_function(attn_scores2)\n",
        "print(\"Attention weights:\",attn_weights2_func)\n",
        "print(\"Sum:\",attn_weights2_func.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB9Qz5tSo3Bm",
        "outputId": "0579f48e-8fd3-40b9-802c-fcd3bd247027"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1669, 0.2713, 0.2796, 0.1544, 0.1278])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The best option is use Pytorch implementation of softmax, which has been extensively optimized for performance:"
      ],
      "metadata": {
        "id": "zDDWaklepXWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights2 = torch.softmax(attn_scores2,dim=0)\n",
        "print(\"Attention weigths:\",attn_weights2)\n",
        "print(\"Sum:\",attn_weights2.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3U-gXD8pjEO",
        "outputId": "87ff06a6-e5f9-4c17-b008-e6a7e55e5ebb"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weigths: tensor([0.1669, 0.2713, 0.2796, 0.1544, 0.1278])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now that we have calculated the normalized attention weights, we are ready for the final step which is calculating the context vectors `z(2)` by multiplying embedded input tokens, `x(i)`,with the corresponding attention weights and summing the resulting vectors.\n",
        "* So basically, the context vector `z(2)` is the weighted sum of all input vectors, obtained by multiplying each input vector by it's corresponding attention weight:"
      ],
      "metadata": {
        "id": "ZkYV61gNp5FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1]#the second input is the query\n",
        "context_vec2 = torch.zeros(query.shape)\n",
        "for i , x_i in enumerate(inputs):\n",
        "  context_vec2 += attn_weights2[i]*x_i\n",
        "print(context_vec2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRn4KOqDqqcS",
        "outputId": "f562e737-8425-4cbd-fe03-1678b079ea47"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4856, 0.6202, 0.5659])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3.1 Computing attention weights for all input tokens\n"
      ],
      "metadata": {
        "id": "4hB0rRhnrIlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* So far we have computed attention weights and context vector for input 2.\n",
        "* Now let's extend this computation to calculate attention weights and context vectors for all input.\n",
        "* We will follow the three steps below:\n",
        "   * Compute attention scores - Compute the attention scores as dot products between the inputs.\n",
        "   * Compute attention weights - the attention weights are normalized version of attention scores.\n",
        "   * Compute context vectors - the context vectors are computed as weighted sum over the inputs"
      ],
      "metadata": {
        "id": "LJaAhY4IrWVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = torch.empty(6,6)\n",
        "for i, x_i in enumerate(inputs):\n",
        "  for j, x_j in enumerate(inputs):\n",
        "    attn_scores[i,j] = torch.dot(x_i,x_j)\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhkOWuAXszdi",
        "outputId": "c0d34591-3529-421e-ecf0-2987575242d5"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[9.8180e-01, 8.9600e-01, 9.3580e-01, 4.7200e-01, 4.5660e-01, 1.5766e-19],\n",
            "        [8.9600e-01, 1.3819e+00, 1.4120e+00, 8.1810e-01, 6.2900e-01, 2.7491e+20],\n",
            "        [9.3580e-01, 1.4120e+00, 1.4570e+00, 8.2960e-01, 7.1540e-01, 7.2250e+28],\n",
            "        [4.7200e-01, 8.1810e-01, 8.2960e-01, 4.9370e-01, 3.4740e-01, 2.1252e-07],\n",
            "        [4.5660e-01, 6.2900e-01, 7.1540e-01, 3.4740e-01, 6.6540e-01, 6.1949e-04],\n",
            "        [6.4805e-10, 6.3013e-10, 4.2481e-05, 6.9120e-04, 6.9362e-07, 1.6967e-07]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Each element in the tenosr above represents an attention score between each pair of inputs.\n",
        "* Note that these tensors are unnormalized, we will deal with normalization later.\n",
        "\n",
        "* When computing the preceding attention tensor we used `for` loops.\n",
        "* However `for` loops are generally slow, and we can achieve the same results using matrix multiplication (matmul)."
      ],
      "metadata": {
        "id": "lDGmLOdwuANp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = inputs @ inputs.T\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WydvvXeyusys",
        "outputId": "1ba040e3-7832-46c8-c074-df5a5a671688"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9818, 0.8960, 0.9358, 0.4720, 0.4566],\n",
            "        [0.8960, 1.3819, 1.4120, 0.8181, 0.6290],\n",
            "        [0.9358, 1.4120, 1.4570, 0.8296, 0.7154],\n",
            "        [0.4720, 0.8181, 0.8296, 0.4937, 0.3474],\n",
            "        [0.4566, 0.6290, 0.7154, 0.3474, 0.6654]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we normalize each row so that the values in each row sum to 1:"
      ],
      "metadata": {
        "id": "CbzqVHpmu8W6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(attn_scores,dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmJ8PYYlvDkY",
        "outputId": "1965616e-572a-4998-c238-592533cd17ee"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2460, 0.2258, 0.2350, 0.1478, 0.1455],\n",
            "        [0.1669, 0.2713, 0.2796, 0.1544, 0.1278],\n",
            "        [0.1668, 0.2685, 0.2809, 0.1500, 0.1338],\n",
            "        [0.1740, 0.2459, 0.2488, 0.1778, 0.1536],\n",
            "        [0.1782, 0.2117, 0.2308, 0.1597, 0.2196]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In Pytorch, the dim parameter in functions like `torch.softmax` specifies the dimension of the input tensor along which the function will be computed.\n",
        "* By setting `dim =-1`, we are instructing the `softmax` function to apply the normalization along the last dimension of the `att_scores` tensor."
      ],
      "metadata": {
        "id": "3PD1DocJwbNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The third and final step is to use these attention weigths to compute all context vectors via matrix multiplication:"
      ],
      "metadata": {
        "id": "9P7iOCpOxY7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_context_vecs = attn_weights @ inputs\n",
        "print(all_context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ty2N6aN7xmuf",
        "outputId": "476ff29c-5d8d-42b7-a302-66fccb3086b9"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4858, 0.5551, 0.5769],\n",
            "        [0.4856, 0.6202, 0.5659],\n",
            "        [0.4887, 0.6178, 0.5640],\n",
            "        [0.4846, 0.5930, 0.5462],\n",
            "        [0.5076, 0.5546, 0.5168]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can double-check that code is correct by comparing the second row with the context vector `z(2)` that we computed previously."
      ],
      "metadata": {
        "id": "94C4d0E8yFZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Previously 2nd context vector:\",context_vec2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk7gLfbOyWxz",
        "outputId": "2bef75cd-d82b-45c0-af1b-305810120b2e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previously 2nd context vector: tensor([0.4856, 0.6202, 0.5659])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can see that the previosly calculated `context_vec2` matches the second row in the previous tensor exactly."
      ],
      "metadata": {
        "id": "ZnWxdIofrb0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Self-attention with trainable weights."
      ],
      "metadata": {
        "id": "eDCpLHsXtFbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We will implement self-attention mechanism step by step by introducing the following trainable weight matrices:\n",
        "  * `Wq`\n",
        "  * `Wk`\n",
        "  * `Wv`\n",
        "* These three matrices are used to project the embedded input tokens, `x(i)`, into query, key and value vectors.\n",
        "* Previously we defined the second input element `x(2)` as the query when we computed the simplified attention weigths to compute the context vector `z(2)`. Later we generalized this to compute all context vectors `z(1)....z(T)` for the six-word input sentence `You are on a mission`.\n",
        "* Similary, we start by computing only one context vector, `z(2)` for example usecases.\n",
        "* We will then modify this code to calculate all context vectors.\n",
        "* Let's start defining a few variables:"
      ],
      "metadata": {
        "id": "sqiaSjP4uELX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = inputs[1]# second input element\n",
        "d_in = inputs.shape[1] #input embedding size, d = 3\n",
        "d_out = 2 #output embedding size, d_out =2"
      ],
      "metadata": {
        "id": "ZWo2_54kvmsY"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Note that in GPT-like models, the input and output dimensions are usually the same, but to better follow, we'll use different input `d_in = 3` and output `d_out=2` dimension here.\n",
        "\n",
        "* Next, we initialize the three weight matrices `Wq,Wk,Wv`"
      ],
      "metadata": {
        "id": "rh2G5orJwA83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "Wq = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
        "Wk = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
        "Wv = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)"
      ],
      "metadata": {
        "id": "f-u-Lrtiwe13"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We have set `requires_grad=False` to reduce clutter in the outputs, but if we were to use the weight matrices for model training, we would instead set `requires_grad=True` to update these matrices during model training.\n",
        "* Next, we compute the query, key and value vectors:"
      ],
      "metadata": {
        "id": "cVGyjWMdxSvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = x2 @ Wq\n",
        "key2 = x2 @ Wk\n",
        "value2 = x2 @ Wv\n",
        "print(query2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp3XFnKexyF7",
        "outputId": "49ba23a3-a8df-4578-8be1-a87a545c12bb"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9839, 1.6369])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The output for the query results in a two-dimensional vector since we previously set the number of columns of the corresponding weight matrix, via `d_out`, to 2.\n",
        "\n",
        "* Even though our temporary goal is only to compute the one context vector, `z(2)`, we still require the key value and value vectors for all input elements as they are involved in computing the attention weigths with respect to the query `q(2)`.\n",
        "\n",
        "* We can obtain all keys and vlues via matrix multiplication:"
      ],
      "metadata": {
        "id": "mYTpwT4A04z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @ Wk\n",
        "values = inputs @ Wv\n",
        "print(\"keys.shape:\",keys.shape)\n",
        "print(\"values.shape\",values.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf4UqvL__cRS",
        "outputId": "c1dc8cf8-f5ca-4312-d291-8a77b58c8a17"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys.shape: torch.Size([5, 2])\n",
            "values.shape torch.Size([5, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As we can tell from the outputs, we successfully projected the five input token from a three-dimensional onto a two-dimensioanl embedding space.\n",
        "*The second step is to compute the attention scores.\n",
        "* Let's compute the attention score `w22`:"
      ],
      "metadata": {
        "id": "YydaJZAT_uXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys2 = keys[1]\n",
        "attn_scores22 = query2.dot(keys2)\n",
        "print(attn_scores22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCYXqOPXBB6R",
        "outputId": "f37c181c-dd95-4638-af9e-b8567d329829"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.9225)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We see the result is an unnormalized attention score."
      ],
      "metadata": {
        "id": "5u4LcjT6BmTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Again, we can generalize this computation to all attention scores via matrix multiplication:"
      ],
      "metadata": {
        "id": "tm0ma-NNBT6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores2 = query2 @ keys.T\n",
        "print(attn_scores2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFtYBS6oASnh",
        "outputId": "240e5f89-7855-4e74-ce60-1160bcb619a7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.5030, 2.9225, 3.0669, 1.6288, 1.6697])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we want to compute the attention weights by scaling the attention scores and using the softmax function.\n",
        "* However, we scale the attention scores by dividing them by the square root of the embedding dimension of the keys (that is taking the square root is mathematically the same as exponentiating by 0.5):"
      ],
      "metadata": {
        "id": "LAxNetcSB7N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dk = keys.shape[-1]\n",
        "attn_weights2 = torch.softmax(attn_scores2/dk**0.5,dim=-1)\n",
        "print(attn_weights2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdL7TufXC1Jf",
        "outputId": "c53ee61b-3aad-47eb-ef23-36e898abff7a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2029, 0.2729, 0.3023, 0.1093, 0.1126])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we compute the context vectors"
      ],
      "metadata": {
        "id": "9jy3wjIRDKj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec2 = attn_weights2 @ values\n",
        "print(context_vec2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oByvbL-xDka2",
        "outputId": "1109cbcc-7af8-4cf3-96df-848ef2dff725"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.4139, 0.8871])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* So far we have only computed a single context vector, `z(2)`.\n",
        "* Next, we will generalize the code to compute all context vectors in the input sequence, `z(1) to z(t)`."
      ],
      "metadata": {
        "id": "6NuLmvqnDyAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In transformers, the query (Q) represents what you're looking for, the key (K) represents what each input token offers, and the value (V) contains the actual information to retrieve. The model compares each query to all keys using dot products to calculate attention scores, determining how much focus each token deserves. Then, it uses those scores to weigh the values and produce a context-aware output for each token."
      ],
      "metadata": {
        "id": "dHRfTRJctN6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 implementing a compact self-attention Python class"
      ],
      "metadata": {
        "id": "dvH2cwanttPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class SelfAttention1(nn.Module):\n",
        "  def __init__(self,d_in,d_out):\n",
        "    super().__init__()\n",
        "    self.Wq = nn.Parameter(torch.rand(d_in,d_out))\n",
        "    self.Wk = nn.Parameter(torch.rand(d_in,d_out))\n",
        "    self.Wv = nn.Parameter(torch.rand(d_in,d_out))\n",
        "\n",
        "  def forward(self,x):\n",
        "    keys = x @ self.Wk\n",
        "    queries = x @ self.Wq\n",
        "    values = x @ self.Wv\n",
        "    attn_scores =queries @ keys.T\n",
        "    attn_weigths = torch.softmax(\n",
        "        attn_scores / keys.shape[-1] ** 0.5, dim=-1\n",
        "    )\n",
        "    context_vec = attn_weigths @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "scyemOM5ty-I"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's initialize this class as follows:"
      ],
      "metadata": {
        "id": "LQBsCwTru_Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "sa1 = SelfAttention1(d_in,d_out)\n",
        "print(sa1(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibSazunTvETZ",
        "outputId": "f98f576b-87ca-4830-ec64-5dcbd217ffdc"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2917, 0.7996],\n",
            "        [0.2983, 0.8155],\n",
            "        [0.2991, 0.8176],\n",
            "        [0.2863, 0.7865],\n",
            "        [0.2840, 0.7809]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Since `inputs` contain five embedding vectors, this reslut in matrix storing five context vectors.\n",
        "\n",
        "* Self-attention involves the trainable weigtht matrices `Wq,Wk and Wq`.\n",
        "* These matrices transform input data into queries,keys and values, respectively, which are  crucial components of attention mechanism.\n",
        "* We can improve `selfattention1` implementation further by utilizing Pytorch's `nn.Linear` layers, which effectively perfrom matrix multiplication when bias units are disabled."
      ],
      "metadata": {
        "id": "nz9R88huvlcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention2(nn.Module):\n",
        "  def __init__(self,d_in,d_out,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.Wq = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.Wk = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.Wv = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "\n",
        "  def forward(self,x):\n",
        "    keys = self.Wk(x)\n",
        "    queries = self.Wq(x)\n",
        "    values = self.Wv(x)\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights= torch.softmax(\n",
        "        attn_scores / keys.shape[-1]** 0.5,dim=-1    )\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "G6gZVabaw49Q"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##instantiating selfattention2\n",
        "torch.manual_seed(789)\n",
        "sa2 = SelfAttention2(d_in,d_out)\n",
        "print(sa2(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg8nyU6H1HDp",
        "outputId": "6ede78ce-1f35-4e3a-c4dd-7705f3ac7aca"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0538,  0.1025],\n",
            "        [-0.0553,  0.1005],\n",
            "        [-0.0553,  0.1004],\n",
            "        [-0.0554,  0.1006],\n",
            "        [-0.0555,  0.1006]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    }
  ]
}